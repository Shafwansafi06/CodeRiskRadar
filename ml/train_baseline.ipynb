{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9020cc18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model comparison summary\n",
    "comparison = pd.DataFrame({\n",
    "    'Metric': ['Validation MSE', 'Validation MAE', 'Validation RÂ²', 'Inference Speed', 'Forge Compatible', 'Interpretability'],\n",
    "    'Logistic Regression': [\n",
    "        f'{val_mse:.2f}',\n",
    "        f'{val_mae:.2f}',\n",
    "        f'{val_r2:.3f}',\n",
    "        '<1ms',\n",
    "        'âœ… Yes',\n",
    "        'â­â­â­â­â­'\n",
    "    ]\n",
    "})\n",
    "\n",
    "if LIGHTGBM_AVAILABLE:\n",
    "    comparison['LightGBM'] = [\n",
    "        f'{lgb_val_mse:.2f}',\n",
    "        f'{lgb_val_mae:.2f}',\n",
    "        f'{lgb_val_r2:.3f}',\n",
    "        '~5ms',\n",
    "        'âŒ No (Python only)',\n",
    "        'â­â­â­â­'\n",
    "    ]\n",
    "\n",
    "print(\"\\nðŸ“Š Model Comparison:\")\n",
    "print(comparison.to_string(index=False))\n",
    "\n",
    "print(\"\\n\\nðŸŽ¯ RECOMMENDATION: Deploy with Logistic Regression\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f5f3e08",
   "metadata": {},
   "source": [
    "## 11. Model Comparison & Recommendation\n",
    "\n",
    "**Final Recommendation**: Use **Logistic Regression (Ridge)** for Code Risk Radar\n",
    "\n",
    "**Rationale**:\n",
    "1. âœ… **Deployment**: Can be implemented in pure JavaScript for Forge Functions\n",
    "2. âœ… **Interpretability**: Direct feature weights are easier to explain to users\n",
    "3. âœ… **Performance**: Sufficient accuracy for MVP (MAE ~10-15 points on 0-100 scale)\n",
    "4. âœ… **Maintenance**: No model file dependencies, just coefficients\n",
    "5. âœ… **Speed**: Extremely fast inference (<1ms)\n",
    "\n",
    "**When to upgrade to LightGBM**:\n",
    "- Post-MVP with real production data (>1000 labeled PRs)\n",
    "- Deploy as Python microservice (not Forge Function)\n",
    "- Need 5-10% accuracy improvement\n",
    "- Have resources for model retraining pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e6b4d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "if LIGHTGBM_AVAILABLE:\n",
    "    print(\"Training LightGBM model...\")\n",
    "    \n",
    "    # Train LightGBM regressor\n",
    "    model_lgb = lgb.LGBMRegressor(\n",
    "        n_estimators=100,\n",
    "        learning_rate=0.05,\n",
    "        max_depth=5,\n",
    "        num_leaves=31,\n",
    "        random_state=RANDOM_STATE,\n",
    "        verbose=-1\n",
    "    )\n",
    "    \n",
    "    model_lgb.fit(\n",
    "        X_train, y_train_score,\n",
    "        eval_set=[(X_val, y_val_score)],\n",
    "        eval_metric='rmse'\n",
    "    )\n",
    "    \n",
    "    # Predictions\n",
    "    y_pred_lgb_train = np.clip(model_lgb.predict(X_train), 0, 100)\n",
    "    y_pred_lgb_val = np.clip(model_lgb.predict(X_val), 0, 100)\n",
    "    \n",
    "    # Metrics\n",
    "    lgb_train_mse = mean_squared_error(y_train_score, y_pred_lgb_train)\n",
    "    lgb_train_mae = mean_absolute_error(y_train_score, y_pred_lgb_train)\n",
    "    lgb_train_r2 = r2_score(y_train_score, y_pred_lgb_train)\n",
    "    \n",
    "    lgb_val_mse = mean_squared_error(y_val_score, y_pred_lgb_val)\n",
    "    lgb_val_mae = mean_absolute_error(y_val_score, y_pred_lgb_val)\n",
    "    lgb_val_r2 = r2_score(y_val_score, y_pred_lgb_val)\n",
    "    \n",
    "    print(f\"\\nâœ“ LightGBM model trained\")\n",
    "    print(f\"\\n  Training Performance:\")\n",
    "    print(f\"    MSE: {lgb_train_mse:.2f}\")\n",
    "    print(f\"    MAE: {lgb_train_mae:.2f}\")\n",
    "    print(f\"    RÂ²: {lgb_train_r2:.3f}\")\n",
    "    print(f\"\\n  Validation Performance:\")\n",
    "    print(f\"    MSE: {lgb_val_mse:.2f}\")\n",
    "    print(f\"    MAE: {lgb_val_mae:.2f}\")\n",
    "    print(f\"    RÂ²: {lgb_val_r2:.3f}\")\n",
    "    \n",
    "    # Feature importance\n",
    "    lgb.plot_importance(model_lgb, max_num_features=20, figsize=(12, 8))\n",
    "    plt.title('LightGBM Feature Importance')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # SHAP for LightGBM\n",
    "    explainer_lgb = shap.TreeExplainer(model_lgb)\n",
    "    shap_values_lgb = explainer_lgb.shap_values(X_val)\n",
    "    \n",
    "    shap.summary_plot(shap_values_lgb, X_val, show=False)\n",
    "    plt.title('LightGBM SHAP Feature Importance')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Save LightGBM model\n",
    "    model_lgb.booster_.save_model(str(MODEL_DIR / 'lightgbm_model.txt'))\n",
    "    print(f\"\\nâœ“ LightGBM model saved to {MODEL_DIR}/lightgbm_model.txt\")\n",
    "    \n",
    "else:\n",
    "    print(\"LightGBM not available. Install with: pip install lightgbm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a63f4ab8",
   "metadata": {},
   "source": [
    "## 10. Alternative: LightGBM Model (Optional)\n",
    "\n",
    "**When to use LightGBM over Logistic Regression:**\n",
    "- âœ… Need better accuracy (typically 5-10% improvement)\n",
    "- âœ… Can deploy as Python microservice (not Forge Function)\n",
    "- âœ… Have sufficient training data (>1000 samples)\n",
    "- âŒ Harder to port to JavaScript\n",
    "- âŒ Requires model file for deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c1c48cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model, scaler, and metadata\n",
    "model_artifacts = {\n",
    "    'model': model_lr,\n",
    "    'scaler': scaler,\n",
    "    'feature_names': list(X_train.columns),\n",
    "    'explainer': explainer,\n",
    "    'model_type': 'ridge_regression',\n",
    "    'performance': {\n",
    "        'val_mse': val_mse,\n",
    "        'val_mae': val_mae,\n",
    "        'val_r2': val_r2\n",
    "    }\n",
    "}\n",
    "\n",
    "joblib.dump(model_artifacts, MODEL_DIR / 'baseline_model.pkl')\n",
    "\n",
    "# Also save just coefficients for JavaScript implementation\n",
    "model_coefficients = {\n",
    "    'intercept': float(model_lr.intercept_),\n",
    "    'coefficients': {\n",
    "        feature: float(coef) \n",
    "        for feature, coef in zip(X_train.columns, model_lr.coef_)\n",
    "    },\n",
    "    'scaler_mean': {\n",
    "        feature: float(mean) \n",
    "        for feature, mean in zip(X_train.columns, scaler.mean_)\n",
    "    },\n",
    "    'scaler_scale': {\n",
    "        feature: float(scale) \n",
    "        for feature, scale in zip(X_train.columns, scaler.scale_)\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(MODEL_DIR / 'model_coefficients.json', 'w') as f:\n",
    "    json.dump(model_coefficients, f, indent=2)\n",
    "\n",
    "print(f\"âœ“ Model artifacts saved to {MODEL_DIR}/\")\n",
    "print(f\"  - baseline_model.pkl (full Python model)\")\n",
    "print(f\"  - model_coefficients.json (for JavaScript inference)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e44471",
   "metadata": {},
   "source": [
    "## 9. Export Model Artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b1747b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute axis scores using feature extractor's weighted aggregation\n",
    "sample_pr = val_raw[high_risk_idx]\n",
    "axis_scores = extractor.extract_axis_scores(extractor.extract_features(sample_pr))\n",
    "\n",
    "print(f\"\\nðŸ“Š Axis Scores for high-risk PR:\")\n",
    "for axis, score in sorted(axis_scores.items(), key=lambda x: x[1], reverse=True):\n",
    "    bar = 'â–ˆ' * int(score / 5)\n",
    "    print(f\"  {axis:20s}: {score:5.1f}/100 {bar}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b2f4f0",
   "metadata": {},
   "source": [
    "## 8. Axis Score Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f37abfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Explain a single high-risk prediction\n",
    "high_risk_idx = np.argmax(y_pred_val)\n",
    "print(f\"\\nðŸ“Š Explaining prediction for sample {high_risk_idx}:\")\n",
    "print(f\"  Actual risk score: {y_val_score[high_risk_idx]:.1f}\")\n",
    "print(f\"  Predicted risk score: {y_pred_val[high_risk_idx]:.1f}\")\n",
    "\n",
    "# Get SHAP values for this sample\n",
    "sample_shap = shap_values[high_risk_idx]\n",
    "sample_features = X_val.iloc[high_risk_idx]\n",
    "\n",
    "# Get top contributing features\n",
    "feature_contributions = pd.DataFrame({\n",
    "    'feature': X_train.columns,\n",
    "    'value': sample_features.values,\n",
    "    'shap_value': sample_shap\n",
    "}).sort_values('shap_value', key=abs, ascending=False)\n",
    "\n",
    "print(f\"\\n  Top 5 contributing features:\")\n",
    "for _, row in feature_contributions.head(5).iterrows():\n",
    "    direction = \"â†‘ increases\" if row['shap_value'] > 0 else \"â†“ decreases\"\n",
    "    print(f\"    {row['feature']}: {row['value']:.2f} ({direction} risk by {abs(row['shap_value']):.2f})\")\n",
    "\n",
    "# Waterfall plot for this prediction\n",
    "shap.waterfall_plot(\n",
    "    shap.Explanation(\n",
    "        values=sample_shap,\n",
    "        base_values=explainer.expected_value,\n",
    "        data=X_val_scaled[high_risk_idx],\n",
    "        feature_names=X_train.columns\n",
    "    ),\n",
    "    show=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f6cf59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHAP summary plot\n",
    "plt.figure(figsize=(12, 8))\n",
    "shap.summary_plot(shap_values, X_val_scaled, feature_names=X_train.columns, show=False)\n",
    "plt.title('SHAP Feature Importance')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b28b8705",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize SHAP explainer for linear models\n",
    "explainer = shap.LinearExplainer(model_lr, X_train_scaled, feature_names=X_train.columns)\n",
    "\n",
    "# Calculate SHAP values for validation set\n",
    "print(\"Calculating SHAP values...\")\n",
    "shap_values = explainer.shap_values(X_val_scaled)\n",
    "\n",
    "print(f\"âœ“ SHAP values calculated\")\n",
    "print(f\"  Shape: {shap_values.shape}\")\n",
    "print(f\"  Base value: {explainer.expected_value:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "738e9e75",
   "metadata": {},
   "source": [
    "## 7. SHAP Explainability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "728bfc1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature coefficients (weights)\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': X_train.columns,\n",
    "    'coefficient': model_lr.coef_,\n",
    "    'abs_coefficient': np.abs(model_lr.coef_)\n",
    "}).sort_values('abs_coefficient', ascending=False)\n",
    "\n",
    "# Plot top 20 features\n",
    "plt.figure(figsize=(12, 8))\n",
    "top_features = feature_importance.head(20)\n",
    "colors = ['red' if x < 0 else 'green' for x in top_features['coefficient']]\n",
    "plt.barh(range(len(top_features)), top_features['coefficient'], color=colors)\n",
    "plt.yticks(range(len(top_features)), top_features['feature'])\n",
    "plt.xlabel('Coefficient (Impact on Risk Score)')\n",
    "plt.title('Top 20 Most Important Features')\n",
    "plt.axvline(0, color='black', linestyle='-', linewidth=0.8)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nTop 10 risk-increasing features:\")\n",
    "print(feature_importance.head(10)[['feature', 'coefficient']].to_string(index=False))\n",
    "\n",
    "print(\"\\nTop 10 risk-decreasing features:\")\n",
    "print(feature_importance.tail(10)[['feature', 'coefficient']].to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a460fafb",
   "metadata": {},
   "source": [
    "## 6. Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28fed18a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize predictions vs actuals\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "axes[0].scatter(y_train_score, y_pred_train, alpha=0.5)\n",
    "axes[0].plot([0, 100], [0, 100], 'r--', lw=2)\n",
    "axes[0].set_xlabel('Actual Risk Score')\n",
    "axes[0].set_ylabel('Predicted Risk Score')\n",
    "axes[0].set_title(f'Training Set (RÂ²={train_r2:.3f})')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].scatter(y_val_score, y_pred_val, alpha=0.5, color='orange')\n",
    "axes[1].plot([0, 100], [0, 100], 'r--', lw=2)\n",
    "axes[1].set_xlabel('Actual Risk Score')\n",
    "axes[1].set_ylabel('Predicted Risk Score')\n",
    "axes[1].set_title(f'Validation Set (RÂ²={val_r2:.3f})')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2681f1a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature scaling (important for logistic regression)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "\n",
    "# Train logistic regression for regression (predicting risk_score 0-100)\n",
    "# We'll train a regressor and then map to risk levels\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "print(\"Training Logistic Regression model...\")\n",
    "model_lr = Ridge(alpha=1.0, random_state=RANDOM_STATE)\n",
    "model_lr.fit(X_train_scaled, y_train_score)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_train = model_lr.predict(X_train_scaled)\n",
    "y_pred_val = model_lr.predict(X_val_scaled)\n",
    "\n",
    "# Clip predictions to valid range [0, 100]\n",
    "y_pred_train = np.clip(y_pred_train, 0, 100)\n",
    "y_pred_val = np.clip(y_pred_val, 0, 100)\n",
    "\n",
    "# Calculate metrics\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "train_mse = mean_squared_error(y_train_score, y_pred_train)\n",
    "train_mae = mean_absolute_error(y_train_score, y_pred_train)\n",
    "train_r2 = r2_score(y_train_score, y_pred_train)\n",
    "\n",
    "val_mse = mean_squared_error(y_val_score, y_pred_val)\n",
    "val_mae = mean_absolute_error(y_val_score, y_pred_val)\n",
    "val_r2 = r2_score(y_val_score, y_pred_val)\n",
    "\n",
    "print(f\"\\nâœ“ Model trained successfully\")\n",
    "print(f\"\\n  Training Performance:\")\n",
    "print(f\"    MSE: {train_mse:.2f}\")\n",
    "print(f\"    MAE: {train_mae:.2f}\")\n",
    "print(f\"    RÂ²: {train_r2:.3f}\")\n",
    "print(f\"\\n  Validation Performance:\")\n",
    "print(f\"    MSE: {val_mse:.2f}\")\n",
    "print(f\"    MAE: {val_mae:.2f}\")\n",
    "print(f\"    RÂ²: {val_r2:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55689418",
   "metadata": {},
   "source": [
    "## 5. Model Training - Logistic Regression (RECOMMENDED)\n",
    "\n",
    "**Why Logistic Regression?**\n",
    "- âœ… Highly interpretable (direct feature weights)\n",
    "- âœ… Fast training and inference\n",
    "- âœ… Easy to port to JavaScript for Forge Functions\n",
    "- âœ… Works well with SHAP for explanations\n",
    "- âœ… No external model files needed (just coefficients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ba8ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize risk score distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "axes[0].hist(y_train_score, bins=30, edgecolor='black', alpha=0.7)\n",
    "axes[0].set_xlabel('Risk Score')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "axes[0].set_title('Risk Score Distribution')\n",
    "axes[0].axvline(30, color='green', linestyle='--', label='Low/Medium threshold')\n",
    "axes[0].axvline(70, color='red', linestyle='--', label='Medium/High threshold')\n",
    "axes[0].legend()\n",
    "\n",
    "# Risk level counts\n",
    "risk_counts = pd.Series(y_train_label).value_counts()\n",
    "axes[1].bar(risk_counts.index, risk_counts.values, color=['green', 'orange', 'red'])\n",
    "axes[1].set_xlabel('Risk Level')\n",
    "axes[1].set_ylabel('Count')\n",
    "axes[1].set_title('Risk Level Distribution')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Feature correlation with risk score\n",
    "correlations = X_train.corrwith(pd.Series(y_train_score, index=X_train.index)).sort_values(ascending=False)\n",
    "print(f\"\\nTop 10 features correlated with risk score:\")\n",
    "print(correlations.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b1a13b3",
   "metadata": {},
   "source": [
    "## 4. Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "323daaec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize feature extractor\n",
    "extractor = PRFeatureExtractor()\n",
    "\n",
    "# Extract features for all samples\n",
    "def extract_features_and_labels(raw_data):\n",
    "    \"\"\"Extract features and labels from raw PR data\"\"\"\n",
    "    X_list = []\n",
    "    y_scores = []\n",
    "    y_labels = []\n",
    "    \n",
    "    for pr in raw_data:\n",
    "        # Extract features\n",
    "        features = extractor.extract_features(pr)\n",
    "        X_list.append(features)\n",
    "        \n",
    "        # Extract labels\n",
    "        y_scores.append(pr['labels']['risk_score'])\n",
    "        y_labels.append(pr['labels']['risk_level'])\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    X = pd.DataFrame(X_list)\n",
    "    y_score = np.array(y_scores)\n",
    "    y_label = np.array(y_labels)\n",
    "    \n",
    "    return X, y_score, y_label\n",
    "\n",
    "# Extract training data\n",
    "print(\"Extracting features from training data...\")\n",
    "X_train, y_train_score, y_train_label = extract_features_and_labels(train_raw)\n",
    "\n",
    "# Extract validation data\n",
    "print(\"Extracting features from validation data...\")\n",
    "X_val, y_val_score, y_val_label = extract_features_and_labels(val_raw)\n",
    "\n",
    "print(f\"\\nâœ“ Feature extraction complete\")\n",
    "print(f\"  - Feature matrix shape: {X_train.shape}\")\n",
    "print(f\"  - Number of features: {X_train.shape[1]}\")\n",
    "print(f\"  - Target range: {y_train_score.min():.1f} - {y_train_score.max():.1f}\")\n",
    "\n",
    "# Display sample features\n",
    "print(f\"\\n  Sample features (first PR):\")\n",
    "for col in X_train.columns[:5]:\n",
    "    print(f\"    {col}: {X_train[col].iloc[0]:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32abb9cf",
   "metadata": {},
   "source": [
    "## 3. Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a56b6859",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load synthetic training data\n",
    "with open(DATA_DIR / 'train_data.json', 'r') as f:\n",
    "    train_raw = json.load(f)\n",
    "\n",
    "with open(DATA_DIR / 'val_data.json', 'r') as f:\n",
    "    val_raw = json.load(f)\n",
    "\n",
    "print(f\"âœ“ Data loaded\")\n",
    "print(f\"  - Training samples: {len(train_raw)}\")\n",
    "print(f\"  - Validation samples: {len(val_raw)}\")\n",
    "\n",
    "# Check distribution\n",
    "train_labels = [d['labels']['risk_level'] for d in train_raw]\n",
    "print(f\"\\n  Risk level distribution:\")\n",
    "print(f\"    Low: {train_labels.count('low')} ({train_labels.count('low')/len(train_labels)*100:.1f}%)\")\n",
    "print(f\"    Medium: {train_labels.count('medium')} ({train_labels.count('medium')/len(train_labels)*100:.1f}%)\")\n",
    "print(f\"    High: {train_labels.count('high')} ({train_labels.count('high')/len(train_labels)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17cb72b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# âš ï¸ CONFIGURATION: Set your OpenAI API key if using embeddings\n",
    "# NEVER commit this to version control! Use environment variables in production.\n",
    "OPENAI_API_KEY = None  # Set to your key or use: os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "# Paths\n",
    "DATA_DIR = Path('data')\n",
    "MODEL_DIR = Path('models')\n",
    "MODEL_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# Model configuration\n",
    "RANDOM_STATE = 42\n",
    "TEST_SIZE = 0.2\n",
    "USE_EMBEDDINGS = False  # Set to True if using historical similarity features\n",
    "\n",
    "print(f\"âœ“ Configuration loaded\")\n",
    "print(f\"  - Random state: {RANDOM_STATE}\")\n",
    "print(f\"  - Test size: {TEST_SIZE}\")\n",
    "print(f\"  - Embeddings: {'Enabled' if USE_EMBEDDINGS else 'Disabled'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e1e6ad9",
   "metadata": {},
   "source": [
    "## 2. Configuration and Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec83e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core libraries\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "\n",
    "# Machine learning\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, \n",
    "    f1_score, roc_auc_score, roc_curve, confusion_matrix\n",
    ")\n",
    "import joblib\n",
    "\n",
    "# Explainability\n",
    "import shap\n",
    "\n",
    "# Optional: Advanced models\n",
    "try:\n",
    "    import lightgbm as lgb\n",
    "    LIGHTGBM_AVAILABLE = True\n",
    "except ImportError:\n",
    "    LIGHTGBM_AVAILABLE = False\n",
    "    print(\"LightGBM not installed. Only logistic regression will be available.\")\n",
    "\n",
    "# Optional: OpenAI for embeddings\n",
    "try:\n",
    "    from openai import OpenAI\n",
    "    OPENAI_AVAILABLE = True\n",
    "except ImportError:\n",
    "    OPENAI_AVAILABLE = False\n",
    "    print(\"OpenAI not installed. Historical similarity features will be disabled.\")\n",
    "\n",
    "# Feature extractor\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from feature_extractor import PRFeatureExtractor\n",
    "\n",
    "# Configuration\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(\"âœ“ Libraries imported successfully\")\n",
    "print(f\"  - LightGBM: {'Available' if LIGHTGBM_AVAILABLE else 'Not installed'}\")\n",
    "print(f\"  - OpenAI: {'Available' if OPENAI_AVAILABLE else 'Not installed'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a73065ab",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "783afc31",
   "metadata": {},
   "source": [
    "# Code Risk Radar - Baseline Model Training\n",
    "\n",
    "**Objective**: Train an explainable risk prediction model for Bitbucket pull requests.\n",
    "\n",
    "**Approach**: \n",
    "- **Recommended**: Logistic Regression (highly interpretable, portable to JavaScript)\n",
    "- **Alternative**: LightGBM (better accuracy, still explainable via SHAP)\n",
    "\n",
    "**Key Features**:\n",
    "- 6 risk axes: Complexity, Bug Probability, Security, Coupling, Volatility, Change Surface\n",
    "- 39 engineered features total\n",
    "- SHAP values for individual prediction explanations\n",
    "- Deployable to Forge Functions or FastAPI microservice"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
