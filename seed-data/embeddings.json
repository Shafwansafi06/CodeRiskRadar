[
  {
    "pr_id": "e1dfca39-7b29-426d-89b1-3f0da03a0eed",
    "content": "TITLE: [AMORO-3974] Migrate JUnit 4 to JUnit 5 in amoro-common module",
    "section": "title",
    "embedding": "[-0.01513546,-0.04517699,-0.0070454753,0.063077815,0.06874097,0.020392463,0.02949871,0.0039356463,-0.00085899595,-0.012641237,-0.0021811582,0.034866765,-0.055719092,-0.012956193,-0.021303138,0.036766432,0.0036542895,-0.03615022,0.06296968,-0.013994184,0.012515226,0.03653774,-0.023541113,0.037786085,-0.040838405,-0.013683828,0.026426656,0.016883945,-0.076531045,-0.02468758,0.0238888"
  },
  {
    "pr_id": "e1dfca39-7b29-426d-89b1-3f0da03a0eed",
    "content": "TITLE: [AMORO-3974] Migrate JUnit 4 to JUnit 5 in amoro-common module\n\nBODY:\n## Why are the changes needed?\r\n\r\nJUnit 4 is no longer actively maintained, and JUnit 5 provides a more modern and powerful testing framework. \r\nThis PR migrates all JUnit 4 tests in the `amoro-common` module to JUnit 5, enabling the use of the latest testing framework and reducing technical debt.\r\n\r\nClose #3974.\r\n\r\n## Brief change log\r\n\r\n### JUnit 4 → JUnit 5 Migration for amoro-common Module\r\n\r\n**Migrated Test Files (9 files):**\r\n\r\n1. `AmoroRunListener.java` - Converted `RunListener` → `TestExecutionListener`\r\n2. `TestServerTableDescriptor.java` - Converted `@Rule`, `@ClassRule` → `@BeforeAll`/`@AfterAll` and `@TempDir`\r\n3. `MemorySizeTest.java` - Converted `@Test(expected=...)` → `assertThrows()`\r\n4. `JacksonUtilTest.java`\r\n5. `TestSimpleFuture.java`\r\n6. `TestAmoroCatalogBase.java`\r\n7. `AmoroCatalogTestBase.java`\r\n8. `TestPoolConfig.java`\r\n9. `MockZookeeperServer.java`\r\n\r\n**Key Changes:**\r\n\r\n- Import changes: `org.junit.*` → `org.junit.jupiter.api.*`\r\n- Annotation changes: `@Before/After` → `@BeforeEach/AfterEach`, `@BeforeClass/AfterClass` → `@BeforeAll/AfterAll` (static)\r\n- Assertions changes: `Assert.*` → `Assertions.*`\r\n- Exception testing: `@Test(expected=...)` → `Assertions.assertThrows()`\r\n- Rules → Extensions: `@Rule` → `@RegisterExtension` or `@TempDir`\r\n- RunListener → TestExecutionListener: `AmoroRunListener` conversion\r\n\r\n## How was this patch tested?\r\n\r\n- [x] All migrated test files executed and passed\r\n- [x] Full test suite for `amoro-common` module: `./mvnw test -pl amoro-common`\r\n- [x] Full build verification: `./mvnw clean package -DskipTests`\r\n\r\n## Documentation\r\n\r\n- Does this pull request introduce a new feature? (no)\r\n\r\n- If yes, how is the feature documented? (not applicable)",
    "section": "full",
    "embedding": "[-0.0025235063,-0.032610513,-0.015230606,0.03227866,0.068431415,0.008438177,0.049009297,0.017121913,-0.0210434,-0.032708477,-0.011574295,0.00936594,-0.06331983,-0.0025691611,-0.03081686,0.057320584,0.05337819,-0.024815517,0.04063658,-0.006625002,0.0014074136,0.023940781,-0.014118238,0.038712498,0.0010594117,0.0044707833,0.016970206,0.020490622,-0.07445736,-0.038836658,0.022809701,-"
  },
  {
    "pr_id": "e1dfca39-7b29-426d-89b1-3f0da03a0eed",
    "content": "",
    "section": "prose",
    "embedding": "[0.012244709,0.039513454,-0.007785414,0.0021320505,0.046604965,0.012159259,0.03977907,0.04207942,-0.020763515,-0.039035235,0.024283497,-0.00805646,-0.04972243,0.017127944,-0.009930698,0.038848016,0.008790611,0.0133490125,-0.017834641,0.0071114884,0.030171363,-0.022801451,0.031216213,-0.00092527375,0.025564943,-0.024615467,0.027907416,0.0069565354,-0.02231851,0.03557899,0.019480716,"
  },
  {
    "pr_id": "e1dfca39-7b29-426d-89b1-3f0da03a0eed",
    "content": "## Why are the changes needed?\r\n\r\nJUnit 4 is no longer actively maintained, and JUnit 5 provides a more modern and powerful testing framework. \r\nThis PR migrates all JUnit 4 tests in the `amoro-common` module to JUnit 5, enabling the use of the latest testing framework and reducing technical debt.\r\n\r\nClose #3974.\r\n\r\n## Brief change log\r\n\r\n### JUnit 4 → JUnit 5 Migration for amoro-common Module\r\n\r\n**Migrated Test Files (9 files):**\r\n\r\n1. `AmoroRunListener.java` - Converted `RunListener` → `TestExecutionListener`\r\n2. `TestServerTableDescriptor.java` - Converted `@Rule`, `@ClassRule` → `@BeforeAll`/`@AfterAll` and `@TempDir`\r\n3. `MemorySizeTest.java` - Converted `@Test(expected=...)` → `assertThrows()`\r\n4. `JacksonUtilTest.java`\r\n5. `TestSimpleFuture.java`\r\n6. `TestAmoroCatalogBase.java`\r\n7. `AmoroCatalogTestBase.java`\r\n8. `TestPoolConfig.java`\r\n9. `MockZookeeperServer.java`\r\n\r\n**Key Changes:**\r\n\r\n- Import changes: `org.junit.*` → `org.junit.jupiter.api.*`\r\n- Annotation changes: `@Before/After` → `@BeforeEach/AfterEach`, `@BeforeClass/AfterClass` → `@BeforeAll/AfterAll` (static)\r\n- Assertions changes: `Assert.*` → `Assertions.*`\r\n- Exception testing: `@Test(expected=...)` → `Assertions.assertThrows()`\r\n- Rules → Extensions: `@Rule` → `@RegisterExtension` or `@TempDir`\r\n- RunListener → TestExecutionListener: `AmoroRunListener` conversion\r\n\r\n## How was this patch tested?\r\n\r\n- [x] All migrated test files executed and passed\r\n- [x] Full test suite for `amoro-common` module: `./mvnw test -pl amoro-common`\r\n- [x] Full build verification: `./mvnw clean package -DskipTests`\r\n\r\n## Documentation\r\n\r\n- Does this pull request introduce a new feature? (no)\r\n\r\n- If yes, how is the feature documented? (not applicable)",
    "section": "prose",
    "embedding": "[0.008033306,-0.04908621,0.0027154956,0.03631702,0.058314912,-0.0076055117,0.06271699,0.027139349,-0.019305544,-0.043937143,0.0008757455,0.026833603,-0.065811194,0.0032774122,-0.022552006,0.04567321,0.03333725,-0.02735123,0.059232857,-0.00974168,0.0036187703,0.017429167,-0.021041255,0.047254603,-0.020067547,-0.03260748,0.019849464,0.021705125,-0.07308285,-0.028870942,0.0068196305,-"
  },
  {
    "pr_id": "e1dfca39-7b29-426d-89b1-3f0da03a0eed",
    "content": "@@ -1,21 +1,21 @@\n <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n <!--\n-  ~ Licensed to the Apache Software Foundation (ASF) under one\n-  ~ or more contributor license agreements.  See the NOTICE file\n-  ~ distributed with this work for additional information\n-  ~ regarding copyright ownership.  The ASF licenses this file\n-  ~ to you under the Apache License, Version 2.0 (the\n-  ~ \"License\"); you may not use this file except in compliance\n-  ~ with the License.  You may obtain a copy of the License at\n-  ~\n-  ~     http://www.apache.org/licenses/LICENSE-2.0\n-  ~\n-  ~ Unless required by applicable law or agreed to in writing, software\n-  ~ distributed under the License is distributed on an \"AS IS\" BASIS,\n-  ~ WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-  ~ See the License for the specific language governing permissions and\n-  ~ limitations under the License.\n-  -->\n+~ Licensed to the Apache Software Foundation (ASF) under one\n+~ or more contributor license agreements.  See the NOTICE file\n+~ distributed with this work for additional information\n+~ regarding copyright ownership.  The ASF licenses this file\n+~ to you under the Apache License, Version 2.0 (the\n+~ \"License\"); you may not use this file except in compliance\n+~ with the License.  You may obtain a copy of the License at\n+~\n+~     http://www.apache.org/licenses/LICENSE-2.0\n+~\n+~ Unless required by applicable law or agreed to in writing, software\n+~ distributed under the License is distributed on an \"AS IS\" BASIS,\n+~ WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+~ See the License for the specific language governing permissions and\n+~ limitations under the License.\n+-->\n <project xmlns=\"http://maven.apache.org/POM/4.0.0\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\n          xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\">\n     <modelVersion>4.0.0</modelVersion>",
    "section": "code",
    "embedding": "[0.012919112,0.00683724,0.0013981204,0.01750114,0.057679825,-0.031837247,0.07366822,-0.005737462,0.009889193,-0.0108242845,-0.026628844,-0.0027576168,-0.0702408,0.06703455,-0.0036060906,0.053498104,0.039757464,-0.0016847259,0.019776266,-0.010803703,0.003199965,0.00398507,0.01165883,0.011647605,-0.04883352,0.045482583,0.02304892,0.028472563,-0.057019204,-0.031348623,0.04669169,-0.01"
  },
  {
    "pr_id": "e1dfca39-7b29-426d-89b1-3f0da03a0eed",
    "content": "@@ -159,6 +159,12 @@\n             <classifier>core</classifier>\n             <scope>test</scope>\n         </dependency>\n+\n+        <dependency>\n+            <groupId>org.junit.platform</groupId>\n+            <artifactId>junit-platform-launcher</artifactId>\n+            <scope>test</scope>\n+        </dependency>\n     </dependencies>\n \n     <build>",
    "section": "code",
    "embedding": "[-0.025941122,-0.021198332,0.0058860458,-0.029311625,0.06325839,0.02183174,0.034429062,-0.027630782,-0.017799659,-0.020694708,-0.009469641,-0.0003751268,-0.041003555,0.026186328,-0.010129595,0.06845507,0.051095273,-0.00024920065,-0.0148069225,0.003030025,0.029097948,0.0131133115,0.008820659,0.030209169,-0.0122087775,-0.013756842,0.060859554,0.011926341,-0.048041265,0.0031934432,0.0"
  },
  {
    "pr_id": "e1dfca39-7b29-426d-89b1-3f0da03a0eed",
    "content": "@@ -22,7 +22,7 @@\n import org.apache.amoro.shade.zookeeper3.org.apache.curator.framework.CuratorFrameworkFactory;\n import org.apache.amoro.shade.zookeeper3.org.apache.curator.retry.ExponentialBackoffRetry;\n import org.apache.curator.test.TestingServer;\n-import org.junit.Test;\n+import org.junit.jupiter.api.Test;\n \n import java.io.IOException;\n import java.util.Random;",
    "section": "code",
    "embedding": "[-0.0041569937,-0.018038865,-0.035656307,0.03129617,0.05905246,-0.007089816,0.058182616,0.023021238,-0.0445523,-0.048900615,0.014872921,-0.010403447,-0.05481939,0.011324635,-0.0032312283,0.024867367,0.047871113,-0.004061062,0.031078724,0.005073919,0.0035909954,-0.0015958478,-0.00867111,0.0033601609,-0.024759268,-0.022358647,-0.00026743248,-0.005975843,-0.05831553,-0.001959636,0.036"
  },
  {
    "pr_id": "e1dfca39-7b29-426d-89b1-3f0da03a0eed",
    "content": "@@ -64,7 +64,7 @@ private static void init() throws Exception {\n   }\n \n   @Test\n-  public void testFoobar() throws Exception {\n+  void testFoobar() throws Exception {\n     System.out.println(\"client: \" + client);\n     getClient().create().forPath(\"/test\", \"test-data\".getBytes());\n ",
    "section": "code",
    "embedding": "[-0.0019091681,-0.0013792753,0.004669313,-0.021599703,0.05200658,0.009757413,0.031736348,0.031887185,-0.024812954,-0.0362683,0.011490713,0.02041303,-0.06973077,0.0067648557,0.04476317,0.04952921,0.063874364,-0.0038906715,0.031189157,0.014701553,-0.013674427,0.002378681,-0.025349105,0.019119097,0.024961516,-0.010361773,0.032348987,0.024442695,-0.04677514,0.0014036428,-0.00035284433,"
  },
  {
    "pr_id": "e1dfca39-7b29-426d-89b1-3f0da03a0eed",
    "content": "@@ -18,40 +18,40 @@\n \n package org.apache.amoro.client;\n \n-import org.junit.Assert;\n-import org.junit.Test;\n+import org.junit.jupiter.api.Assertions;\n+import org.junit.jupiter.api.Test;\n \n public class TestPoolConfig {\n \n   @Test\n-  public void testConstructFromUrl() {\n+  void testConstructFromUrl() {\n     String url =\n         \"thrift://127.0.0.1:1261?connectTimeout=5000&socketTimeout=6000&maxMessageSize=100&autoReconnect=false\"\n             + \"&maxReconnects=3&minIdle=10&maxIdle=10&maxWaitMillis=500\";\n     PoolConfig<?> poolConfig = PoolConfig.forUrl(url);\n \n-    Assert.assertEquals(5000, poolConfig.getConnectTimeout());\n-    Assert.assertEquals(6000, poolConfig.getSocketTimeout());\n-    Assert.assertEquals(100, poolConfig.getMaxMessageSize());\n-    Assert.assertFalse(poolConfig.isAutoReconnect());\n-    Assert.assertEquals(3, poolConfig.getMaxReconnects());\n-    Assert.assertEquals(10, poolConfig.getMinIdle());\n-    Assert.assertEquals(10, poolConfig.getMaxIdle());\n-    Assert.assertEquals(500, poolConfig.getMaxWaitMillis());\n+    Assertions.assertEquals(5000, poolConfig.getConnectTimeout());\n+    Assertions.assertEquals(6000, poolConfig.getSocketTimeout());\n+    Assertions.assertEquals(100, poolConfig.getMaxMessageSize());\n+    Assertions.assertFalse(poolConfig.isAutoReconnect());\n+    Assertions.assertEquals(3, poolConfig.getMaxReconnects());\n+    Assertions.assertEquals(10, poolConfig.getMinIdle());\n+    Assertions.assertEquals(10, poolConfig.getMaxIdle());\n+    Assertions.assertEquals(500, poolConfig.getMaxWaitMillis());\n   }\n \n   @Test\n-  public void tetUrlParameterNameError() {\n+  void tetUrlParameterNameError() {\n     // We will ignore parameters with unknown name\n     String url = \"thrift://127.0.0.1:1261?connectTimeouts=300\";\n     PoolConfig<?> poolConfig = PoolConfig.forUrl(url);\n-    Assert.assertEquals(0, poolConfig.getConnectTimeout());\n+    Assertions.assertEquals(0, poolConfig.getConnectTimeout());\n   }\n \n   @Test\n-  public void testUrlFormatError() {\n+  void testUrlFormatError() {\n     String url = \"thrift://127.0.0.1:1261?connectTimeout=5000& \";\n-    Assert.assertThrows(\n+    Assertions.assertThrows(\n         IllegalArgumentException.class,\n         () -> {\n           PoolConfig.forUrl(url);",
    "section": "code",
    "embedding": "[-0.011022978,-0.025935244,-0.01863021,0.035289835,0.04813571,0.0295613,0.027248798,0.03515444,-0.027414076,-0.05357903,0.007093819,0.028469253,-0.0737612,0.005912067,-0.0051049283,0.032333534,0.047352575,-0.008867633,0.051541813,-0.015667265,0.0015258831,-0.002081926,-0.031065479,0.017361358,0.014913443,-0.00891752,0.020425599,0.020210678,-0.045476496,-0.030370733,0.024580313,-0.0"
  },
  {
    "pr_id": "c54f1503-8c1f-4886-bf46-fedb8f14f1bb",
    "content": "CI failures are unrelated.",
    "section": "comment",
    "embedding": "[0.02154353,-0.024860274,-0.038047113,0.068454824,0.014537124,-0.0054852418,0.03887525,0.0221503,0.0071902163,-0.053322814,0.01499101,0.016616754,-0.08114981,0.06818701,-0.0001585866,0.055712193,0.067874186,-0.004910537,-0.0038647656,0.00259425,-0.011628784,0.029880995,0.010012223,0.018374499,0.014514966,0.015941117,0.058807377,-0.015298741,-0.06691764,0.0008635054,0.06162765,0.032"
  },
  {
    "pr_id": "e1dfca39-7b29-426d-89b1-3f0da03a0eed",
    "content": "@@ -19,16 +19,16 @@\n package org.apache.amoro.formats;\n \n import org.apache.amoro.AmoroCatalog;\n-import org.junit.After;\n-import org.junit.Before;\n-import org.junit.Rule;\n-import org.junit.rules.TemporaryFolder;\n+import org.junit.jupiter.api.AfterEach;\n+import org.junit.jupiter.api.BeforeEach;\n+import org.junit.jupiter.api.io.TempDir;\n \n import java.io.IOException;\n+import java.nio.file.Path;\n \n public abstract class AmoroCatalogTestBase {\n \n-  @Rule public TemporaryFolder temp = new TemporaryFolder();\n+  @TempDir Path tempDir;\n \n   protected AmoroCatalogTestHelper<?> catalogTestHelper;\n ",
    "section": "code",
    "embedding": "[-0.018223885,0.009290361,-0.033130445,0.040267464,0.061694723,-0.007049537,0.050365675,0.017511453,-0.02458306,-0.056195885,-0.010679262,0.009765585,-0.035651416,0.011072189,0.018297628,0.05083533,0.04055349,-0.031674672,0.048683144,-0.03234101,0.00557732,-0.010809965,-0.031888038,0.02037005,-0.0075682774,-0.013779803,0.030009374,0.021481453,-0.040793873,-0.00015883202,-0.01391997"
  },
  {
    "pr_id": "e1dfca39-7b29-426d-89b1-3f0da03a0eed",
    "content": "@@ -40,16 +40,16 @@ public AmoroCatalogTestBase(AmoroCatalogTestHelper<?> catalogTestHelper) {\n     this.catalogTestHelper = catalogTestHelper;\n   }\n \n-  @Before\n-  public void setupCatalog() throws IOException {\n-    String path = temp.newFolder().getPath();\n+  @BeforeEach\n+  void setupCatalog() throws IOException {\n+    String path = tempDir.toFile().getAbsolutePath();\n     catalogTestHelper.initWarehouse(path);\n     this.amoroCatalog = catalogTestHelper.amoroCatalog();\n     this.originalCatalog = catalogTestHelper.originalCatalog();\n   }\n \n-  @After\n-  public void cleanCatalog() {\n+  @AfterEach\n+  void cleanCatalog() {\n     catalogTestHelper.clean();\n   }\n }",
    "section": "code",
    "embedding": "[0.0127580315,0.01257174,0.004545849,0.030211462,0.046682283,-0.0053626443,0.007823625,0.0012773557,-0.024133256,-0.026207522,0.004608294,0.0028111252,-0.023607008,0.003421755,0.026654068,0.07431067,0.016648306,-0.024761504,0.049350828,-0.033067975,0.010468672,-0.026650766,-0.048196457,0.04265329,-0.0013337344,0.012689894,0.015684705,0.008938675,-0.058868352,-0.004522462,0.01672393"
  },
  {
    "pr_id": "e1dfca39-7b29-426d-89b1-3f0da03a0eed",
    "content": "@@ -21,8 +21,8 @@\n import org.apache.amoro.AmoroTable;\n import org.apache.amoro.shade.guava32.com.google.common.collect.Sets;\n import org.apache.amoro.table.TableIdentifier;\n-import org.junit.Assert;\n-import org.junit.Test;\n+import org.junit.jupiter.api.Assertions;\n+import org.junit.jupiter.api.Test;\n \n import java.util.HashMap;\n import java.util.HashSet;",
    "section": "code",
    "embedding": "[-0.027315838,0.007871219,-0.020228392,0.005078732,0.051727258,0.008171296,0.025302952,0.011770569,-0.018419305,-0.029713962,0.0019289181,0.008715072,-0.06825023,0.035023652,-0.024245257,0.039633807,0.05151934,0.004541915,0.03393984,-0.021083012,-1.7471904e-06,0.002061814,0.0015971027,0.016975041,-0.002251651,-0.03139448,0.042621117,0.001986116,-0.059703335,-0.013486589,0.021690961"
  },
  {
    "pr_id": "e1dfca39-7b29-426d-89b1-3f0da03a0eed",
    "content": "@@ -49,54 +49,54 @@ protected abstract void createTable(\n   protected abstract List<String> listDatabases();\n \n   @Test\n-  public void testListDatabases() {\n+  void testListDatabases() {\n     createDatabase(DB1);\n     createDatabase(DB2);\n     createDatabase(DB3);\n     HashSet<String> databases = Sets.newHashSet(amoroCatalog.listDatabases());\n-    Assert.assertTrue(databases.contains(DB1));\n-    Assert.assertTrue(databases.contains(DB2));\n-    Assert.assertTrue(databases.contains(DB3));\n+    Assertions.assertTrue(databases.contains(DB1));\n+    Assertions.assertTrue(databases.contains(DB2));\n+    Assertions.assertTrue(databases.contains(DB3));\n   }\n \n   @Test\n-  public void testDropDatabases() {\n+  void testDropDatabases() {\n     createDatabase(DB1);\n     amoroCatalog.dropDatabase(DB1);\n \n-    Assert.assertFalse(amoroCatalog.listDatabases().contains(DB1));\n+    Assertions.assertFalse(amoroCatalog.listDatabases().contains(DB1));\n   }\n \n   @Test\n-  public void testCreateDatabases() {\n+  void testCreateDatabases() {\n     amoroCatalog.createDatabase(DB1);\n-    Assert.assertTrue(listDatabases().contains(DB1));\n+    Assertions.assertTrue(listDatabases().contains(DB1));\n   }\n \n   @Test\n-  public void testExistsDatabase() {\n+  void testExistsDatabase() {\n     createDatabase(DB1);\n-    Assert.assertTrue(amoroCatalog.databaseExists(DB1));\n+    Assertions.assertTrue(amoroCatalog.databaseExists(DB1));\n   }\n \n   @Test\n-  public void testExistsTable() {\n+  void testExistsTable() {\n     createDatabase(DB1);\n     createTable(DB1, TABLE, new HashMap<>());\n-    Assert.assertTrue(amoroCatalog.tableExists(DB1, TABLE));\n+    Assertions.assertTrue(amoroCatalog.tableExists(DB1, TABLE));\n   }\n \n   @Test\n-  public void testLoadTable() {\n+  void testLoadTable() {\n     createDatabase(DB1);\n     Map<String, String> properties = new HashMap<>();\n     properties.put(\"key1\", \"value1\");\n     createTable(DB1, TABLE, properties);\n     AmoroTable<?> amoroTable = amoroCatalog.loadTable(DB1, TABLE);\n-    Assert.assertEquals(amoroTable.properties().get(\"key1\"), \"value1\");\n-    Assert.assertEquals(\n-        amoroTable.name(), catalogTestHelper.catalogName() + \".\" + DB1 + \".\" + TABLE);\n-    Assert.assertEquals(\n-        amoroTable.id(), TableIdentifier.of(catalogTestHelper.catalogName(), DB1, TABLE));\n+    Assertions.assertEquals(\"value1\", amoroTable.properties().get(\"key1\"));\n+    Assertions.assertEquals(\n+        catalogTestHelper.catalogName() + \".\" + DB1 + \".\" + TABLE, amoroTable.name());\n+    Assertions.assertEquals(\n+        TableIdentifier.of(catalogTestHelper.catalogName(), DB1, TABLE), amoroTable.id());\n   }\n }",
    "section": "code",
    "embedding": "[0.00054859376,0.017878346,-0.024997976,0.041162506,0.07300466,0.008764328,0.02299205,-0.021162642,-0.043494277,-0.033768807,-0.0016075661,0.012403905,-0.07991183,0.023903098,0.017266339,0.037362702,0.033918604,-0.003951288,0.065197736,-0.031646784,-0.020965777,0.012276876,0.005418461,0.035672605,0.022929648,0.015812192,0.049703702,-0.0048649227,-0.04178182,-0.036941063,0.01661559,"
  },
  {
    "pr_id": "e1dfca39-7b29-426d-89b1-3f0da03a0eed",
    "content": "@@ -18,74 +18,82 @@\n \n package org.apache.amoro.listener;\n \n-import org.junit.Assert;\n-import org.junit.Ignore;\n-import org.junit.runner.Description;\n-import org.junit.runner.Result;\n-import org.junit.runner.notification.Failure;\n-import org.junit.runner.notification.RunListener;\n+import org.junit.jupiter.api.Assertions;\n+import org.junit.platform.engine.TestExecutionResult;\n+import org.junit.platform.launcher.TestExecutionListener;\n+import org.junit.platform.launcher.TestIdentifier;\n+import org.junit.platform.launcher.TestPlan;\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n \n+import java.util.Optional;\n import java.util.PriorityQueue;\n \n-public class AmoroRunListener extends RunListener {\n+public class AmoroRunListener implements TestExecutionListener {\n   private static final Logger LOG = LoggerFactory.getLogger(AmoroRunListener.class);\n   private long startTime;\n   private long singleTestStartTime;\n \n   private final PriorityQueue<TestCase> testCaseQueue = new PriorityQueue<>();\n \n   @Override\n-  public void testRunStarted(Description description) {\n+  public void testPlanExecutionStarted(TestPlan testPlan) {\n     startTime = System.currentTimeMillis();\n-    LOG.info(\n-        \"{} Tests started! Number of Test case: {}\",\n-        description == null ? \"Unknown\" : description.getClassName(),\n-        description == null ? 0 : description.testCount());\n+    int testCount = (int) testPlan.countTestIdentifiers(TestIdentifier::isTest);\n+    LOG.info(\"Tests started! Number of Test case: {}\", testCount);\n   }\n \n   @Override\n-  public void testRunFinished(Result result) {\n+  public void testPlanExecutionFinished(TestPlan testPlan) {\n     long endTime = System.currentTimeMillis();\n-    LOG.info(\"Tests finished! Number of test case: {}\", result.getRunCount());\n+    int testCount = (int) testPlan.countTestIdentifiers(TestIdentifier::isTest);\n+    LOG.info(\"Tests finished! Number of test case: {}\", testCount);\n     long elapsedSeconds = (endTime - startTime) / 1000;\n     LOG.info(\"Elapsed time of tests execution: {} seconds\", elapsedSeconds);\n     int printNum = Math.min(testCaseQueue.size(), 50);\n     LOG.info(\"Print the top cost test case method name:\");\n     for (int i = 0; i < printNum; i++) {\n       TestCase testCase = testCaseQueue.poll();\n-      Assert.assertNotNull(testCase);\n+      Assertions.assertNotNull(testCase);\n       LOG.info(\"NO-{}, cost: {}ms, methodName:{}\", i + 1, testCase.cost, testCase.methodName);\n     }\n   }\n \n   @Override\n-  public void testStarted(Description description) {\n-    singleTestStartTime = System.currentTimeMillis();\n-    LOG.info(\"{} test is starting...\", description.getMethodName());\n+  public void executionStarted(TestIdentifier testIdentifier) {\n+    if (testIdentifier.isTest()) {\n+      singleTestStartTime = System.currentTimeMillis();\n+      LOG.info(\"{} test is starting...\", testIdentifier.getDisplayName());\n+    }\n   }\n \n   @Override\n-  public void testFinished(Description description) {\n-    long cost = System.currentTimeMillis() - singleTestStartTime;\n-    testCaseQueue.add(TestCase.of(cost, description.getMethodName()));\n-    LOG.info(\"{} test is finished, cost {}ms...\\n\", description.getMethodName(), cost);\n-  }\n+  public void executionFinished(\n+      TestIdentifier testIdentifier, TestExecutionResult testExecutionResult) {\n+    if (testIdentifier.isTest()) {\n+      long cost = System.currentTimeMillis() - singleTestStartTime;\n+      testCaseQueue.add(TestCase.of(cost, testIdentifier.getDisplayName()));\n+      LOG.info(\"{} test is finished, cost {}ms...\\n\", testIdentifier.getDisplayName(), cost);\n \n-  @Override\n-  public void testFailure(Failure failure) {\n-    LOG.info(\"{} test FAILED!!!\", failure.getDescription().getMethodName());\n+      if (testExecutionResult.getStatus() == TestExecutionResult.Status.FAILED) {\n+        LOG.info(\"{} test FAILED!!!\", testIdentifier.getDisplayName());\n+        Optional<Throwable> throwable = testExecutionResult.getThrowable();\n+        if (throwable.isPresent()) {\n+          LOG.info(\"Failure reason: {}\", throwable.get().getMessage());\n+        }\n+      }\n+    }\n   }\n \n   @Override\n-  public void testIgnored(Description description) throws Exception {\n-    super.testIgnored(description);\n-    Ignore ignore = description.getAnnotation(Ignore.class);\n-    LOG.info(\n-        \"@Ignore test method '{}', ignored reason '{}'.\",\n-        description.getMethodName(),\n-        ignore.value());\n+  public void executionSkipped(TestIdentifier testIdentifier, String reason) {\n+    if (testIdentifier.isTest()) {\n+      String ignoredReason = reason != null && !reason.isEmpty() ? reason : \"No reason provided\";\n+      LOG.info(\n+          \"@Disabled method '{}', ignored reason '{}'.\",\n+          testIdentifier.getDisplayName(),\n+          ignoredReason);\n+    }\n   }\n \n   private static class TestCase implements Comparable<TestCase> {",
    "section": "code",
    "embedding": "[-0.013598368,-0.0047834385,-0.01975754,-0.0009505711,0.05189866,0.025261063,0.05848839,0.021354724,-0.04590898,-0.057157602,0.017184962,0.042675614,-0.05624739,0.007895394,0.0139928935,0.028974341,0.070904404,-0.013875047,0.0354495,-0.008432174,0.024834488,0.0254253,-0.025499286,0.05290571,0.0074446597,-0.031782474,0.028706823,0.034876652,-0.06363575,0.00088560197,-0.002359201,-0."
  },
  {
    "pr_id": "e1dfca39-7b29-426d-89b1-3f0da03a0eed",
    "content": "@@ -103,7 +111,7 @@ public static TestCase of(long cost, String methodName) {\n \n     @Override\n     public int compareTo(AmoroRunListener.TestCase that) {\n-      Assert.assertNotNull(that);\n+      Assertions.assertNotNull(that);\n       return that.cost.compareTo(cost);\n     }\n   }",
    "section": "code",
    "embedding": "[-0.035913203,-0.018889053,-0.03442332,0.008728493,0.042058926,0.05352273,0.0480968,-0.010947203,-0.01788481,-0.03550691,0.025334828,0.060023174,-0.043985467,0.022994507,-0.0053822515,0.05979325,0.06894112,-0.021272797,0.08257797,-0.0426105,-0.006329901,0.0031806002,-0.022002615,0.0294706,-0.03358325,0.011376664,0.03687449,0.011902448,-0.046718877,0.0006320035,0.073076345,0.0392349"
  },
  {
    "pr_id": "e1dfca39-7b29-426d-89b1-3f0da03a0eed",
    "content": "@@ -23,9 +23,9 @@\n import static org.mockito.Mockito.mock;\n \n import org.apache.amoro.exception.AmoroRuntimeException;\n-import org.junit.Assert;\n-import org.junit.Before;\n-import org.junit.Test;\n+import org.junit.jupiter.api.Assertions;\n+import org.junit.jupiter.api.BeforeEach;\n+import org.junit.jupiter.api.Test;\n \n import java.util.Arrays;\n import java.util.List;",
    "section": "code",
    "embedding": "[-0.039290365,0.013821464,-0.022460245,0.021563306,0.041611344,0.02382423,0.04253885,0.0226351,-0.020311002,-0.031317595,0.02498879,0.045447294,-0.07786948,-0.0057290513,-0.013194578,0.041292913,0.052782476,0.011126131,0.0493463,-0.040848617,0.002607142,-0.000384461,0.0118734455,0.009956768,-0.017009992,-0.030010134,0.038427435,1.6327265e-05,-0.05101782,-0.021053467,0.016326498,0.0"
  },
  {
    "pr_id": "e1dfca39-7b29-426d-89b1-3f0da03a0eed",
    "content": "@@ -43,8 +43,8 @@ private void resetCallbackData() {\n     calledFlag = new boolean[] {false, false, false, false, false};\n   }\n \n-  @Before\n-  public void setUp() {\n+  @BeforeEach\n+  void setUp() {\n     resetCallbackData();\n     simpleFuture = new SimpleFuture();\n     long threadId = Thread.currentThread().getId();",
    "section": "code",
    "embedding": "[-0.027162606,0.0030829534,0.023194296,-0.004786092,0.036454175,0.020324107,0.06969426,-0.02449192,-0.043289874,-0.03846089,0.012637664,0.0043857363,-0.07348769,-0.000100146506,-0.003620448,0.032563817,0.05615832,-0.020519774,0.05983557,0.027478937,-0.0071193953,-0.0018859238,-0.02354662,0.026150942,0.0029928354,-0.025885645,0.021006908,-0.018286956,-0.022328893,0.011642236,0.03365"
  },
  {
    "pr_id": "e1dfca39-7b29-426d-89b1-3f0da03a0eed",
    "content": "@@ -18,13 +18,13 @@\n \n package org.apache.amoro.utils;\n \n-import static org.junit.Assert.assertEquals;\n-import static org.junit.Assert.assertFalse;\n-import static org.junit.Assert.assertNull;\n-import static org.junit.Assert.assertTrue;\n+import static org.junit.jupiter.api.Assertions.assertEquals;\n+import static org.junit.jupiter.api.Assertions.assertFalse;\n+import static org.junit.jupiter.api.Assertions.assertNull;\n+import static org.junit.jupiter.api.Assertions.assertTrue;\n \n import org.apache.amoro.shade.jackson2.com.fasterxml.jackson.databind.JsonNode;\n-import org.junit.Test;\n+import org.junit.jupiter.api.Test;\n \n import java.util.ArrayList;\n import java.util.Collections;",
    "section": "code",
    "embedding": "[-0.031557422,0.0019283312,-0.012310878,0.032361183,0.03556101,0.035874225,0.06327757,0.0009343011,-0.04367914,-0.04173426,-0.005808011,0.023539385,-0.08289363,-0.0038781639,-0.0076557263,0.035755772,0.040431302,0.011611127,0.022137875,-0.007846444,0.0065599843,0.0035052414,-0.006686633,0.018698003,0.027038522,-0.028707989,0.049367383,0.008809148,-0.06713186,-0.0017467466,0.0068062"
  },
  {
    "pr_id": "e1dfca39-7b29-426d-89b1-3f0da03a0eed",
    "content": "@@ -36,7 +36,7 @@\n public class JacksonUtilTest {\n \n   @Test\n-  public void jacksonTest() {\n+  void jacksonTest() {\n \n     JsonTestBean testObject = new JsonTestBean();\n     testObject.setBoolValue(true);",
    "section": "code",
    "embedding": "[-0.049413737,-0.01741031,0.027386537,0.0076527633,0.016745508,0.0047436766,0.09958264,0.000643796,-0.025225936,-0.022303604,-0.0026858693,-0.021004748,-0.06684348,0.025856668,-0.021227933,0.047881786,0.04736254,0.012115237,0.04162413,-0.01350491,-0.0076451027,-0.021406053,-0.037620068,0.030295584,0.021388285,-0.026152452,0.053401835,0.009054943,-0.01284274,-0.04936186,-0.022466887"
  },
  {
    "pr_id": "b2e982f0-8b05-43bf-a510-04e8a37c4771",
    "content": "@@ -8080,9 +8080,6 @@ const docTemplate = `{\n                 \"id\": {\n                     \"type\": \"string\"\n                 },\n-                \"question_id\": {\n-                    \"type\": \"string\"\n-                },\n                 \"title\": {\n                     \"type\": \"string\"\n                 }",
    "section": "code",
    "embedding": "[-0.011664862,0.013352623,-0.0027825611,0.030724283,0.02828929,-0.017513614,0.036673192,0.0057914406,-0.019220103,-0.05684115,-0.018403444,0.011443841,-0.057029895,0.020217676,-0.00043362833,0.07508772,0.06825916,0.010836753,-0.0318932,-0.037352458,-0.0046796524,-0.027547594,0.011425718,-0.018306898,0.027835255,0.015519543,-0.005586376,-0.0044233995,-0.059642684,0.0236689,-0.022846"
  },
  {
    "pr_id": "e1dfca39-7b29-426d-89b1-3f0da03a0eed",
    "content": "@@ -53,186 +53,196 @@ public void setUp() {\n       simpleFuture.whenCompleted(\n           () -> {\n             for (int j = num; j < calledFlag.length; j++) {\n-              Assert.assertFalse(\n-                  \"callback \" + j + \" should not be called before \" + num, calledFlag[j]);\n+              Assertions.assertFalse(\n+                  calledFlag[j], \"callback \" + j + \" should not be called before \" + num);\n             }\n             // Trigger error if callbackNum[num] == 0\n             if (callbackNum[num] == 0) {\n               throw new RuntimeException(\"Callback error\");\n             }\n             callbackNum[num] = num;\n-            Assert.assertEquals(\n-                \"Callback should be run in the same thread\",\n+            Assertions.assertEquals(\n                 threadId,\n-                Thread.currentThread().getId());\n+                Thread.currentThread().getId(),\n+                \"Callback should be run in the same thread\");\n           });\n     }\n   }\n \n   @Test\n-  public void testComplete() {\n+  void testComplete() {\n     simpleFuture.complete();\n \n     for (int i = 0; i < 5; i++) {\n-      Assert.assertEquals(\"Current callback num: \" + i, i, callbackNum[i]);\n+      Assertions.assertEquals(i, callbackNum[i], \"Current callback num: \" + i);\n     }\n-    Assert.assertTrue(\"SimpleFuture should complete if callback no error\", simpleFuture.isDone());\n+    Assertions.assertTrue(\n+        simpleFuture.isDone(), \"SimpleFuture should complete if callback no error\");\n   }\n \n   @Test\n-  public void testCompleteTwiceButNotReset() {\n+  void testCompleteTwiceButNotReset() {\n     simpleFuture.complete();\n     // The second call will not trigger any callback\n     for (int i = 0; i < 5; i++) {\n       callbackNum[i] = -1;\n     }\n     simpleFuture.complete();\n     for (int i = 0; i < 5; i++) {\n-      Assert.assertEquals(\"Current callback num: \" + i, -1, callbackNum[i]);\n+      Assertions.assertEquals(-1, callbackNum[i], \"Current callback num: \" + i);\n     }\n-    Assert.assertTrue(\"SimpleFuture should not complete if callback error\", simpleFuture.isDone());\n+    Assertions.assertTrue(\n+        simpleFuture.isDone(), \"SimpleFuture should not complete if callback error\");\n   }\n \n   // Additional tests for edge cases and error conditions\n   @Test\n-  public void testCallbackError() {\n+  void testCallbackError() {\n     callbackNum[2] = 0; // Trigger error in callbackNum[2]\n \n     try {\n       simpleFuture.complete();\n     } catch (Throwable throwable) {\n-      Assert.assertTrue(\"Should catch the error\", throwable instanceof AmoroRuntimeException);\n-      Assert.assertTrue(\"Should catch the error\", throwable.getCause() instanceof RuntimeException);\n-      Assert.assertEquals(\n-          \"Should catch the error\", \"Callback error\", throwable.getCause().getMessage());\n+      Assertions.assertTrue(throwable instanceof AmoroRuntimeException, \"Should catch the error\");\n+      Assertions.assertTrue(\n+          throwable.getCause() instanceof RuntimeException, \"Should catch the error\");\n+      Assertions.assertEquals(\n+          \"Callback error\", throwable.getCause().getMessage(), \"Should catch the error\");\n     }\n     for (int i = 0; i < 5; i++) {\n       if (i < 2) {\n-        Assert.assertEquals(\"Current callback num: \" + i, i, callbackNum[i]);\n+        Assertions.assertEquals(i, callbackNum[i], \"Current callback num: \" + i);\n       } else if (i == 2) {\n-        Assert.assertEquals(\"Current callback num: \" + i, 0, callbackNum[i]);\n+        Assertions.assertEquals(0, callbackNum[i], \"Current callback num: \" + i);\n       } else {\n-        Assert.assertEquals(\"Current callback num: \" + i, -1, callbackNum[i]);\n+        Assertions.assertEquals(-1, callbackNum[i], \"Current callback num: \" + i);\n       }\n     }\n-    Assert.assertFalse(\"SimpleFuture should not complete if callback error\", simpleFuture.isDone());\n+    Assertions.assertFalse(\n+        simpleFuture.isDone(), \"SimpleFuture should not complete if callback error\");\n   }\n \n   @Test\n-  public void testReset() {\n+  void testReset() {\n     callbackNum[2] = 0; // Trigger error in callbackNum[2]\n     try {\n       simpleFuture.complete();\n     } catch (Throwable throwable) {\n-      Assert.assertTrue(\"Should catch the error\", throwable instanceof AmoroRuntimeException);\n+      Assertions.assertTrue(throwable instanceof AmoroRuntimeException, \"Should catch the error\");\n     }\n-    Assert.assertFalse(\"SimpleFuture should not complete if callback error\", simpleFuture.isDone());\n+    Assertions.assertFalse(\n+        simpleFuture.isDone(), \"SimpleFuture should not complete if callback error\");\n \n     resetCallbackData(); // Trigger normal callback\n     simpleFuture.reset();\n     simpleFuture.complete();\n     for (int i = 0; i < 5; i++) {\n-      Assert.assertEquals(\"Current callback num: \" + i, i, callbackNum[i]);\n+      Assertions.assertEquals(i, callbackNum[i], \"Current callback num: \" + i);\n     }\n-    Assert.assertTrue(\"SimpleFuture should not complete if callback error\", simpleFuture.isDone());\n+    Assertions.assertTrue(\n+        simpleFuture.isDone(), \"SimpleFuture should not complete if callback error\");\n   }\n \n   @Test\n-  public void testIsDone() {\n+  void testIsDone() {\n     simpleFuture.complete();\n-    Assert.assertTrue(\"Future should be completed\", simpleFuture.isDone());\n+    Assertions.assertTrue(simpleFuture.isDone(), \"Future should be completed\");\n   }\n \n-  @Test(expected = AmoroRuntimeException.class)\n-  public void testCompleteException() throws ExecutionException, InterruptedException {\n+  @Test\n+  void testCompleteException() throws ExecutionException, InterruptedException {\n     CompletableFuture<?> future = mock(CompletableFuture.class);\n     doReturn(true).when(future).complete(null);\n     doThrow(new RuntimeException()).when(future).get();\n     SimpleFuture simpleFuture = new SimpleFuture(future);\n \n-    simpleFuture.complete();\n+    Assertions.assertThrows(\n+        AmoroRuntimeException.class,\n+        () -> {\n+          simpleFuture.complete();\n+        });\n   }\n \n   @Test\n-  public void testJoin() {\n+  void testJoin() {\n     simpleFuture.complete();\n     simpleFuture.join();\n-    Assert.assertTrue(\"Future should be completed\", simpleFuture.isDone());\n+    Assertions.assertTrue(simpleFuture.isDone(), \"Future should be completed\");\n   }\n \n   @Test\n-  public void testJoinException() throws ExecutionException, InterruptedException {\n+  void testJoinException() throws ExecutionException, InterruptedException {\n     CompletableFuture<?> future = mock(CompletableFuture.class);\n     doThrow(new RuntimeException()).when(future).get();\n     SimpleFuture simpleFuture = new SimpleFuture(future);\n     simpleFuture.reset();\n     simpleFuture.complete();\n     simpleFuture.join();\n-    Assert.assertTrue(\"Future should be completed\", simpleFuture.isDone());\n+    Assertions.assertTrue(simpleFuture.isDone(), \"Future should be completed\");\n   }\n \n   @Test\n-  public void testOr() {\n+  void testOr() {\n     SimpleFuture anotherFuture = new SimpleFuture();\n     SimpleFuture combinedFuture = simpleFuture.or(anotherFuture);\n \n     simpleFuture.complete();\n-    Assert.assertTrue(\n-        \"Combined future should be completed when either future completes\",\n-        combinedFuture.isDone());\n+    Assertions.assertTrue(\n+        combinedFuture.isDone(),\n+        \"Combined future should be completed when either future completes\");\n   }\n \n   @Test\n-  public void testAnd() {\n+  void testAnd() {\n     SimpleFuture anotherFuture = new SimpleFuture();\n     SimpleFuture combinedFuture = simpleFuture.and(anotherFuture);\n \n     simpleFuture.complete();\n     anotherFuture.complete();\n-    Assert.assertTrue(\n-        \"Combined future should be completed when both futures complete\", combinedFuture.isDone());\n+    Assertions.assertTrue(\n+        combinedFuture.isDone(), \"Combined future should be completed when both futures complete\");\n   }\n \n   @Test\n-  public void testAllOf() {\n+  void testAllOf() {\n     List<SimpleFuture> futures =\n         Arrays.asList(new SimpleFuture(), new SimpleFuture(), new SimpleFuture());\n     SimpleFuture combinedFuture = SimpleFuture.allOf(futures);\n \n     futures.forEach(SimpleFuture::complete);\n-    Assert.assertTrue(\n-        \"Combined future should be completed when all futures complete\", combinedFuture.isDone());\n+    Assertions.assertTrue(\n+        combinedFuture.isDone(), \"Combined future should be completed when all futures complete\");\n   }\n \n   @Test\n-  public void testAnyOf() {\n+  void testAnyOf() {\n     List<SimpleFuture> futures = Arrays.asList(new SimpleFuture(), new SimpleFuture());\n     SimpleFuture combinedFuture = SimpleFuture.anyOf(futures);\n \n     futures.get(0).complete();\n-    Assert.assertTrue(\n-        \"Combined future should be completed when any future completes\", combinedFuture.isDone());\n+    Assertions.assertTrue(\n+        combinedFuture.isDone(), \"Combined future should be completed when any future completes\");\n   }\n \n   // Test for when the future is already completed before calling complete()\n   @Test\n-  public void testCompleteAlreadyCompleted() {\n+  void testCompleteAlreadyCompleted() {\n     simpleFuture.complete();\n     try {\n       simpleFuture.complete();\n     } catch (Throwable throwable) {\n-      Assert.fail(throwable.getMessage());\n+      Assertions.fail(throwable.getMessage());\n     }\n   }\n \n   // Test for when the future is already completed before calling join()\n   @Test\n-  public void testJoinAlreadyCompleted() {\n+  void testJoinAlreadyCompleted() {\n     simpleFuture.complete();\n     try {\n       simpleFuture.join();\n     } catch (Throwable throwable) {\n-      Assert.fail(throwable.getMessage());\n+      Assertions.fail(throwable.getMessage());\n     }\n   }\n }",
    "section": "code",
    "embedding": "[0.0071111694,-0.01311494,0.022690296,0.0090621635,0.04218424,0.030825071,0.065287784,-0.0064757587,-0.015921632,-0.03685182,0.014574417,0.024134537,-0.0634395,-0.028316509,0.006018562,0.056428578,0.06536498,-0.018099554,0.028698785,-0.007424785,-0.03698556,0.024202667,-0.0071301362,0.005759759,0.00706858,-0.0007122591,0.029414918,-0.043518562,-0.042600255,-0.011250123,0.03349085,0"
  },
  {
    "pr_id": "e1dfca39-7b29-426d-89b1-3f0da03a0eed",
    "content": "@@ -18,19 +18,22 @@\n \n package org.apache.amoro.table.descriptor;\n \n+import static org.junit.jupiter.api.Assertions.assertEquals;\n+import static org.junit.jupiter.api.Assertions.assertTrue;\n+\n import org.apache.amoro.AmoroCatalog;\n import org.apache.amoro.AmoroTable;\n import org.apache.amoro.formats.AmoroCatalogTestHelper;\n import org.apache.amoro.hive.TestHMS;\n-import org.junit.After;\n-import org.junit.Assert;\n-import org.junit.Before;\n-import org.junit.ClassRule;\n-import org.junit.Rule;\n-import org.junit.Test;\n-import org.junit.rules.TemporaryFolder;\n+import org.junit.jupiter.api.AfterAll;\n+import org.junit.jupiter.api.AfterEach;\n+import org.junit.jupiter.api.BeforeAll;\n+import org.junit.jupiter.api.BeforeEach;\n+import org.junit.jupiter.api.Test;\n+import org.junit.jupiter.api.io.TempDir;\n \n import java.io.IOException;\n+import java.nio.file.Path;\n import java.util.List;\n import java.util.concurrent.Executors;\n ",
    "section": "code",
    "embedding": "[-0.018425908,0.024817161,-0.032034934,0.036236424,0.052545294,0.0063122953,0.034100235,0.019406926,-0.029823039,-0.042300526,-0.007164715,0.00072965916,-0.07575917,0.025718002,0.0035560804,0.04310859,0.046841636,0.010076061,0.025785126,-0.024924938,0.01895377,-0.004458221,-0.01138413,0.02363794,-0.0045498023,0.004566698,0.038187653,0.004853245,-0.08038913,0.0073239547,0.0030172362"
  },
  {
    "pr_id": "e1dfca39-7b29-426d-89b1-3f0da03a0eed",
    "content": "@@ -40,9 +43,9 @@ public abstract class TestServerTableDescriptor {\n \n   protected static final String TEST_TABLE = \"test_table\";\n \n-  @Rule public TemporaryFolder temp = new TemporaryFolder();\n+  @TempDir Path tempDir;\n \n-  @ClassRule public static TestHMS TEST_HMS = new TestHMS();\n+  protected static TestHMS TEST_HMS;\n \n   private final AmoroCatalogTestHelper<?> amoroCatalogTestHelper;\n   private AmoroCatalog amoroCatalog;",
    "section": "code",
    "embedding": "[-0.016750904,-0.023280688,0.015258067,0.0068566604,0.07015025,-0.032073345,0.057113115,-2.807325e-05,-0.0035888252,-0.04922137,-0.0005453295,0.0020784426,-0.041106876,0.07412761,0.029370954,0.06888094,0.02495975,-0.03965351,0.04763308,-0.022287946,0.028455826,-0.011041779,-0.014485174,0.0111815855,-0.009605196,0.010056137,0.044271138,0.031184176,-0.07198281,0.013513491,-0.00049993"
  },
  {
    "pr_id": "e1dfca39-7b29-426d-89b1-3f0da03a0eed",
    "content": "@@ -52,9 +55,22 @@ public TestServerTableDescriptor(AmoroCatalogTestHelper<?> amoroCatalogTestHelpe\n     this.amoroCatalogTestHelper = amoroCatalogTestHelper;\n   }\n \n-  @Before\n-  public void before() throws IOException {\n-    String path = temp.newFolder().getPath();\n+  @BeforeAll\n+  static void setupTestHMS() throws Exception {\n+    TEST_HMS = new TestHMS();\n+    TEST_HMS.before();\n+  }\n+\n+  @AfterAll\n+  static void teardownTestHMS() {\n+    if (TEST_HMS != null) {\n+      TEST_HMS.after();\n+    }\n+  }\n+\n+  @BeforeEach\n+  void before() throws IOException {\n+    String path = tempDir.toFile().getAbsolutePath();\n     amoroCatalogTestHelper.initWarehouse(path);\n     amoroCatalogTestHelper.initHiveConf(TEST_HMS.getHiveConf());\n     this.amoroCatalog = amoroCatalogTestHelper.amoroCatalog();",
    "section": "code",
    "embedding": "[-0.0025144846,0.008446711,0.014801336,0.014133435,0.06981193,-0.006261386,0.03663292,-0.0068049543,0.0031565202,-0.04158292,0.010095459,0.012914068,-0.03476571,0.03203306,0.056385767,0.04639381,0.037124816,-0.03738743,0.042659406,-0.020589946,0.051540025,-0.0101116765,-0.023363084,0.021588322,-0.006779818,0.015750261,0.021736681,-0.0039252047,-0.08281914,0.016176319,0.028731521,-0"
  },
  {
    "pr_id": "e1dfca39-7b29-426d-89b1-3f0da03a0eed",
    "content": "@@ -68,8 +84,8 @@ public void before() throws IOException {\n     }\n   }\n \n-  @After\n-  public void after() throws IOException {\n+  @AfterEach\n+  void after() throws IOException {\n     try {\n       this.amoroCatalogTestHelper.amoroCatalog().dropTable(TEST_DB, TEST_TABLE, true);\n       this.amoroCatalogTestHelper.amoroCatalog().dropDatabase(TEST_DB);",
    "section": "code",
    "embedding": "[-0.029165832,0.041653726,0.017981188,0.019282373,0.04362908,-0.0005901018,0.021843867,0.03143638,-0.046013247,-0.01717133,0.0038043538,0.005901256,-0.041160446,0.011403105,0.04971507,0.05099731,0.056826863,-0.015237422,0.054615587,-0.04012633,0.019743985,-0.013702395,-0.04344074,0.054569762,0.026741497,-0.022174329,0.018522963,-0.018321339,-0.03784255,-0.0049037435,-0.01207173,-0."
  },
  {
    "pr_id": "e1dfca39-7b29-426d-89b1-3f0da03a0eed",
    "content": "@@ -79,7 +95,7 @@ public void after() throws IOException {\n   }\n \n   @Test\n-  public void tableOperations() throws Exception {\n+  void tableOperations() throws Exception {\n     FormatTableDescriptor tableDescriptor = getTableDescriptor();\n     tableDescriptor.withIoExecutor(Executors.newSingleThreadExecutor());\n ",
    "section": "code",
    "embedding": "[-0.03206118,0.0036953527,0.016763723,-0.023836518,0.032007452,0.00056350674,0.022484986,0.043273278,-0.07060919,-0.014267803,0.018244397,0.004491762,-0.056042247,0.01928939,0.023450568,0.06731366,0.07622659,0.0031397063,0.029892415,-0.040193412,-0.0044135167,0.019633358,-0.026098842,0.009713762,0.0067807916,-0.025312068,0.024056088,-0.032601204,-0.05079199,-0.036606308,0.024064254"
  },
  {
    "pr_id": "e1dfca39-7b29-426d-89b1-3f0da03a0eed",
    "content": "@@ -111,44 +127,44 @@ public void tableOperations() throws Exception {\n \n     List<DDLInfo> tableOperations = tableDescriptor.getTableOperations(table);\n \n-    Assert.assertEquals(\n-        tableOperations.get(0).getDdl(), \"ALTER TABLE test_table SET TBLPROPERTIES ('k1' = 'v1')\");\n+    assertEquals(\n+        \"ALTER TABLE test_table SET TBLPROPERTIES ('k1' = 'v1')\", tableOperations.get(0).getDdl());\n \n-    Assert.assertEquals(\n-        tableOperations.get(1).getDdl(), \"ALTER TABLE test_table UNSET TBLPROPERTIES ('k1')\");\n+    assertEquals(\n+        \"ALTER TABLE test_table UNSET TBLPROPERTIES ('k1')\", tableOperations.get(1).getDdl());\n \n-    Assert.assertTrue(\n+    assertTrue(\n         tableOperations\n             .get(2)\n             .getDdl()\n             .equalsIgnoreCase(\"ALTER TABLE test_table ADD COLUMNS (new_col int)\"));\n \n-    Assert.assertTrue(\n+    assertTrue(\n         tableOperations\n             .get(3)\n             .getDdl()\n             .equalsIgnoreCase(\"ALTER TABLE test_table RENAME COLUMN new_col TO renamed_col\"));\n \n-    Assert.assertTrue(\n+    assertTrue(\n         tableOperations\n             .get(4)\n             .getDdl()\n             .equalsIgnoreCase(\"ALTER TABLE test_table ALTER COLUMN renamed_col TYPE BIGINT\"));\n \n-    Assert.assertTrue(\n+    assertTrue(\n         tableOperations\n             .get(5)\n             .getDdl()\n             .equalsIgnoreCase(\n                 \"ALTER TABLE test_table ALTER COLUMN renamed_col COMMENT 'new comment'\"));\n \n-    Assert.assertTrue(\n+    assertTrue(\n         tableOperations\n             .get(6)\n             .getDdl()\n             .equalsIgnoreCase(\"ALTER TABLE test_table ALTER COLUMN renamed_col DROP NOT NULL\"));\n \n-    Assert.assertTrue(\n+    assertTrue(\n         tableOperations\n             .get(7)\n             .getDdl()",
    "section": "code",
    "embedding": "[-0.037344277,0.00089399685,-0.022560755,-0.0329809,0.07263691,-0.0016454611,0.02804968,0.0110498695,-0.04926661,-0.02483136,0.010392503,0.009305738,-0.07958441,0.032144617,0.0014095986,0.07340689,0.041975908,-0.03456016,0.05439831,-0.050967,0.02448723,0.013379581,-0.008872286,0.009921578,0.0042336257,-0.015758518,0.043029137,-0.007136285,-0.052247357,-0.0028409362,0.04729493,-0.00"
  },
  {
    "pr_id": "5d2c55be-8622-413c-bd3f-4d71ccb63151",
    "content": "@@ -22,6 +22,7 @@\n     <div id=\"content\">\n     {{ partial \"home-intro.html\" . }}\n     {{ partial \"home-content.html\" . }}\n+    {{ partial \"home-users.html\" . }}\n     {{ partial \"home-feature.html\" . }}\n     {{ partial \"home-footer.html\" . }}\n     <!-- Include termynal.js for landing-page homepage -->",
    "section": "code",
    "embedding": "[-0.029315097,0.05030071,-0.0005605002,-0.03638735,0.06911791,-0.035876155,0.06506946,0.0074677593,-0.004117794,-0.019024825,-0.040440742,-0.022232689,-0.021441942,0.015843593,0.019837111,0.055257235,0.053452853,-0.009773413,-0.003381185,-0.016445795,-0.041305017,0.016868822,-0.00089095125,-0.023313604,0.008539512,-0.04968454,0.044285048,0.027110515,-0.07627447,0.019117411,0.009981"
  },
  {
    "pr_id": "e1dfca39-7b29-426d-89b1-3f0da03a0eed",
    "content": "@@ -19,13 +19,11 @@\n package org.apache.amoro.utils;\n \n import static org.apache.amoro.utils.MemorySize.MemoryUnit.MEGA_BYTES;\n-import static org.hamcrest.CoreMatchers.is;\n-import static org.junit.Assert.assertEquals;\n-import static org.junit.Assert.assertNotEquals;\n-import static org.junit.Assert.assertThat;\n-import static org.junit.Assert.fail;\n+import static org.junit.jupiter.api.Assertions.assertEquals;\n+import static org.junit.jupiter.api.Assertions.assertNotEquals;\n+import static org.junit.jupiter.api.Assertions.assertThrows;\n \n-import org.junit.Test;\n+import org.junit.jupiter.api.Test;\n import org.junit.jupiter.params.ParameterizedTest;\n import org.junit.jupiter.params.provider.Arguments;\n import org.junit.jupiter.params.provider.CsvSource;",
    "section": "code",
    "embedding": "[-0.039510578,-0.0018467086,-0.023957577,0.025371218,0.05409237,0.025665585,0.03449438,0.02403268,-0.040960692,-0.04491561,0.0029456713,-0.010359622,-0.082565136,0.025855726,-0.012896207,0.03860776,0.032482993,0.01393498,0.03484157,0.009939172,0.0074467687,0.029958759,0.0037198714,0.007358356,-0.014604911,-0.016193826,0.03191377,0.03068387,-0.057394773,-0.0018366356,0.012612248,-0."
  },
  {
    "pr_id": "e1dfca39-7b29-426d-89b1-3f0da03a0eed",
    "content": "@@ -68,9 +66,13 @@ void testUnitConversion(\n     assertEquals(expectedTebiBytes, memorySize.getTebiBytes());\n   }\n \n-  @Test(expected = IllegalArgumentException.class)\n-  public void testInvalid() {\n-    new MemorySize(-1);\n+  @Test\n+  void testInvalid() {\n+    assertThrows(\n+        IllegalArgumentException.class,\n+        () -> {\n+          new MemorySize(-1);\n+        });\n   }\n \n   @ParameterizedTest",
    "section": "code",
    "embedding": "[-0.0388883,-0.013692523,-0.005939616,0.009017524,0.037300408,0.008866141,0.06800557,-0.00479129,-0.04349705,-0.02782003,0.021464875,-0.00091130036,-0.07089049,0.028955953,-0.02558781,0.07218077,0.008556275,0.0030933707,0.05976092,-0.023053223,0.015805926,0.009386401,-0.009588918,-0.0037508172,0.03725206,0.0037514954,0.07301283,-0.007264631,-0.065967865,-0.027395563,0.03320933,-0.0"
  },
  {
    "pr_id": "e1dfca39-7b29-426d-89b1-3f0da03a0eed",
    "content": "@@ -81,7 +83,7 @@ public void testInvalid() {\n     \"'1234bytes', 1234\",\n     \"'1234 bytes', 1234\"\n   })\n-  public void testParseBytes(String input, long expected) {\n+  void testParseBytes(String input, long expected) {\n     assertEquals(expected, MemorySize.parseBytes(input));\n   }\n ",
    "section": "code",
    "embedding": "[-0.012293901,-0.011890845,-0.031559553,0.018535418,0.052550115,0.01630281,0.030330656,-0.0079686465,-0.026906306,0.0016682717,0.0011126212,-0.0015092209,-0.07079662,0.012424016,-0.012181621,0.062979385,0.07086547,-0.014540606,0.09791122,-0.011449791,-0.030886773,-0.009528469,0.016220689,0.012422017,0.046601277,0.0029485817,0.026823584,-0.0053518056,-0.054715104,-0.03549181,0.04367"
  },
  {
    "pr_id": "e1dfca39-7b29-426d-89b1-3f0da03a0eed",
    "content": "@@ -94,7 +96,7 @@ public void testParseBytes(String input, long expected) {\n     \"667766kibibytes, 667766\",\n     \"667766 kibibytes, 667766\"\n   })\n-  public void testParseKibiBytes(String input, int expected) {\n+  void testParseKibiBytes(String input, int expected) {\n     assertEquals(expected, MemorySize.parse(input).getKibiBytes());\n   }\n ",
    "section": "code",
    "embedding": "[-0.00985396,-0.046962395,-0.03612046,0.023032956,0.05101525,0.016490767,0.030259337,0.001555178,-0.041785568,-0.0014832322,-0.01829516,-0.0075663766,-0.072819896,0.012293188,-0.016779266,0.077244274,0.075128034,-0.005010317,0.08714493,-0.012777942,-0.0073246923,0.010310163,0.022845363,0.021140674,0.031166123,0.0013138892,0.03211933,-0.039440542,-0.042728066,-0.045526102,0.04349418"
  },
  {
    "pr_id": "e1dfca39-7b29-426d-89b1-3f0da03a0eed",
    "content": "@@ -107,7 +109,7 @@ public void testParseKibiBytes(String input, int expected) {\n     \"7657623mebibytes, 7657623\",\n     \"7657623 mebibytes, 7657623\"\n   })\n-  public void testParseMebiBytes(String input, int expected) {\n+  void testParseMebiBytes(String input, int expected) {\n     assertEquals(expected, MemorySize.parse(input).getMebiBytes());\n   }\n ",
    "section": "code",
    "embedding": "[-0.0058103506,-0.03963713,-0.039909765,0.03507825,0.048452478,0.0242679,0.026740674,-0.009971351,-0.047352012,-0.012094979,0.00772593,0.011642249,-0.07769727,0.0117104575,-0.02495659,0.07435748,0.06497003,-0.002873676,0.08272127,-0.01967236,-0.0035818813,0.016227445,0.0311578,0.01773487,0.02980971,0.03549296,0.045153636,-0.045151334,-0.052298523,-0.049042266,0.037094038,0.00665364"
  },
  {
    "pr_id": "e1dfca39-7b29-426d-89b1-3f0da03a0eed",
    "content": "@@ -120,7 +122,7 @@ public void testParseMebiBytes(String input, int expected) {\n     \"987654gibibytes, 987654\",\n     \"987654 gibibytes, 987654\"\n   })\n-  public void testParseGibiBytes(String input, int expected) {\n+  void testParseGibiBytes(String input, int expected) {\n     assertEquals(expected, MemorySize.parse(input).getGibiBytes());\n   }\n ",
    "section": "code",
    "embedding": "[-0.013946109,-0.03351236,-0.023238612,0.017923854,0.03909994,0.011749683,0.025384475,0.008383769,-0.041200176,-0.015105007,0.0049265027,0.012501469,-0.09321428,0.008139602,-0.043521028,0.07274322,0.071203925,-0.006876652,0.04832415,-0.017612131,-0.0015069833,0.0010214968,0.021092914,0.028582323,0.04390824,0.044173162,0.04751281,-0.035015497,-0.040779214,-0.047381077,0.047737382,0."
  },
  {
    "pr_id": "e1dfca39-7b29-426d-89b1-3f0da03a0eed",
    "content": "@@ -133,12 +135,12 @@ public void testParseGibiBytes(String input, int expected) {\n     \"1234567tebibytes, 1234567\",\n     \"1234567 tebibytes, 1234567\"\n   })\n-  public void testParseTebiBytes(String input, int expected) {\n+  void testParseTebiBytes(String input, int expected) {\n     assertEquals(expected, MemorySize.parse(input).getTebiBytes());\n   }\n \n   @Test\n-  public void testUpperCase() {\n+  void testUpperCase() {\n     assertEquals(1L, MemorySize.parse(\"1 B\").getBytes());\n     assertEquals(1L, MemorySize.parse(\"1 K\").getKibiBytes());\n     assertEquals(1L, MemorySize.parse(\"1 M\").getMebiBytes());",
    "section": "code",
    "embedding": "[-0.010956537,-0.028406478,-0.030314637,0.02961293,0.039487433,0.012337553,0.029337682,-0.0010820496,-0.034832556,-0.016926955,-0.0060994765,-0.0062875976,-0.084132776,0.006353301,-0.037876878,0.07706745,0.068622634,-0.01218814,0.059180573,-0.020780029,0.009555544,-0.01122533,0.014659834,0.02025512,0.040316194,0.03305668,0.05466076,-0.019218147,-0.03941632,-0.04332734,0.045106992,0"
  },
  {
    "pr_id": "e1dfca39-7b29-426d-89b1-3f0da03a0eed",
    "content": "@@ -147,75 +149,83 @@ public void testUpperCase() {\n   }\n \n   @Test\n-  public void testTrimBeforeParse() {\n+  void testTrimBeforeParse() {\n     assertEquals(155L, MemorySize.parseBytes(\"      155      \"));\n     assertEquals(155L, MemorySize.parseBytes(\"      155      bytes   \"));\n   }\n \n   @Test\n-  public void testParseInvalid() {\n+  void testParseInvalid() {\n     // null\n-    try {\n-      MemorySize.parseBytes(null);\n-      fail(\"exception expected\");\n-    } catch (IllegalArgumentException ignored) {\n-    }\n+    assertThrows(\n+        IllegalArgumentException.class,\n+        () -> {\n+          MemorySize.parseBytes(null);\n+        });\n \n     // empty\n-    try {\n-      MemorySize.parseBytes(\"\");\n-      fail(\"exception expected\");\n-    } catch (IllegalArgumentException ignored) {\n-    }\n+    assertThrows(\n+        IllegalArgumentException.class,\n+        () -> {\n+          MemorySize.parseBytes(\"\");\n+        });\n \n     // blank\n-    try {\n-      MemorySize.parseBytes(\"     \");\n-      fail(\"exception expected\");\n-    } catch (IllegalArgumentException ignored) {\n-    }\n+    assertThrows(\n+        IllegalArgumentException.class,\n+        () -> {\n+          MemorySize.parseBytes(\"     \");\n+        });\n \n     // no number\n-    try {\n-      MemorySize.parseBytes(\"foobar or fubar or foo bazz\");\n-      fail(\"exception expected\");\n-    } catch (IllegalArgumentException ignored) {\n-    }\n+    assertThrows(\n+        IllegalArgumentException.class,\n+        () -> {\n+          MemorySize.parseBytes(\"foobar or fubar or foo bazz\");\n+        });\n \n     // wrong unit\n-    try {\n-      MemorySize.parseBytes(\"16 gjah\");\n-      fail(\"exception expected\");\n-    } catch (IllegalArgumentException ignored) {\n-    }\n+    assertThrows(\n+        IllegalArgumentException.class,\n+        () -> {\n+          MemorySize.parseBytes(\"16 gjah\");\n+        });\n \n     // multiple numbers\n-    try {\n-      MemorySize.parseBytes(\"16 16 17 18 bytes\");\n-      fail(\"exception expected\");\n-    } catch (IllegalArgumentException ignored) {\n-    }\n+    assertThrows(\n+        IllegalArgumentException.class,\n+        () -> {\n+          MemorySize.parseBytes(\"16 16 17 18 bytes\");\n+        });\n \n     // negative number\n-    try {\n-      MemorySize.parseBytes(\"-100 bytes\");\n-      fail(\"exception expected\");\n-    } catch (IllegalArgumentException ignored) {\n-    }\n+    assertThrows(\n+        IllegalArgumentException.class,\n+        () -> {\n+          MemorySize.parseBytes(\"-100 bytes\");\n+        });\n   }\n \n-  @Test(expected = IllegalArgumentException.class)\n-  public void testParseNumberOverflow() {\n-    MemorySize.parseBytes(\"100000000000000000000000000000000 bytes\");\n+  @Test\n+  void testParseNumberOverflow() {\n+    assertThrows(\n+        IllegalArgumentException.class,\n+        () -> {\n+          MemorySize.parseBytes(\"100000000000000000000000000000000 bytes\");\n+        });\n   }\n \n-  @Test(expected = IllegalArgumentException.class)\n-  public void testParseNumberTimeUnitOverflow() {\n-    MemorySize.parseBytes(\"100000000000000 tb\");\n+  @Test\n+  void testParseNumberTimeUnitOverflow() {\n+    assertThrows(\n+        IllegalArgumentException.class,\n+        () -> {\n+          MemorySize.parseBytes(\"100000000000000 tb\");\n+        });\n   }\n \n   @Test\n-  public void testParseWithDefaultUnit() {\n+  void testParseWithDefaultUnit() {\n     assertEquals(7, MemorySize.parse(\"7\", MEGA_BYTES).getMebiBytes());\n     assertNotEquals(7, MemorySize.parse(\"7340032\", MEGA_BYTES));\n     assertEquals(7, MemorySize.parse(\"7m\", MEGA_BYTES).getMebiBytes());",
    "section": "code",
    "embedding": "[-0.054654412,0.0096750315,-0.004960716,-0.014604341,0.05019815,0.008709991,0.0775865,0.020981979,-0.04277205,-0.024411762,0.004617389,0.03670971,-0.059835732,0.015904453,0.0039148154,0.07513919,0.05300152,-0.026572194,0.040089853,-0.007524849,0.02051618,0.023349866,-0.025477238,0.029347945,0.018920697,0.007521474,0.043334052,0.028292576,-0.062227305,0.013878155,0.01861468,0.005265"
  },
  {
    "pr_id": "e1dfca39-7b29-426d-89b1-3f0da03a0eed",
    "content": "@@ -229,15 +239,19 @@ public void testParseWithDefaultUnit() {\n   }\n \n   @Test\n-  public void testDivideByLong() {\n+  void testDivideByLong() {\n     final MemorySize memory = new MemorySize(100L);\n-    assertThat(memory.divide(23), is(new MemorySize(4L)));\n+    assertEquals(new MemorySize(4L), memory.divide(23));\n   }\n \n-  @Test(expected = IllegalArgumentException.class)\n-  public void testDivideByNegativeLong() {\n+  @Test\n+  void testDivideByNegativeLong() {\n     final MemorySize memory = new MemorySize(100L);\n-    memory.divide(-23L);\n+    assertThrows(\n+        IllegalArgumentException.class,\n+        () -> {\n+          memory.divide(-23L);\n+        });\n   }\n \n   static Stream<Arguments> testToHumanReadableStringProvider() {",
    "section": "code",
    "embedding": "[-0.03966817,-0.0051189275,-0.0028062859,0.0029172355,0.03626753,0.029582007,0.07076138,0.022582017,-0.011015637,-0.011967988,0.0065640532,-0.002290948,-0.07135739,0.02412589,-0.01416997,0.06841245,0.039275285,-0.022754457,0.041461594,0.011888845,0.013934614,-0.013930083,0.002116098,0.029730948,0.02711131,-0.017843189,0.102712676,0.020636534,-0.03345753,-0.032825846,0.03879336,0.00"
  },
  {
    "pr_id": "e1dfca39-7b29-426d-89b1-3f0da03a0eed",
    "content": "@@ -255,7 +269,7 @@ static Stream<Arguments> testToHumanReadableStringProvider() {\n \n   @ParameterizedTest\n   @MethodSource(\"testToHumanReadableStringProvider\")\n-  public void testToHumanReadableString(long bytes, String expected) {\n-    assertThat(new MemorySize(bytes).toHumanReadableString(), is(expected));\n+  void testToHumanReadableString(long bytes, String expected) {\n+    assertEquals(expected, new MemorySize(bytes).toHumanReadableString());\n   }\n }",
    "section": "code",
    "embedding": "[-0.024177808,-0.013959918,-0.008157393,0.005665304,0.060594946,-0.007868883,0.057517424,0.030300906,0.0011136977,-0.039128765,0.01994278,0.008764041,-0.04788647,0.018378232,-0.0051378827,0.050112776,0.050798472,-0.013418521,0.048745662,-0.031510096,-0.018911047,0.013087438,0.00012809644,0.006404922,0.025456347,-0.015752506,0.03702309,0.022590881,-0.02756876,0.010049104,0.018829018"
  },
  {
    "pr_id": "e1dfca39-7b29-426d-89b1-3f0da03a0eed",
    "content": "Thank you for your contribution to the new unit test support. You can run `mvn spotless:apply` to format code .It will be reviewed after the CI runs @Jungkihong07 ",
    "section": "comment",
    "embedding": "[0.01580147,-0.032708786,-0.004543645,0.005088209,0.058696866,0.019256566,0.06787758,0.016807815,0.001157616,-0.084860325,0.011736302,0.011701148,-0.053653564,0.011589993,-0.009947045,0.06799841,0.032824755,-0.012208015,0.042604405,0.033233125,-0.010199098,-0.0036802082,-0.0028563295,0.005013397,-0.04276058,0.010825524,0.07580932,-0.012888852,-0.08393109,-0.019162875,0.040410083,-0"
  },
  {
    "pr_id": "e1dfca39-7b29-426d-89b1-3f0da03a0eed",
    "content": "I’ve completed the code formatting.\r\nA review would be appreciated when you get a chance. @czy006 ",
    "section": "comment",
    "embedding": "[0.02878525,0.0051395725,-0.0011805589,0.02318577,0.060642548,0.030830579,0.019699754,0.008128516,0.015264647,-0.05493182,0.020764971,0.009619587,-0.06021289,-0.003267973,-0.011560543,0.10013691,0.03971149,0.019229088,0.03158287,-0.014753463,0.0021385003,0.033667535,0.01266941,0.0024809306,0.0065938067,-0.0032817388,0.020193756,0.017721102,-0.081391096,0.01501549,0.047092114,0.0251"
  },
  {
    "pr_id": "b2e982f0-8b05-43bf-a510-04e8a37c4771",
    "content": "@@ -50,8 +59,12 @@ install-ui-packages:\n ui:\n \t@cd ui && pnpm pre-install && pnpm build && cd -\n \n-lint: generate\n+lint: generate $(GOLANGCI)\n+\t@bash ./script/check-asf-header.sh\n+\t$(GOLANGCI) run\n+\n+lint-fix: generate $(GOLANGCI)\n \t@bash ./script/check-asf-header.sh\n-\t@gofmt -w -l .\n+\t$(GOLANGCI) run --fix\n \n all: clean build",
    "section": "code",
    "embedding": "[-0.020662047,0.011550399,0.023183802,0.007287245,0.02608713,-0.009657639,0.05541273,0.009428334,0.036188647,-0.065680176,-0.004246177,0.0136976335,-0.07696756,0.012392542,0.013512793,0.05787535,0.038574506,0.0072915433,-0.0040669236,0.010123555,0.0066226213,0.034353927,-0.02306073,0.016882274,-0.0034153196,0.0014739574,0.060501188,-0.014140525,-0.06454645,-0.013087907,0.007058594,"
  },
  {
    "pr_id": "c54f1503-8c1f-4886-bf46-fedb8f14f1bb",
    "content": "Thank you @raulcd !!!",
    "section": "comment",
    "embedding": "[0.024820715,-0.01940116,0.061170034,0.019648088,0.033399813,0.004592516,0.027961044,0.046083655,-0.030763982,-0.017069772,0.021343693,0.0154470755,-0.03305793,-0.021051917,0.02462843,0.06703288,0.03567146,0.030764295,0.023141358,0.010777507,0.0066953846,0.0066300016,0.007972979,0.013844054,0.0029974924,0.016468955,0.012359745,0.015084779,-0.053732213,-0.029467285,0.06557906,0.0107"
  },
  {
    "pr_id": "e1dfca39-7b29-426d-89b1-3f0da03a0eed",
    "content": "I noticed that some tests in the amoro-format-iceberg module are failing with the following error:\r\n\r\nNo ParameterResolver registered for parameter [org.apache.amoro.formats.AmoroCatalogTestHelper arg0] in constructor [public org.apache.amoro.formats.TestIcebergAmoroCatalog(org.apache.amoro.formats.AmoroCatalogTestHelper)].\r\n\r\nSimilar errors occur for several test methods in TestMixedIcebergFormatCatalog, which causes the amoro-format-iceberg module tests to fail and the subsequent modules to be skipped.\r\n\r\nMy understanding is that, in JUnit 4, AmoroCatalogTestHelper was likely injected via a custom runner, rule, or test utility, but after migrating to JUnit 5 there is no ParameterResolver (or equivalent mechanism) configured for this helper.\r\n\r\nBefore making further changes, I would like to ask for guidance on the preferred approach in this project:\r\n\r\nShould I introduce a JUnit 5 ParameterResolver (and register it via @ExtendWith or the service loader) to provide AmoroCatalogTestHelper?\r\n\r\nOr would you prefer to refactor these tests to avoid constructor injection and use another pattern that is more consistent with the existing JUnit 5 tests in this repository?\r\n\r\nOnce I know the preferred direction, I can update the tests accordingly.",
    "section": "comment",
    "embedding": "[-0.02530461,-0.010361159,0.010581049,0.04854831,0.05997429,-0.00036542106,0.049410947,0.00022998848,-0.014415028,-0.02817074,0.007844499,0.009319274,-0.05059032,0.006293763,0.00925029,0.041868344,0.033023424,0.0023453627,0.037108205,-0.02985685,0.02920848,-0.017333437,0.006146757,0.039599404,-0.014142587,-0.023118608,0.013508819,-0.0031030364,-0.03706867,-0.0024061645,0.0309772,-0"
  },
  {
    "pr_id": "e2e361f7-5f72-4045-9534-c2cb38d536b9",
    "content": "TITLE: [AMORO-3973] Support spark3.4 for mixed format",
    "section": "title",
    "embedding": "[-0.0083203195,-0.018417215,-0.009800947,0.024521977,0.052759416,0.03288873,-0.008074261,-0.011896973,-0.0058615883,-0.023877477,-0.005681691,0.0139916055,-0.06840828,0.016520804,-0.034617856,0.04247042,0.0013901391,-0.025740195,0.03962587,0.019462753,0.03935306,0.018413229,-0.018256644,0.011834287,-0.02919693,-0.024103267,0.01641039,-0.011803829,-0.069652066,-0.025379406,0.0473972"
  },
  {
    "pr_id": "e2e361f7-5f72-4045-9534-c2cb38d536b9",
    "content": "TITLE: [AMORO-3973] Support spark3.4 for mixed format\n\nBODY:\n<!--\r\nThanks for sending a pull request!\r\n\r\nHere are some tips for you:\r\n  1. If this is your first time, please read our contributor guidelines: https://amoro.apache.org/how-to-contribute/\r\n  2. If the PR is related to an issue in https://github.com/apache/amoro/issues, add '[AMORO-XXXX]' in your PR title, e.g., '[AMORO-XXXX] Your PR title ...'.\r\n  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][AMORO-XXXX] Your PR title ...'.\r\n-->\r\n\r\n## Why are the changes needed?\r\n<!--\r\nPlease clarify why the changes are needed. For instance,\r\n  1. If you add a feature, you can talk about its use case.\r\n  2. If you fix a bug, you can clarify why it is a bug.\r\n  3. Use Fix/Resolve/Close #{ISSUE_NUMBER} to link this PR to its related issue\r\n-->\r\n\r\nClose #3973.\r\n\r\n## Brief change log\r\n<!--\r\nClearly describe the changes made in modules, classes, methods, etc.\r\n-->\r\n\r\n-\r\n\r\n## How was this patch tested?\r\n\r\n- [ ] Add some test cases that check the changes thoroughly including negative and positive cases if possible\r\n\r\n- [ ] Add screenshots for manual tests if appropriate\r\n\r\n- [x] Run test locally before making a pull request\r\n\r\n## Documentation\r\n\r\n- Does this pull request introduce a new feature? (yes / no)\r\n- If yes, how is the feature documented? (not applicable / docs / JavaDocs / not documented)\r\n",
    "section": "full",
    "embedding": "[0.011639632,-0.024459759,-0.00061776023,0.02224975,0.05042779,-0.0020660935,0.004921391,-2.7465392e-05,-0.031240774,-0.03367808,-0.021536024,0.017839344,-0.06841158,0.015553609,-0.038082954,0.054785594,0.042863205,0.01506132,0.030835358,0.0050570797,0.018169403,0.007261739,-0.02172549,0.011600104,-0.023427578,-0.007838368,-0.008886331,-0.0054502734,-0.07345563,-0.04913221,0.053669"
  },
  {
    "pr_id": "e2e361f7-5f72-4045-9534-c2cb38d536b9",
    "content": "",
    "section": "prose",
    "embedding": "[0.012244709,0.039513454,-0.007785414,0.0021320505,0.046604965,0.012159259,0.03977907,0.04207942,-0.020763515,-0.039035235,0.024283497,-0.00805646,-0.04972243,0.017127944,-0.009930698,0.038848016,0.008790611,0.0133490125,-0.017834641,0.0071114884,0.030171363,-0.022801451,0.031216213,-0.00092527375,0.025564943,-0.024615467,0.027907416,0.0069565354,-0.02231851,0.03557899,0.019480716,"
  },
  {
    "pr_id": "e2e361f7-5f72-4045-9534-c2cb38d536b9",
    "content": "<!--\r\nThanks for sending a pull request!\r\n\r\nHere are some tips for you:\r\n  1. If this is your first time, please read our contributor guidelines: https://amoro.apache.org/how-to-contribute/\r\n  2. If the PR is related to an issue in https://github.com/apache/amoro/issues, add '[AMORO-XXXX]' in your PR title, e.g., '[AMORO-XXXX] Your PR title ...'.\r\n  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][AMORO-XXXX] Your PR title ...'.\r\n-->\r\n\r\n## Why are the changes needed?\r\n<!--\r\nPlease clarify why the changes are needed. For instance,\r\n  1. If you add a feature, you can talk about its use case.\r\n  2. If you fix a bug, you can clarify why it is a bug.\r\n  3. Use Fix/Resolve/Close #{ISSUE_NUMBER} to link this PR to its related issue\r\n-->\r\n\r\nClose #3973.\r\n\r\n## Brief change log\r\n<!--\r\nClearly describe the changes made in modules, classes, methods, etc.\r\n-->\r\n\r\n-\r\n\r\n## How was this patch tested?\r\n\r\n- [ ] Add some test cases that check the changes thoroughly including negative and positive cases if possible\r\n\r\n- [ ] Add screenshots for manual tests if appropriate\r\n\r\n- [x] Run test locally before making a pull request\r\n\r\n## Documentation\r\n\r\n- Does this pull request introduce a new feature? (yes / no)\r\n- If yes, how is the feature documented? (not applicable / docs / JavaDocs / not documented)",
    "section": "prose",
    "embedding": "[0.012648492,-0.015791846,0.017445112,0.012189255,0.042703573,-0.013461968,0.022366079,0.007361418,-0.025504543,-0.04421883,-0.04512941,0.020362142,-0.07005572,0.0015439398,-0.036664877,0.057805836,0.05428832,0.024349306,0.031081075,-0.00683149,0.004866929,0.001004503,-0.015853459,0.009567998,-0.026213855,-0.017183093,-0.0062649795,-0.008267084,-0.059814148,-0.056138832,0.04862206,"
  },
  {
    "pr_id": "e2e361f7-5f72-4045-9534-c2cb38d536b9",
    "content": "@@ -38,7 +38,7 @@ jobs:\n     strategy:\n       matrix:\n         jdk: [ '8', '11' ]\n-        spark: [ '3.3', '3.5' ]\n+        spark: [ '3.3', '3.4', '3.5' ]\n     name: Build Amoro with JDK ${{ matrix.jdk }} Spark-${{ matrix.spark }}\n     steps:\n       - uses: actions/checkout@v3",
    "section": "code",
    "embedding": "[-0.01594982,0.019050868,0.012173298,0.010411478,0.028127013,0.029171906,0.035428394,0.015697682,-0.00954128,-0.038413152,0.00021633616,0.00294349,-0.07608488,0.015940556,-0.037189897,0.017071154,0.035622124,0.01998169,0.017093774,-0.01656991,0.030399248,0.03053418,0.002391841,0.030304357,0.004320969,-0.015119975,0.052123994,0.004128494,-0.043757576,-0.047412172,0.00928402,0.021642"
  },
  {
    "pr_id": "e2e361f7-5f72-4045-9534-c2cb38d536b9",
    "content": "@@ -0,0 +1,474 @@\n+<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n+<!--\n+  ~ Licensed to the Apache Software Foundation (ASF) under one\n+  ~ or more contributor license agreements.  See the NOTICE file\n+  ~ distributed with this work for additional information\n+  ~ regarding copyright ownership.  The ASF licenses this file\n+  ~ to you under the Apache License, Version 2.0 (the\n+  ~ \"License\"); you may not use this file except in compliance\n+  ~ with the License.  You may obtain a copy of the License at\n+  ~\n+  ~     http://www.apache.org/licenses/LICENSE-2.0\n+  ~\n+  ~ Unless required by applicable law or agreed to in writing, software\n+  ~ distributed under the License is distributed on an \"AS IS\" BASIS,\n+  ~ WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+  ~ See the License for the specific language governing permissions and\n+  ~ limitations under the License.\n+  -->\n+<project xmlns=\"http://maven.apache.org/POM/4.0.0\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\n+         xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\">\n+    <modelVersion>4.0.0</modelVersion>\n+    <parent>\n+        <groupId>org.apache.amoro</groupId>\n+        <artifactId>amoro-mixed-spark_${scala.binary.version}</artifactId>\n+        <version>0.9-SNAPSHOT</version>\n+        <relativePath>../../pom.xml</relativePath>\n+    </parent>\n+\n+    <artifactId>amoro-format-mixed-spark-3.4_${scala.binary.version}</artifactId>\n+    <packaging>jar</packaging>\n+    <name>Amoro Project Mixed Format Spark 3.4</name>\n+    <url>https://amoro.apache.org</url>\n+\n+    <properties>\n+        <hive.version>2.3.9</hive.version>\n+        <scala.collection.compat>2.11.0</scala.collection.compat>\n+    </properties>\n+\n+    <dependencies>\n+        <dependency>\n+            <groupId>org.scala-lang.modules</groupId>\n+            <artifactId>scala-collection-compat_${scala.binary.version}</artifactId>\n+            <version>${scala.collection.compat}</version>\n+        </dependency>\n+\n+        <dependency>\n+            <groupId>org.apache.spark</groupId>\n+            <artifactId>spark-sql_${scala.binary.version}</artifactId>\n+            <version>${spark.version}</version>\n+            <scope>provided</scope>\n+            <exclusions>\n+                <exclusion>\n+                    <groupId>org.apache.parquet</groupId>\n+                    <artifactId>parquet-column</artifactId>\n+                </exclusion>\n+                <exclusion>\n+                    <groupId>org.apache.parquet</groupId>\n+                    <artifactId>parquet-hadoop</artifactId>\n+                </exclusion>\n+                <exclusion>\n+                    <groupId>org.apache.parquet</groupId>\n+                    <artifactId>parquet-hadoop</artifactId>\n+                </exclusion>\n+                <exclusion>\n+                    <groupId>org.slf4j</groupId>\n+                    <artifactId>slf4j-api</artifactId>\n+                </exclusion>\n+                <exclusion>\n+                    <groupId>org.apache.avro</groupId>\n+                    <artifactId>avro</artifactId>\n+                </exclusion>\n+                <exclusion>\n+                    <groupId>org.apache.arrow</groupId>\n+                    <artifactId>arrow-memory-core</artifactId>\n+                </exclusion>\n+                <exclusion>\n+                    <groupId>org.apache.arrow</groupId>\n+                    <artifactId>arrow-memory-netty</artifactId>\n+                </exclusion>\n+                <exclusion>\n+                    <groupId>org.apache.arrow</groupId>\n+                    <artifactId>arrow-vector</artifactId>\n+                </exclusion>\n+                <exclusion>\n+                    <groupId>org.apache.orc</groupId>\n+                    <artifactId>orc-core</artifactId>\n+                </exclusion>\n+                <exclusion>\n+                    <groupId>org.apache.orc</groupId>\n+                    <artifactId>orc-mapreduce</artifactId>\n+                </exclusion>\n+            </exclusions>\n+        </dependency>\n+\n+        <dependency>\n+            <groupId>org.scala-lang</groupId>\n+            <artifactId>scala-library</artifactId>\n+            <version>${scala.version}</version>\n+        </dependency>\n+\n+        <dependency>\n+            <groupId>org.scala-lang</groupId>\n+            <artifactId>scala-compiler</artifactId>\n+            <version>${scala.version}</version>\n+        </dependency>\n+\n+        <dependency>\n+            <groupId>org.apache.spark</groupId>\n+            <artifactId>spark-core_${scala.binary.version}</artifactId>\n+            <version>${spark.version}</version>\n+            <scope>provided</scope>\n+            <exclusions>\n+                <exclusion>\n+                    <groupId>com.google.guava</groupId>\n+                    <artifactId>guava</artifactId>\n+                </exclusion>\n+                <exclusion>\n+                    <groupId>org.apache.hadoop</groupId>\n+                    <artifactId>hadoop-client-api</artifactId>\n+                </exclusion>\n+                <exclusion>\n+                    <groupId>org.slf4j</groupId>\n+                    <artifactId>slf4j-api</artifactId>\n+                </exclusion>\n+                <exclusion>\n+                    <groupId>org.apache.logging.log4j</groupId>\n+                    <artifactId>log4j-slf4j2-impl</artifactId>\n+                </exclusion>\n+                <exclusion>\n+                    <groupId>org.apache.avro</groupId>\n+                    <artifactId>avro</artifactId>\n+                </exclusion>\n+                <exclusion>\n+                    <groupId>org.apache.arrow</groupId>\n+                    <artifactId>arrow-memory-core</artifactId>\n+                </exclusion>\n+                <exclusion>\n+                    <groupId>org.apache.arrow</groupId>\n+                    <artifactId>arrow-memory-netty</artifactId>\n+                </exclusion>\n+                <exclusion>\n+                    <groupId>org.apache.arrow</groupId>\n+                    <artifactId>arrow-vector</artifactId>\n+                </exclusion>\n+            </exclusions>\n+        </dependency>\n+\n+        <dependency>\n+            <groupId>org.apache.spark</groupId>\n+            <artifactId>spark-hive_${scala.binary.version}</artifactId>\n+            <version>${spark.version}</version>\n+            <scope>provided</scope>\n+            <exclusions>\n+                <exclusion>\n+                    <groupId>org.apache.hive</groupId>\n+                    <artifactId>hive-metastore</artifactId>\n+                </exclusion>\n+                <exclusion>\n+                    <groupId>commons-logging</groupId>\n+                    <artifactId>commons-logging</artifactId>\n+                </exclusion>\n+                <exclusion>\n+                    <groupId>com.google.guava</groupId>\n+                    <artifactId>guava</artifactId>\n+                </exclusion>\n+                <exclusion>\n+                    <groupId>org.slf4j</groupId>\n+                    <artifactId>slf4j-api</artifactId>\n+                </exclusion>\n+                <exclusion>\n+                    <groupId>org.apache.avro</groupId>\n+                    <artifactId>avro</artifactId>\n+                </exclusion>\n+                <exclusion>\n+                    <groupId>org.apache.arrow</groupId>\n+                    <artifactId>arrow-memory-core</artifactId>\n+                </exclusion>\n+                <exclusion>\n+                    <groupId>org.apache.arrow</groupId>\n+                    <artifactId>arrow-memory-netty</artifactId>\n+                </exclusion>\n+                <exclusion>\n+                    <groupId>org.apache.arrow</groupId>\n+                    <artifactId>arrow-vector</artifactId>\n+                </exclusion>\n+                <exclusion>\n+                    <groupId>org.apache.hive</groupId>\n+                    <artifactId>*</artifactId>\n+                </exclusion>\n+            </exclusions>\n+        </dependency>\n+\n+        <!-- amoro dependency -->\n+        <dependency>\n+            <groupId>org.apache.amoro</groupId>\n+            <artifactId>amoro-common</artifactId>\n+            <version>${project.version}</version>\n+            <exclusions>\n+                <exclusion>\n+                    <groupId>org.apache.hadoop</groupId>\n+                    <artifactId>hadoop-hdfs</artifactId>\n+                </exclusion>\n+                <exclusion>\n+                    <groupId>commons-logging</groupId>\n+                    <artifactId>commons-logging</artifactId>\n+                </exclusion>\n+                <exclusion>\n+                    <groupId>javax.servlet</groupId>\n+                    <artifactId>servlet-api</artifactId>\n+                </exclusion>\n+                <exclusion>\n+                    <groupId>com.google.guava</groupId>\n+                    <artifactId>guava</artifactId>\n+                </exclusion>\n+            </exclusions>\n+        </dependency>\n+\n+        <dependency>\n+            <groupId>org.apache.amoro</groupId>\n+            <artifactId>amoro-mixed-hive</artifactId>\n+            <version>${project.version}</version>\n+            <exclusions>\n+                <exclusion>\n+                    <groupId>com.google.guava</groupId>\n+                    <artifactId>guava</artifactId>\n+                </exclusion>\n+                <exclusion>\n+                    <groupId>org.apache.hive</groupId>\n+                    <artifactId>*</artifactId>\n+                </exclusion>\n+            </exclusions>\n+        </dependency>\n+\n+        <dependency>\n+            <groupId>org.apache.amoro</groupId>\n+            <artifactId>amoro-format-mixed-spark-3-common_${scala.binary.version}</artifactId>\n+            <version>${project.version}</version>\n+        </dependency>\n+\n+        <dependency>\n+            <groupId>org.apache.iceberg</groupId>\n+            <artifactId>iceberg-spark-3.4_${scala.binary.version}</artifactId>\n+            <version>${iceberg.version}</version>\n+            <exclusions>\n+                <exclusion>\n+                    <groupId>org.apache.parquet</groupId>\n+                    <artifactId>parquet-column</artifactId>\n+                </exclusion>\n+                <exclusion>\n+                    <groupId>org.apache.parquet</groupId>\n+                    <artifactId>parquet-hadoop</artifactId>\n+                </exclusion>\n+            </exclusions>\n+        </dependency>\n+\n+        <dependency>\n+            <groupId>org.apache.iceberg</groupId>\n+            <artifactId>iceberg-spark-extensions-3.4_${scala.binary.version}</artifactId>\n+            <version>${iceberg.version}</version>\n+            <exclusions>\n+                <exclusion>\n+                    <groupId>org.apache.parquet</groupId>\n+                    <artifactId>parquet-column</artifactId>\n+                </exclusion>\n+            </exclusions>\n+        </dependency>\n+\n+        <!-- test dependencies -->\n+        <dependency>\n+            <groupId>org.apache.paimon</groupId>\n+            <artifactId>paimon-spark-${spark.major.version}</artifactId>\n+            <version>${paimon.version}</version>\n+            <scope>test</scope>\n+        </dependency>\n+        <dependency>\n+            <groupId>org.apache.paimon</groupId>\n+            <artifactId>paimon-hive-connector-3.1</artifactId>\n+            <version>${paimon.version}</version>\n+            <scope>test</scope>\n+        </dependency>\n+        <dependency>\n+            <groupId>org.apache.amoro</groupId>\n+            <artifactId>amoro-format-paimon</artifactId>\n+            <version>${parent.version}</version>\n+            <scope>test</scope>\n+        </dependency>\n+\n+        <dependency>\n+            <groupId>org.junit.platform</groupId>\n+            <artifactId>junit-platform-launcher</artifactId>\n+            <scope>test</scope>\n+        </dependency>\n+        <dependency>\n+            <groupId>org.junit.platform</groupId>\n+   \n...<truncated>...",
    "section": "code",
    "embedding": "[-0.0024548327,-0.021658882,0.00076062005,0.023235433,0.03823195,-0.023587693,0.057313614,-0.0018208518,0.0062562716,-0.022452695,-0.013567137,-0.0036318128,-0.08710561,0.045275766,-0.040001407,0.05023629,0.025165442,0.008190702,0.03409936,-0.023222849,0.018676845,-0.00016183686,-0.012447689,0.032180604,-0.05286006,0.022364913,0.039637998,0.024369275,-0.049553063,-0.012884388,0.020"
  },
  {
    "pr_id": "e2e361f7-5f72-4045-9534-c2cb38d536b9",
    "content": "@@ -0,0 +1,1658 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ *\n+ * This file is an adaptation of Presto's presto-parser/src/main/antlr4/com/facebook/presto/sql/parser/SqlBase.g4 grammar.\n+ */\n+\n+grammar MixedFormatSqlExtend;\n+\n+@parser::members {\n+  /**\n+   * When false, INTERSECT is given the greater precedence over the other set\n+   * operations (UNION, EXCEPT and MINUS) as per the SQL standard.\n+   */\n+  public boolean legacy_setops_precedence_enabled = false;\n+\n+  /**\n+   * When false, a literal with an exponent would be converted into\n+   * double type rather than decimal type.\n+   */\n+  public boolean legacy_exponent_literal_as_decimal_enabled = false;\n+\n+  /**\n+   * When true, the behavior of keywords follows ANSI SQL standard.\n+   */\n+  public boolean SQL_standard_keyword_behavior = false;\n+}\n+\n+@lexer::members {\n+  /**\n+   * When true, parser should throw ParseExcetion for unclosed bracketed comment.\n+   */\n+  public boolean has_unclosed_bracketed_comment = false;\n+\n+  /**\n+   * Verify whether current token is a valid decimal token (which contains dot).\n+   * Returns true if the character that follows the token is not a digit or letter or underscore.\n+   *\n+   * For example:\n+   * For char stream \"2.3\", \"2.\" is not a valid decimal token, because it is followed by digit '3'.\n+   * For char stream \"2.3_\", \"2.3\" is not a valid decimal token, because it is followed by '_'.\n+   * For char stream \"2.3W\", \"2.3\" is not a valid decimal token, because it is followed by 'W'.\n+   * For char stream \"12.0D 34.E2+0.12 \"  12.0D is a valid decimal token because it is followed\n+   * by a space. 34.E2 is a valid decimal token because it is followed by symbol '+'\n+   * which is not a digit or letter or underscore.\n+   */\n+  public boolean isValidDecimal() {\n+    int nextChar = _input.LA(1);\n+    if (nextChar >= 'A' && nextChar <= 'Z' || nextChar >= '0' && nextChar <= '9' ||\n+      nextChar == '_') {\n+      return false;\n+    } else {\n+      return true;\n+    }\n+  }\n+\n+  /**\n+   * This method will be called when we see '/*' and try to match it as a bracketed comment.\n+   * If the next character is '+', it should be parsed as hint later, and we cannot match\n+   * it as a bracketed comment.\n+   *\n+   * Returns true if the next character is '+'.\n+   */\n+  public boolean isHint() {\n+    int nextChar = _input.LA(1);\n+    if (nextChar == '+') {\n+      return true;\n+    } else {\n+      return false;\n+    }\n+  }\n+\n+  /**\n+   * This method will be called when the character stream ends and try to find out the\n+   * unclosed bracketed comment.\n+   * If the method be called, it means the end of the entire character stream match,\n+   * and we set the flag and fail later.\n+   */\n+  public void markUnclosedComment() {\n+    has_unclosed_bracketed_comment = true;\n+  }\n+}\n+\n+\n+extendStatement\n+    : statement SEMICOLON* EOF\n+    ;\n+\n+statement\n+    : createTableHeader colListAndPk tableProvider?\n+        createTableClauses\n+        (AS? query)?                                                   #createTableWithPk\n+    | EXPLAIN (LOGICAL | FORMATTED | EXTENDED | CODEGEN | COST)?\n+        statement                                                      #explain\n+    ;\n+\n+createTableHeader\n+    : CREATE TEMPORARY? EXTERNAL? TABLE (IF NOT EXISTS)? multipartIdentifier\n+    ;\n+\n+colListAndPk\n+    : LEFT_PAREN colTypeList (',' primarySpec )? RIGHT_PAREN             #colListWithPk\n+    | primarySpec                                                        #colListOnlyPk\n+    ;\n+\n+primarySpec\n+    : PRIMARY KEY identifierList\n+    ;\n+\n+bucketSpec\n+    : CLUSTERED BY identifierList\n+      (SORTED BY orderedIdentifierList)?\n+      INTO INTEGER_VALUE BUCKETS\n+    ;\n+\n+skewSpec\n+    : SKEWED BY identifierList\n+      ON (constantList | nestedConstantList)\n+      (STORED AS DIRECTORIES)?\n+    ;\n+\n+locationSpec\n+    : LOCATION STRING\n+    ;\n+\n+commentSpec\n+    : COMMENT STRING\n+    ;\n+\n+query\n+    : ctes? queryTerm queryOrganization\n+    ;\n+\n+ctes\n+    : WITH namedQuery (COMMA namedQuery)*\n+    ;\n+\n+namedQuery\n+    : name=errorCapturingIdentifier (columnAliases=identifierList)? AS? LEFT_PAREN query RIGHT_PAREN\n+    ;\n+\n+tableProvider\n+    : USING multipartIdentifier\n+    ;\n+\n+createTableClauses\n+    :((OPTIONS options=propertyList) |\n+     (PARTITIONED BY partitioning=partitionFieldList) |\n+     skewSpec |\n+     bucketSpec |\n+     rowFormat |\n+     createFileFormat |\n+     locationSpec |\n+     commentSpec |\n+     (TBLPROPERTIES tableProps=propertyList))*\n+    ;\n+\n+propertyList\n+    : LEFT_PAREN property (COMMA property)* RIGHT_PAREN\n+    ;\n+\n+property\n+    : key=propertyKey (EQ? value=propertyValue)?\n+    ;\n+\n+propertyKey\n+    : identifier (DOT identifier)*\n+    | STRING\n+    ;\n+\n+propertyValue\n+    : INTEGER_VALUE\n+    | DECIMAL_VALUE\n+    | booleanValue\n+    | STRING\n+    ;\n+\n+constantList\n+    : LEFT_PAREN constant (COMMA constant)* RIGHT_PAREN\n+    ;\n+\n+nestedConstantList\n+    : LEFT_PAREN constantList (COMMA constantList)* RIGHT_PAREN\n+    ;\n+\n+createFileFormat\n+    : STORED AS fileFormat\n+    | STORED BY storageHandler\n+    ;\n+\n+fileFormat\n+    : INPUTFORMAT inFmt=STRING OUTPUTFORMAT outFmt=STRING    #tableFileFormat\n+    | identifier                                             #genericFileFormat\n+    ;\n+\n+storageHandler\n+    : STRING (WITH SERDEPROPERTIES propertyList)?\n+    ;\n+\n+\n+queryOrganization\n+    : (ORDER BY order+=sortItem (COMMA order+=sortItem)*)?\n+      (CLUSTER BY clusterBy+=expression (COMMA clusterBy+=expression)*)?\n+      (DISTRIBUTE BY distributeBy+=expression (COMMA distributeBy+=expression)*)?\n+      (SORT BY sort+=sortItem (COMMA sort+=sortItem)*)?\n+      windowClause?\n+      (LIMIT (ALL | limit=expression))?\n+    ;\n+\n+queryTerm\n+    : queryPrimary                                                                       #queryTermDefault\n+    | left=queryTerm {legacy_setops_precedence_enabled}?\n+        operator=(INTERSECT | UNION | EXCEPT | SETMINUS) setQuantifier? right=queryTerm  #setOperation\n+    | left=queryTerm {!legacy_setops_precedence_enabled}?\n+        operator=INTERSECT setQuantifier? right=queryTerm                                #setOperation\n+    | left=queryTerm {!legacy_setops_precedence_enabled}?\n+        operator=(UNION | EXCEPT | SETMINUS) setQuantifier? right=queryTerm              #setOperation\n+    ;\n+\n+queryPrimary\n+    : querySpecification                                                    #queryPrimaryDefault\n+    | fromStatement                                                         #fromStmt\n+    | TABLE multipartIdentifier                                             #table\n+    | inlineTable                                                           #inlineTableDefault1\n+    | LEFT_PAREN query RIGHT_PAREN                                          #subquery\n+    ;\n+\n+sortItem\n+    : expression ordering=(ASC | DESC)? (NULLS nullOrder=(LAST | FIRST))?\n+    ;\n+\n+fromStatement\n+    : fromClause fromStatementBody+\n+    ;\n+\n+fromStatementBody\n+    : transformClause\n+      whereClause?\n+      queryOrganization\n+    | selectClause\n+      lateralView*\n+      whereClause?\n+      aggregationClause?\n+      havingClause?\n+      windowClause?\n+      queryOrganization\n+    ;\n+\n+querySpecification\n+    : transformClause\n+      fromClause?\n+      lateralView*\n+      whereClause?\n+      aggregationClause?\n+      havingClause?\n+      windowClause?                                                         #transformQuerySpecification\n+    | selectClause\n+      fromClause?\n+      lateralView*\n+      whereClause?\n+      aggregationClause?\n+      havingClause?\n+      windowClause?                                                         #regularQuerySpecification\n+    ;\n+\n+transformClause\n+    : (SELECT kind=TRANSFORM LEFT_PAREN setQuantifier? expressionSeq RIGHT_PAREN\n+            | kind=MAP setQuantifier? expressionSeq\n+            | kind=REDUCE setQuantifier? expressionSeq)\n+      inRowFormat=rowFormat?\n+      (RECORDWRITER recordWriter=STRING)?\n+      USING script=STRING\n+      (AS (identifierSeq | colTypeList | (LEFT_PAREN (identifierSeq | colTypeList) RIGHT_PAREN)))?\n+      outRowFormat=rowFormat?\n+      (RECORDREADER recordReader=STRING)?\n+    ;\n+\n+selectClause\n+    : SELECT (hints+=hint)* setQuantifier? namedExpressionSeq\n+    ;\n+\n+whereClause\n+    : WHERE booleanExpression\n+    ;\n+\n+havingClause\n+    : HAVING booleanExpression\n+    ;\n+\n+hint\n+    : HENT_START hintStatements+=hintStatement (COMMA? hintStatements+=hintStatement)* HENT_END\n+    ;\n+\n+hintStatement\n+    : hintName=identifier\n+    | hintName=identifier LEFT_PAREN parameters+=primaryExpression (COMMA parameters+=primaryExpression)* RIGHT_PAREN\n+    ;\n+\n+fromClause\n+    : FROM relation (COMMA relation)* lateralView* pivotClause?\n+    ;\n+\n+temporalClause\n+    : FOR? (SYSTEM_VERSION | VERSION) AS OF version=(INTEGER_VALUE | STRING)\n+    | FOR? (SYSTEM_TIME | TIMESTAMP) AS OF timestamp=valueExpression\n+    ;\n+\n+aggregationClause\n+    : GROUP BY groupingExpressionsWithGroupingAnalytics+=groupByClause\n+        (COMMA groupingExpressionsWithGroupingAnalytics+=groupByClause)*\n+    | GROUP BY groupingExpressions+=expression (COMMA groupingExpressions+=expression)* (\n+      WITH kind=ROLLUP\n+    | WITH kind=CUBE\n+    | kind=GROUPING SETS LEFT_PAREN groupingSet (COMMA groupingSet)* RIGHT_PAREN)?\n+    ;\n+\n+groupByClause\n+    : groupingAnalytics\n+    | expression\n+    ;\n+\n+groupingAnalytics\n+    : (ROLLUP | CUBE) LEFT_PAREN groupingSet (COMMA groupingSet)* RIGHT_PAREN\n+    | GROUPING SETS LEFT_PAREN groupingElement (COMMA groupingElement)* RIGHT_PAREN\n+    ;\n+\n+groupingElement\n+    : groupingAnalytics\n+    | groupingSet\n+    ;\n+\n+groupingSet\n+    : LEFT_PAREN (expression (COMMA expression)*)? RIGHT_PAREN\n+    | expression\n+    ;\n+\n+pivotClause\n+    : PIVOT LEFT_PAREN aggregates=namedExpressionSeq FOR pivotColumn IN LEFT_PAREN pivotValues+=pivotValue (COMMA pivotValues+=pivotValue)* RIGHT_PAREN RIGHT_PAREN\n+    ;\n+\n+pivotColumn\n+    : identifiers+=identifier\n+    | LEFT_PAREN identifiers+=identifier (COMMA identifiers+=identifier)* RIGHT_PAREN\n+    ;\n+\n+pivotValue\n+    : expression (AS? identifier)?\n+    ;\n+\n+lateralView\n+    : LATERAL VIEW (OUTER)? qualifiedName LEFT_PAREN (expression (COMMA expression)*)? RIGHT_PAREN tblName=identifier (AS? colName+=identifier (COMMA colName+=identifier)*)?\n+    ;\n+\n+setQuantifier\n+    : DISTINCT\n+    | ALL\n+    ;\n+\n+relation\n+    : LATERAL? relationPrimary joinRelation*\n+    ;\n+\n+joinRelation\n+    : (joinType) JOIN LATERAL? right=relationPrimary joinCriteria?\n+    | NATURAL joinType JOIN LATERAL? right=relationPrimary\n+    ;\n+\n+joinType\n+    : INNER?\n+    | CROSS\n+    | LEFT OUTER?\n+    | LEFT? SEMI\n+    | RIGHT OUTER?\n+    | FULL OUTER?\n+    | LEFT? ANTI\n+    ;\n+\n+joinCriteria\n+    : ON booleanExpression\n+    | USING identifierList\n+    ;\n+\n+sample\n+    : TABLESAMPLE LEFT_PAREN sampleMethod? RIGHT_PAREN (REPEATABLE LEFT_PAREN seed=INTEGER_VALUE RIGHT_PAREN)?\n+    ;\n+\n+sampleMethod\n+    : negativeSign=MINUS? percentage=(INTEGER_VALUE | DECIMAL_VALUE) PERCENTLIT   #sampleByPercentile\n+    | expression ROWS                                                             #sampleByRows\n+    | sampleType=BUCKET numerator=INTEGER_VALUE OUT OF denominator=INTEGER_VALUE\n+        (ON (identifier | qualifiedName LEFT_PAREN RIGHT_PAREN))?                 #sampleByBucket\n+    | bytes=expression                                                            #sampleByBytes\n+    ;\n+\n+identifierList\n+    : L\n...<truncated>...",
    "section": "code",
    "embedding": "[-0.028086103,0.028472481,0.011751823,0.00028510529,0.044841696,-0.033211455,0.048658654,0.023325998,-0.0025757372,-0.030038178,-0.018708777,-0.046381947,-0.05139867,0.026343893,0.01546269,0.06412449,0.065153554,0.010357108,0.015323243,-0.018894544,-0.031290505,-0.020508958,-0.025206735,0.023803396,-0.02030666,0.060157593,0.013846661,0.009670282,-0.066043794,-0.009341396,0.0900696,"
  },
  {
    "pr_id": "e2e361f7-5f72-4045-9534-c2cb38d536b9",
    "content": "@@ -0,0 +1,2 @@\n+*.tokens\n+*.interp\n\\ No newline at end of file",
    "section": "code",
    "embedding": "[-0.02688007,-0.0007283893,-0.031594552,-0.015455488,0.015024072,0.023593273,0.03396268,-0.0013756776,-0.026789369,-0.051527336,-0.036515966,-0.027399998,-0.05307942,-0.0010121204,-1.3629925e-05,0.056185886,0.011742849,-0.013589721,0.030070813,-0.017790053,-0.018678453,0.020436082,0.0065539754,-0.018995885,-0.013235601,-0.028433787,-0.022517363,-0.02910057,-0.07644907,0.025008097,0"
  },
  {
    "pr_id": "e2e361f7-5f72-4045-9534-c2cb38d536b9",
    "content": "@@ -0,0 +1,2163 @@\n+// Generated from org/apache/amoro/spark/sql/parser/MixedFormatSqlExtend.g4 by ANTLR 4.8\n+package org.apache.amoro.spark.sql.parser;\n+\n+import org.antlr.v4.runtime.ParserRuleContext;\n+import org.antlr.v4.runtime.tree.ErrorNode;\n+import org.antlr.v4.runtime.tree.TerminalNode;\n+\n+/**\n+ * This class provides an empty implementation of {@link org.apache.amoro.spark.sql.parser.MixedFormatSqlExtendListener},\n+ * which can be extended to create a listener which only needs to handle a subset\n+ * of the available methods.\n+ */\n+public class MixedFormatSqlExtendBaseListener implements MixedFormatSqlExtendListener {\n+\t/**\n+\t * {@inheritDoc}\n+\t *\n+\t * <p>The default implementation does nothing.</p>\n+\t */\n+\t@Override public void enterExtendStatement(MixedFormatSqlExtendParser.ExtendStatementContext ctx) { }\n+\t/**\n+\t * {@inheritDoc}\n+\t *\n+\t * <p>The default implementation does nothing.</p>\n+\t */\n+\t@Override public void exitExtendStatement(MixedFormatSqlExtendParser.ExtendStatementContext ctx) { }\n+\t/**\n+\t * {@inheritDoc}\n+\t *\n+\t * <p>The default implementation does nothing.</p>\n+\t */\n+\t@Override public void enterCreateTableWithPk(MixedFormatSqlExtendParser.CreateTableWithPkContext ctx) { }\n+\t/**\n+\t * {@inheritDoc}\n+\t *\n+\t * <p>The default implementation does nothing.</p>\n+\t */\n+\t@Override public void exitCreateTableWithPk(MixedFormatSqlExtendParser.CreateTableWithPkContext ctx) { }\n+\t/**\n+\t * {@inheritDoc}\n+\t *\n+\t * <p>The default implementation does nothing.</p>\n+\t */\n+\t@Override public void enterExplain(MixedFormatSqlExtendParser.ExplainContext ctx) { }\n+\t/**\n+\t * {@inheritDoc}\n+\t *\n+\t * <p>The default implementation does nothing.</p>\n+\t */\n+\t@Override public void exitExplain(MixedFormatSqlExtendParser.ExplainContext ctx) { }\n+\t/**\n+\t * {@inheritDoc}\n+\t *\n+\t * <p>The default implementation does nothing.</p>\n+\t */\n+\t@Override public void enterCreateTableHeader(MixedFormatSqlExtendParser.CreateTableHeaderContext ctx) { }\n+\t/**\n+\t * {@inheritDoc}\n+\t *\n+\t * <p>The default implementation does nothing.</p>\n+\t */\n+\t@Override public void exitCreateTableHeader(MixedFormatSqlExtendParser.CreateTableHeaderContext ctx) { }\n+\t/**\n+\t * {@inheritDoc}\n+\t *\n+\t * <p>The default implementation does nothing.</p>\n+\t */\n+\t@Override public void enterColListWithPk(MixedFormatSqlExtendParser.ColListWithPkContext ctx) { }\n+\t/**\n+\t * {@inheritDoc}\n+\t *\n+\t * <p>The default implementation does nothing.</p>\n+\t */\n+\t@Override public void exitColListWithPk(MixedFormatSqlExtendParser.ColListWithPkContext ctx) { }\n+\t/**\n+\t * {@inheritDoc}\n+\t *\n+\t * <p>The default implementation does nothing.</p>\n+\t */\n+\t@Override public void enterColListOnlyPk(MixedFormatSqlExtendParser.ColListOnlyPkContext ctx) { }\n+\t/**\n+\t * {@inheritDoc}\n+\t *\n+\t * <p>The default implementation does nothing.</p>\n+\t */\n+\t@Override public void exitColListOnlyPk(MixedFormatSqlExtendParser.ColListOnlyPkContext ctx) { }\n+\t/**\n+\t * {@inheritDoc}\n+\t *\n+\t * <p>The default implementation does nothing.</p>\n+\t */\n+\t@Override public void enterPrimarySpec(MixedFormatSqlExtendParser.PrimarySpecContext ctx) { }\n+\t/**\n+\t * {@inheritDoc}\n+\t *\n+\t * <p>The default implementation does nothing.</p>\n+\t */\n+\t@Override public void exitPrimarySpec(MixedFormatSqlExtendParser.PrimarySpecContext ctx) { }\n+\t/**\n+\t * {@inheritDoc}\n+\t *\n+\t * <p>The default implementation does nothing.</p>\n+\t */\n+\t@Override public void enterBucketSpec(MixedFormatSqlExtendParser.BucketSpecContext ctx) { }\n+\t/**\n+\t * {@inheritDoc}\n+\t *\n+\t * <p>The default implementation does nothing.</p>\n+\t */\n+\t@Override public void exitBucketSpec(MixedFormatSqlExtendParser.BucketSpecContext ctx) { }\n+\t/**\n+\t * {@inheritDoc}\n+\t *\n+\t * <p>The default implementation does nothing.</p>\n+\t */\n+\t@Override public void enterSkewSpec(MixedFormatSqlExtendParser.SkewSpecContext ctx) { }\n+\t/**\n+\t * {@inheritDoc}\n+\t *\n+\t * <p>The default implementation does nothing.</p>\n+\t */\n+\t@Override public void exitSkewSpec(MixedFormatSqlExtendParser.SkewSpecContext ctx) { }\n+\t/**\n+\t * {@inheritDoc}\n+\t *\n+\t * <p>The default implementation does nothing.</p>\n+\t */\n+\t@Override public void enterLocationSpec(MixedFormatSqlExtendParser.LocationSpecContext ctx) { }\n+\t/**\n+\t * {@inheritDoc}\n+\t *\n+\t * <p>The default implementation does nothing.</p>\n+\t */\n+\t@Override public void exitLocationSpec(MixedFormatSqlExtendParser.LocationSpecContext ctx) { }\n+\t/**\n+\t * {@inheritDoc}\n+\t *\n+\t * <p>The default implementation does nothing.</p>\n+\t */\n+\t@Override public void enterCommentSpec(MixedFormatSqlExtendParser.CommentSpecContext ctx) { }\n+\t/**\n+\t * {@inheritDoc}\n+\t *\n+\t * <p>The default implementation does nothing.</p>\n+\t */\n+\t@Override public void exitCommentSpec(MixedFormatSqlExtendParser.CommentSpecContext ctx) { }\n+\t/**\n+\t * {@inheritDoc}\n+\t *\n+\t * <p>The default implementation does nothing.</p>\n+\t */\n+\t@Override public void enterQuery(MixedFormatSqlExtendParser.QueryContext ctx) { }\n+\t/**\n+\t * {@inheritDoc}\n+\t *\n+\t * <p>The default implementation does nothing.</p>\n+\t */\n+\t@Override public void exitQuery(MixedFormatSqlExtendParser.QueryContext ctx) { }\n+\t/**\n+\t * {@inheritDoc}\n+\t *\n+\t * <p>The default implementation does nothing.</p>\n+\t */\n+\t@Override public void enterCtes(MixedFormatSqlExtendParser.CtesContext ctx) { }\n+\t/**\n+\t * {@inheritDoc}\n+\t *\n+\t * <p>The default implementation does nothing.</p>\n+\t */\n+\t@Override public void exitCtes(MixedFormatSqlExtendParser.CtesContext ctx) { }\n+\t/**\n+\t * {@inheritDoc}\n+\t *\n+\t * <p>The default implementation does nothing.</p>\n+\t */\n+\t@Override public void enterNamedQuery(MixedFormatSqlExtendParser.NamedQueryContext ctx) { }\n+\t/**\n+\t * {@inheritDoc}\n+\t *\n+\t * <p>The default implementation does nothing.</p>\n+\t */\n+\t@Override public void exitNamedQuery(MixedFormatSqlExtendParser.NamedQueryContext ctx) { }\n+\t/**\n+\t * {@inheritDoc}\n+\t *\n+\t * <p>The default implementation does nothing.</p>\n+\t */\n+\t@Override public void enterTableProvider(MixedFormatSqlExtendParser.TableProviderContext ctx) { }\n+\t/**\n+\t * {@inheritDoc}\n+\t *\n+\t * <p>The default implementation does nothing.</p>\n+\t */\n+\t@Override public void exitTableProvider(MixedFormatSqlExtendParser.TableProviderContext ctx) { }\n+\t/**\n+\t * {@inheritDoc}\n+\t *\n+\t * <p>The default implementation does nothing.</p>\n+\t */\n+\t@Override public void enterCreateTableClauses(MixedFormatSqlExtendParser.CreateTableClausesContext ctx) { }\n+\t/**\n+\t * {@inheritDoc}\n+\t *\n+\t * <p>The default implementation does nothing.</p>\n+\t */\n+\t@Override public void exitCreateTableClauses(MixedFormatSqlExtendParser.CreateTableClausesContext ctx) { }\n+\t/**\n+\t * {@inheritDoc}\n+\t *\n+\t * <p>The default implementation does nothing.</p>\n+\t */\n+\t@Override public void enterPropertyList(MixedFormatSqlExtendParser.PropertyListContext ctx) { }\n+\t/**\n+\t * {@inheritDoc}\n+\t *\n+\t * <p>The default implementation does nothing.</p>\n+\t */\n+\t@Override public void exitPropertyList(MixedFormatSqlExtendParser.PropertyListContext ctx) { }\n+\t/**\n+\t * {@inheritDoc}\n+\t *\n+\t * <p>The default implementation does nothing.</p>\n+\t */\n+\t@Override public void enterProperty(MixedFormatSqlExtendParser.PropertyContext ctx) { }\n+\t/**\n+\t * {@inheritDoc}\n+\t *\n+\t * <p>The default implementation does nothing.</p>\n+\t */\n+\t@Override public void exitProperty(MixedFormatSqlExtendParser.PropertyContext ctx) { }\n+\t/**\n+\t * {@inheritDoc}\n+\t *\n+\t * <p>The default implementation does nothing.</p>\n+\t */\n+\t@Override public void enterPropertyKey(MixedFormatSqlExtendParser.PropertyKeyContext ctx) { }\n+\t/**\n+\t * {@inheritDoc}\n+\t *\n+\t * <p>The default implementation does nothing.</p>\n+\t */\n+\t@Override public void exitPropertyKey(MixedFormatSqlExtendParser.PropertyKeyContext ctx) { }\n+\t/**\n+\t * {@inheritDoc}\n+\t *\n+\t * <p>The default implementation does nothing.</p>\n+\t */\n+\t@Override public void enterPropertyValue(MixedFormatSqlExtendParser.PropertyValueContext ctx) { }\n+\t/**\n+\t * {@inheritDoc}\n+\t *\n+\t * <p>The default implementation does nothing.</p>\n+\t */\n+\t@Override public void exitPropertyValue(MixedFormatSqlExtendParser.PropertyValueContext ctx) { }\n+\t/**\n+\t * {@inheritDoc}\n+\t *\n+\t * <p>The default implementation does nothing.</p>\n+\t */\n+\t@Override public void enterConstantList(MixedFormatSqlExtendParser.ConstantListContext ctx) { }\n+\t/**\n+\t * {@inheritDoc}\n+\t *\n+\t * <p>The default implementation does nothing.</p>\n+\t */\n+\t@Override public void exitConstantList(MixedFormatSqlExtendParser.ConstantListContext ctx) { }\n+\t/**\n+\t * {@inheritDoc}\n+\t *\n+\t * <p>The default implementation does nothing.</p>\n+\t */\n+\t@Override public void enterNestedConstantList(MixedFormatSqlExtendParser.NestedConstantListContext ctx) { }\n+\t/**\n+\t * {@inheritDoc}\n+\t *\n+\t * <p>The default implementation does nothing.</p>\n+\t */\n+\t@Override public void exitNestedConstantList(MixedFormatSqlExtendParser.NestedConstantListContext ctx) { }\n+\t/**\n+\t * {@inheritDoc}\n+\t *\n+\t * <p>The default implementation does nothing.</p>\n+\t */\n+\t@Override public void enterCreateFileFormat(MixedFormatSqlExtendParser.CreateFileFormatContext ctx) { }\n+\t/**\n+\t * {@inheritDoc}\n+\t *\n+\t * <p>The default implementation does nothing.</p>\n+\t */\n+\t@Override public void exitCreateFileFormat(MixedFormatSqlExtendParser.CreateFileFormatContext ctx) { }\n+\t/**\n+\t * {@inheritDoc}\n+\t *\n+\t * <p>The default implementation does nothing.</p>\n+\t */\n+\t@Override public void enterTableFileFormat(MixedFormatSqlExtendParser.TableFileFormatContext ctx) { }\n+\t/**\n+\t * {@inheritDoc}\n+\t *\n+\t * <p>The default implementation does nothing.</p>\n+\t */\n+\t@Override public void exitTableFileFormat(MixedFormatSqlExtendParser.TableFileFormatContext ctx) { }\n+\t/**\n+\t * {@inheritDoc}\n+\t *\n+\t * <p>The default implementation does nothing.</p>\n+\t */\n+\t@Override public void enterGenericFileFormat(MixedFormatSqlExtendParser.GenericFileFormatContext ctx) { }\n+\t/**\n+\t * {@inheritDoc}\n+\t *\n+\t * <p>The default implementation does nothing.</p>\n+\t */\n+\t@Override public void exitGenericFileFormat(MixedFormatSqlExtendParser.GenericFileFormatContext ctx) { }\n+\t/**\n+\t * {@inheritDoc}\n+\t *\n+\t * <p>The default implementation does nothing.</p>\n+\t */\n+\t@Override public void enterStorageHandler(MixedFormatSqlExtendParser.StorageHandlerContext ctx) { }\n+\t/**\n+\t * {@inheritDoc}\n+\t *\n+\t * <p>The default implementation does nothing.</p>\n+\t */\n+\t@Override public void exitStorageHandler(MixedFormatSqlExtendParser.StorageHandlerContext ctx) { }\n+\t/**\n+\t * {@inheritDoc}\n+\t *\n+\t * <p>The default implementation does nothing.</p>\n+\t */\n+\t@Override public void enterQueryOrganization(MixedFormatSqlExtendParser.QueryOrganizationContext ctx) { }\n+\t/**\n+\t * {@inheritDoc}\n+\t *\n+\t * <p>The default implementation does nothing.</p>\n+\t */\n+\t@Override public void exitQueryOrganization(MixedFormatSqlExtendParser.QueryOrganizationContext ctx) { }\n+\t/**\n+\t * {@inheritDoc}\n+\t *\n+\t * <p>The default implementation does nothing.</p>\n+\t */\n+\t@Override public void enterQueryTermDefault(MixedFormatSqlExtendParser.QueryTermDefaultContext ctx) { }\n+\t/**\n+\t * {@inheritDoc}\n+\t *\n+\t * <p>The default implementation does nothing.</p>\n+\t */\n+\t@Override public void exitQueryTermDefault(MixedFormatSqlExtendParser.QueryTermDefaultContext ctx) { }\n+\t/**\n+\t * {@inheritDoc}\n+\t *\n+\t * <p>The default implementation does nothing.</p>\n+\t */\n+\t@Override public void enterSetOperation(MixedFormatSqlExtendParser.SetOperationContext ctx) { }\n+\t/**\n+\t * {@inheritDoc}\n+\t *\n+\t * <p>The default implementation does nothing.</p>\n+\t */\n+\t@Override public void exitSetOperation(MixedFormatSqlExtendParser.SetOperationContext ctx) { }\n+\t/**\n+\t * {@inheritDoc}\n+\t *\n+\t * <p>The default implementation does nothing.</p>\n+\t */\n+\t@Override public void enterQueryPrimaryDefault(MixedFormatSqlExtendParser.QueryPrimaryDefaultContext ctx) { }\n+\t/**\n+\t * {@inheritDoc}\n+\t *\n+\t * <p>The default implementation does nothing.</p>\n+\t */\n+\t@Override public void exitQueryPrimaryDefault(MixedFormatSqlExtendParser.QueryPrimaryDefaultContext ctx) { }\n+\t/**\n+\t * {@inheritDoc}\n+\t *\n+\t * <p>The default implementation does nothing.</p\n...<truncated>...",
    "section": "code",
    "embedding": "[-0.028760623,-0.022910716,-0.010617364,-0.010160562,0.021665491,-0.02594014,0.0635307,0.023083521,-0.013860269,-0.048104454,-0.042951398,-0.0072107683,-0.07491213,0.030181136,0.0006405683,0.077932045,0.049054038,-0.016342675,0.01325469,-0.016360238,0.035745572,-0.0017379827,-0.030082097,0.04289341,0.00031824605,0.02993881,0.012687799,-0.0125872865,-0.064082704,-0.027935443,0.05751"
  },
  {
    "pr_id": "b2e982f0-8b05-43bf-a510-04e8a37c4771",
    "content": "@@ -23,7 +23,7 @@ To learn more about the project, visit [answer.apache.org](https://answer.apache\n ### Running with docker\n \n ```bash\n-docker run -d -p 9080:80 -v answer-data:/data --name answer apache/answer:1.7.0\n+docker run -d -p 9080:80 -v answer-data:/data --name answer apache/answer:1.7.1\n ```\n \n For more information, see [Installation](https://answer.apache.org/docs/installation).",
    "section": "code",
    "embedding": "[0.008830708,0.010447197,0.018867217,0.0030125873,0.025554445,-0.0044209408,0.05161208,-0.0120116025,-0.03271352,-0.066546656,0.02015238,0.010397292,-0.05991901,0.04005087,0.02041365,0.012505736,0.05614283,0.0016350411,0.013681531,0.05051684,0.003752461,-0.014863298,-0.023756942,0.024839321,-0.062100187,-0.014535732,0.076894045,0.014558984,-0.0678957,-0.06893801,0.03360756,-0.02981"
  },
  {
    "pr_id": "e2e361f7-5f72-4045-9534-c2cb38d536b9",
    "content": "@@ -0,0 +1,1253 @@\n+// Generated from org/apache/amoro/spark/sql/parser/MixedFormatSqlExtend.g4 by ANTLR 4.8\n+package org.apache.amoro.spark.sql.parser;\n+import org.antlr.v4.runtime.tree.AbstractParseTreeVisitor;\n+\n+/**\n+ * This class provides an empty implementation of {@link org.apache.amoro.spark.sql.parser.MixedFormatSqlExtendVisitor},\n+ * which can be extended to create a visitor which only needs to handle a subset\n+ * of the available methods.\n+ *\n+ * @param <T> The return type of the visit operation. Use {@link Void} for\n+ * operations with no return type.\n+ */\n+public class MixedFormatSqlExtendBaseVisitor<T> extends AbstractParseTreeVisitor<T> implements MixedFormatSqlExtendVisitor<T> {\n+\t/**\n+\t * {@inheritDoc}\n+\t *\n+\t * <p>The default implementation returns the result of calling\n+\t * {@link #visitChildren} on {@code ctx}.</p>\n+\t */\n+\t@Override public T visitExtendStatement(MixedFormatSqlExtendParser.ExtendStatementContext ctx) { return visitChildren(ctx); }\n+\t/**\n+\t * {@inheritDoc}\n+\t *\n+\t * <p>The default implementation returns the result of calling\n+\t * {@link #visitChildren} on {@code ctx}.</p>\n+\t */\n+\t@Override public T visitCreateTableWithPk(MixedFormatSqlExtendParser.CreateTableWithPkContext ctx) { return visitChildren(ctx); }\n+\t/**\n+\t * {@inheritDoc}\n+\t *\n+\t * <p>The default implementation returns the result of calling\n+\t * {@link #visitChildren} on {@code ctx}.</p>\n+\t */\n+\t@Override public T visitExplain(MixedFormatSqlExtendParser.ExplainContext ctx) { return visitChildren(ctx); }\n+\t/**\n+\t * {@inheritDoc}\n+\t *\n+\t * <p>The default implementation returns the result of calling\n+\t * {@link #visitChildren} on {@code ctx}.</p>\n+\t */\n+\t@Override public T visitCreateTableHeader(MixedFormatSqlExtendParser.CreateTableHeaderContext ctx) { return visitChildren(ctx); }\n+\t/**\n+\t * {@inheritDoc}\n+\t *\n+\t * <p>The default implementation returns the result of calling\n+\t * {@link #visitChildren} on {@code ctx}.</p>\n+\t */\n+\t@Override public T visitColListWithPk(MixedFormatSqlExtendParser.ColListWithPkContext ctx) { return visitChildren(ctx); }\n+\t/**\n+\t * {@inheritDoc}\n+\t *\n+\t * <p>The default implementation returns the result of calling\n+\t * {@link #visitChildren} on {@code ctx}.</p>\n+\t */\n+\t@Override public T visitColListOnlyPk(MixedFormatSqlExtendParser.ColListOnlyPkContext ctx) { return visitChildren(ctx); }\n+\t/**\n+\t * {@inheritDoc}\n+\t *\n+\t * <p>The default implementation returns the result of calling\n+\t * {@link #visitChildren} on {@code ctx}.</p>\n+\t */\n+\t@Override public T visitPrimarySpec(MixedFormatSqlExtendParser.PrimarySpecContext ctx) { return visitChildren(ctx); }\n+\t/**\n+\t * {@inheritDoc}\n+\t *\n+\t * <p>The default implementation returns the result of calling\n+\t * {@link #visitChildren} on {@code ctx}.</p>\n+\t */\n+\t@Override public T visitBucketSpec(MixedFormatSqlExtendParser.BucketSpecContext ctx) { return visitChildren(ctx); }\n+\t/**\n+\t * {@inheritDoc}\n+\t *\n+\t * <p>The default implementation returns the result of calling\n+\t * {@link #visitChildren} on {@code ctx}.</p>\n+\t */\n+\t@Override public T visitSkewSpec(MixedFormatSqlExtendParser.SkewSpecContext ctx) { return visitChildren(ctx); }\n+\t/**\n+\t * {@inheritDoc}\n+\t *\n+\t * <p>The default implementation returns the result of calling\n+\t * {@link #visitChildren} on {@code ctx}.</p>\n+\t */\n+\t@Override public T visitLocationSpec(MixedFormatSqlExtendParser.LocationSpecContext ctx) { return visitChildren(ctx); }\n+\t/**\n+\t * {@inheritDoc}\n+\t *\n+\t * <p>The default implementation returns the result of calling\n+\t * {@link #visitChildren} on {@code ctx}.</p>\n+\t */\n+\t@Override public T visitCommentSpec(MixedFormatSqlExtendParser.CommentSpecContext ctx) { return visitChildren(ctx); }\n+\t/**\n+\t * {@inheritDoc}\n+\t *\n+\t * <p>The default implementation returns the result of calling\n+\t * {@link #visitChildren} on {@code ctx}.</p>\n+\t */\n+\t@Override public T visitQuery(MixedFormatSqlExtendParser.QueryContext ctx) { return visitChildren(ctx); }\n+\t/**\n+\t * {@inheritDoc}\n+\t *\n+\t * <p>The default implementation returns the result of calling\n+\t * {@link #visitChildren} on {@code ctx}.</p>\n+\t */\n+\t@Override public T visitCtes(MixedFormatSqlExtendParser.CtesContext ctx) { return visitChildren(ctx); }\n+\t/**\n+\t * {@inheritDoc}\n+\t *\n+\t * <p>The default implementation returns the result of calling\n+\t * {@link #visitChildren} on {@code ctx}.</p>\n+\t */\n+\t@Override public T visitNamedQuery(MixedFormatSqlExtendParser.NamedQueryContext ctx) { return visitChildren(ctx); }\n+\t/**\n+\t * {@inheritDoc}\n+\t *\n+\t * <p>The default implementation returns the result of calling\n+\t * {@link #visitChildren} on {@code ctx}.</p>\n+\t */\n+\t@Override public T visitTableProvider(MixedFormatSqlExtendParser.TableProviderContext ctx) { return visitChildren(ctx); }\n+\t/**\n+\t * {@inheritDoc}\n+\t *\n+\t * <p>The default implementation returns the result of calling\n+\t * {@link #visitChildren} on {@code ctx}.</p>\n+\t */\n+\t@Override public T visitCreateTableClauses(MixedFormatSqlExtendParser.CreateTableClausesContext ctx) { return visitChildren(ctx); }\n+\t/**\n+\t * {@inheritDoc}\n+\t *\n+\t * <p>The default implementation returns the result of calling\n+\t * {@link #visitChildren} on {@code ctx}.</p>\n+\t */\n+\t@Override public T visitPropertyList(MixedFormatSqlExtendParser.PropertyListContext ctx) { return visitChildren(ctx); }\n+\t/**\n+\t * {@inheritDoc}\n+\t *\n+\t * <p>The default implementation returns the result of calling\n+\t * {@link #visitChildren} on {@code ctx}.</p>\n+\t */\n+\t@Override public T visitProperty(MixedFormatSqlExtendParser.PropertyContext ctx) { return visitChildren(ctx); }\n+\t/**\n+\t * {@inheritDoc}\n+\t *\n+\t * <p>The default implementation returns the result of calling\n+\t * {@link #visitChildren} on {@code ctx}.</p>\n+\t */\n+\t@Override public T visitPropertyKey(MixedFormatSqlExtendParser.PropertyKeyContext ctx) { return visitChildren(ctx); }\n+\t/**\n+\t * {@inheritDoc}\n+\t *\n+\t * <p>The default implementation returns the result of calling\n+\t * {@link #visitChildren} on {@code ctx}.</p>\n+\t */\n+\t@Override public T visitPropertyValue(MixedFormatSqlExtendParser.PropertyValueContext ctx) { return visitChildren(ctx); }\n+\t/**\n+\t * {@inheritDoc}\n+\t *\n+\t * <p>The default implementation returns the result of calling\n+\t * {@link #visitChildren} on {@code ctx}.</p>\n+\t */\n+\t@Override public T visitConstantList(MixedFormatSqlExtendParser.ConstantListContext ctx) { return visitChildren(ctx); }\n+\t/**\n+\t * {@inheritDoc}\n+\t *\n+\t * <p>The default implementation returns the result of calling\n+\t * {@link #visitChildren} on {@code ctx}.</p>\n+\t */\n+\t@Override public T visitNestedConstantList(MixedFormatSqlExtendParser.NestedConstantListContext ctx) { return visitChildren(ctx); }\n+\t/**\n+\t * {@inheritDoc}\n+\t *\n+\t * <p>The default implementation returns the result of calling\n+\t * {@link #visitChildren} on {@code ctx}.</p>\n+\t */\n+\t@Override public T visitCreateFileFormat(MixedFormatSqlExtendParser.CreateFileFormatContext ctx) { return visitChildren(ctx); }\n+\t/**\n+\t * {@inheritDoc}\n+\t *\n+\t * <p>The default implementation returns the result of calling\n+\t * {@link #visitChildren} on {@code ctx}.</p>\n+\t */\n+\t@Override public T visitTableFileFormat(MixedFormatSqlExtendParser.TableFileFormatContext ctx) { return visitChildren(ctx); }\n+\t/**\n+\t * {@inheritDoc}\n+\t *\n+\t * <p>The default implementation returns the result of calling\n+\t * {@link #visitChildren} on {@code ctx}.</p>\n+\t */\n+\t@Override public T visitGenericFileFormat(MixedFormatSqlExtendParser.GenericFileFormatContext ctx) { return visitChildren(ctx); }\n+\t/**\n+\t * {@inheritDoc}\n+\t *\n+\t * <p>The default implementation returns the result of calling\n+\t * {@link #visitChildren} on {@code ctx}.</p>\n+\t */\n+\t@Override public T visitStorageHandler(MixedFormatSqlExtendParser.StorageHandlerContext ctx) { return visitChildren(ctx); }\n+\t/**\n+\t * {@inheritDoc}\n+\t *\n+\t * <p>The default implementation returns the result of calling\n+\t * {@link #visitChildren} on {@code ctx}.</p>\n+\t */\n+\t@Override public T visitQueryOrganization(MixedFormatSqlExtendParser.QueryOrganizationContext ctx) { return visitChildren(ctx); }\n+\t/**\n+\t * {@inheritDoc}\n+\t *\n+\t * <p>The default implementation returns the result of calling\n+\t * {@link #visitChildren} on {@code ctx}.</p>\n+\t */\n+\t@Override public T visitQueryTermDefault(MixedFormatSqlExtendParser.QueryTermDefaultContext ctx) { return visitChildren(ctx); }\n+\t/**\n+\t * {@inheritDoc}\n+\t *\n+\t * <p>The default implementation returns the result of calling\n+\t * {@link #visitChildren} on {@code ctx}.</p>\n+\t */\n+\t@Override public T visitSetOperation(MixedFormatSqlExtendParser.SetOperationContext ctx) { return visitChildren(ctx); }\n+\t/**\n+\t * {@inheritDoc}\n+\t *\n+\t * <p>The default implementation returns the result of calling\n+\t * {@link #visitChildren} on {@code ctx}.</p>\n+\t */\n+\t@Override public T visitQueryPrimaryDefault(MixedFormatSqlExtendParser.QueryPrimaryDefaultContext ctx) { return visitChildren(ctx); }\n+\t/**\n+\t * {@inheritDoc}\n+\t *\n+\t * <p>The default implementation returns the result of calling\n+\t * {@link #visitChildren} on {@code ctx}.</p>\n+\t */\n+\t@Override public T visitFromStmt(MixedFormatSqlExtendParser.FromStmtContext ctx) { return visitChildren(ctx); }\n+\t/**\n+\t * {@inheritDoc}\n+\t *\n+\t * <p>The default implementation returns the result of calling\n+\t * {@link #visitChildren} on {@code ctx}.</p>\n+\t */\n+\t@Override public T visitTable(MixedFormatSqlExtendParser.TableContext ctx) { return visitChildren(ctx); }\n+\t/**\n+\t * {@inheritDoc}\n+\t *\n+\t * <p>The default implementation returns the result of calling\n+\t * {@link #visitChildren} on {@code ctx}.</p>\n+\t */\n+\t@Override public T visitInlineTableDefault1(MixedFormatSqlExtendParser.InlineTableDefault1Context ctx) { return visitChildren(ctx); }\n+\t/**\n+\t * {@inheritDoc}\n+\t *\n+\t * <p>The default implementation returns the result of calling\n+\t * {@link #visitChildren} on {@code ctx}.</p>\n+\t */\n+\t@Override public T visitSubquery(MixedFormatSqlExtendParser.SubqueryContext ctx) { return visitChildren(ctx); }\n+\t/**\n+\t * {@inheritDoc}\n+\t *\n+\t * <p>The default implementation returns the result of calling\n+\t * {@link #visitChildren} on {@code ctx}.</p>\n+\t */\n+\t@Override public T visitSortItem(MixedFormatSqlExtendParser.SortItemContext ctx) { return visitChildren(ctx); }\n+\t/**\n+\t * {@inheritDoc}\n+\t *\n+\t * <p>The default implementation returns the result of calling\n+\t * {@link #visitChildren} on {@code ctx}.</p>\n+\t */\n+\t@Override public T visitFromStatement(MixedFormatSqlExtendParser.FromStatementContext ctx) { return visitChildren(ctx); }\n+\t/**\n+\t * {@inheritDoc}\n+\t *\n+\t * <p>The default implementation returns the result of calling\n+\t * {@link #visitChildren} on {@code ctx}.</p>\n+\t */\n+\t@Override public T visitFromStatementBody(MixedFormatSqlExtendParser.FromStatementBodyContext ctx) { return visitChildren(ctx); }\n+\t/**\n+\t * {@inheritDoc}\n+\t *\n+\t * <p>The default implementation returns the result of calling\n+\t * {@link #visitChildren} on {@code ctx}.</p>\n+\t */\n+\t@Override public T visitTransformQuerySpecification(MixedFormatSqlExtendParser.TransformQuerySpecificationContext ctx) { return visitChildren(ctx); }\n+\t/**\n+\t * {@inheritDoc}\n+\t *\n+\t * <p>The default implementation returns the result of calling\n+\t * {@link #visitChildren} on {@code ctx}.</p>\n+\t */\n+\t@Override public T visitRegularQuerySpecification(MixedFormatSqlExtendParser.RegularQuerySpecificationContext ctx) { return visitChildren(ctx); }\n+\t/**\n+\t * {@inheritDoc}\n+\t *\n+\t * <p>The default implementation returns the result of calling\n+\t * {@link #visitChildren} on {@code ctx}.</p>\n+\t */\n+\t@Override public T visitTransformClause(MixedFormatSqlExtendParser.TransformClauseContext ctx) { return visitChildren(ctx); }\n+\t/**\n+\t * {@inheritDoc}\n+\t *\n+\t * <p>The default implementation returns the result of calling\n+\t * {@link #visitChildren} on {@code ctx}.</p>\n+\t */\n+\t@Override public T visitSelectClause(MixedFormatSqlExtendParser.SelectClauseContext ctx) { return visitChildren(ctx); }\n+\t/**\n+\t * {@inheritDoc}\n+\t *\n+\t * <p>The default implementation returns the result of \n...<truncated>...",
    "section": "code",
    "embedding": "[-0.022797491,-0.023567248,0.018774847,-0.025114257,0.01680354,-0.012336321,0.057348367,0.03148562,-0.017171934,-0.046868306,-0.04529739,-0.027248912,-0.069002375,0.032853335,0.0018167748,0.061469585,0.03828871,-0.024048284,0.021825885,-0.03530588,0.025575906,0.00015361619,-0.02088425,0.030361708,0.0047819526,0.020620925,0.03303719,-0.011158309,-0.04482939,-0.047797065,0.061033454,"
  },
  {
    "pr_id": "e2e361f7-5f72-4045-9534-c2cb38d536b9",
    "content": "@@ -0,0 +1,1958 @@\n+// Generated from org/apache/amoro/spark/sql/parser/MixedFormatSqlExtend.g4 by ANTLR 4.8\n+package org.apache.amoro.spark.sql.parser;\n+import org.antlr.v4.runtime.tree.ParseTreeListener;\n+\n+/**\n+ * This interface defines a complete listener for a parse tree produced by\n+ * {@link org.apache.amoro.spark.sql.parser.MixedFormatSqlExtendParser}.\n+ */\n+public interface MixedFormatSqlExtendListener extends ParseTreeListener {\n+\t/**\n+\t * Enter a parse tree produced by {@link org.apache.amoro.spark.sql.parser.MixedFormatSqlExtendParser#extendStatement}.\n+\t * @param ctx the parse tree\n+\t */\n+\tvoid enterExtendStatement(MixedFormatSqlExtendParser.ExtendStatementContext ctx);\n+\t/**\n+\t * Exit a parse tree produced by {@link org.apache.amoro.spark.sql.parser.MixedFormatSqlExtendParser#extendStatement}.\n+\t * @param ctx the parse tree\n+\t */\n+\tvoid exitExtendStatement(MixedFormatSqlExtendParser.ExtendStatementContext ctx);\n+\t/**\n+\t * Enter a parse tree produced by the {@code createTableWithPk}\n+\t * labeled alternative in {@link org.apache.amoro.spark.sql.parser.MixedFormatSqlExtendParser#statement}.\n+\t * @param ctx the parse tree\n+\t */\n+\tvoid enterCreateTableWithPk(MixedFormatSqlExtendParser.CreateTableWithPkContext ctx);\n+\t/**\n+\t * Exit a parse tree produced by the {@code createTableWithPk}\n+\t * labeled alternative in {@link org.apache.amoro.spark.sql.parser.MixedFormatSqlExtendParser#statement}.\n+\t * @param ctx the parse tree\n+\t */\n+\tvoid exitCreateTableWithPk(MixedFormatSqlExtendParser.CreateTableWithPkContext ctx);\n+\t/**\n+\t * Enter a parse tree produced by the {@code explain}\n+\t * labeled alternative in {@link org.apache.amoro.spark.sql.parser.MixedFormatSqlExtendParser#statement}.\n+\t * @param ctx the parse tree\n+\t */\n+\tvoid enterExplain(MixedFormatSqlExtendParser.ExplainContext ctx);\n+\t/**\n+\t * Exit a parse tree produced by the {@code explain}\n+\t * labeled alternative in {@link org.apache.amoro.spark.sql.parser.MixedFormatSqlExtendParser#statement}.\n+\t * @param ctx the parse tree\n+\t */\n+\tvoid exitExplain(MixedFormatSqlExtendParser.ExplainContext ctx);\n+\t/**\n+\t * Enter a parse tree produced by {@link org.apache.amoro.spark.sql.parser.MixedFormatSqlExtendParser#createTableHeader}.\n+\t * @param ctx the parse tree\n+\t */\n+\tvoid enterCreateTableHeader(MixedFormatSqlExtendParser.CreateTableHeaderContext ctx);\n+\t/**\n+\t * Exit a parse tree produced by {@link org.apache.amoro.spark.sql.parser.MixedFormatSqlExtendParser#createTableHeader}.\n+\t * @param ctx the parse tree\n+\t */\n+\tvoid exitCreateTableHeader(MixedFormatSqlExtendParser.CreateTableHeaderContext ctx);\n+\t/**\n+\t * Enter a parse tree produced by the {@code colListWithPk}\n+\t * labeled alternative in {@link org.apache.amoro.spark.sql.parser.MixedFormatSqlExtendParser#colListAndPk}.\n+\t * @param ctx the parse tree\n+\t */\n+\tvoid enterColListWithPk(MixedFormatSqlExtendParser.ColListWithPkContext ctx);\n+\t/**\n+\t * Exit a parse tree produced by the {@code colListWithPk}\n+\t * labeled alternative in {@link org.apache.amoro.spark.sql.parser.MixedFormatSqlExtendParser#colListAndPk}.\n+\t * @param ctx the parse tree\n+\t */\n+\tvoid exitColListWithPk(MixedFormatSqlExtendParser.ColListWithPkContext ctx);\n+\t/**\n+\t * Enter a parse tree produced by the {@code colListOnlyPk}\n+\t * labeled alternative in {@link org.apache.amoro.spark.sql.parser.MixedFormatSqlExtendParser#colListAndPk}.\n+\t * @param ctx the parse tree\n+\t */\n+\tvoid enterColListOnlyPk(MixedFormatSqlExtendParser.ColListOnlyPkContext ctx);\n+\t/**\n+\t * Exit a parse tree produced by the {@code colListOnlyPk}\n+\t * labeled alternative in {@link org.apache.amoro.spark.sql.parser.MixedFormatSqlExtendParser#colListAndPk}.\n+\t * @param ctx the parse tree\n+\t */\n+\tvoid exitColListOnlyPk(MixedFormatSqlExtendParser.ColListOnlyPkContext ctx);\n+\t/**\n+\t * Enter a parse tree produced by {@link org.apache.amoro.spark.sql.parser.MixedFormatSqlExtendParser#primarySpec}.\n+\t * @param ctx the parse tree\n+\t */\n+\tvoid enterPrimarySpec(MixedFormatSqlExtendParser.PrimarySpecContext ctx);\n+\t/**\n+\t * Exit a parse tree produced by {@link org.apache.amoro.spark.sql.parser.MixedFormatSqlExtendParser#primarySpec}.\n+\t * @param ctx the parse tree\n+\t */\n+\tvoid exitPrimarySpec(MixedFormatSqlExtendParser.PrimarySpecContext ctx);\n+\t/**\n+\t * Enter a parse tree produced by {@link org.apache.amoro.spark.sql.parser.MixedFormatSqlExtendParser#bucketSpec}.\n+\t * @param ctx the parse tree\n+\t */\n+\tvoid enterBucketSpec(MixedFormatSqlExtendParser.BucketSpecContext ctx);\n+\t/**\n+\t * Exit a parse tree produced by {@link org.apache.amoro.spark.sql.parser.MixedFormatSqlExtendParser#bucketSpec}.\n+\t * @param ctx the parse tree\n+\t */\n+\tvoid exitBucketSpec(MixedFormatSqlExtendParser.BucketSpecContext ctx);\n+\t/**\n+\t * Enter a parse tree produced by {@link org.apache.amoro.spark.sql.parser.MixedFormatSqlExtendParser#skewSpec}.\n+\t * @param ctx the parse tree\n+\t */\n+\tvoid enterSkewSpec(MixedFormatSqlExtendParser.SkewSpecContext ctx);\n+\t/**\n+\t * Exit a parse tree produced by {@link org.apache.amoro.spark.sql.parser.MixedFormatSqlExtendParser#skewSpec}.\n+\t * @param ctx the parse tree\n+\t */\n+\tvoid exitSkewSpec(MixedFormatSqlExtendParser.SkewSpecContext ctx);\n+\t/**\n+\t * Enter a parse tree produced by {@link org.apache.amoro.spark.sql.parser.MixedFormatSqlExtendParser#locationSpec}.\n+\t * @param ctx the parse tree\n+\t */\n+\tvoid enterLocationSpec(MixedFormatSqlExtendParser.LocationSpecContext ctx);\n+\t/**\n+\t * Exit a parse tree produced by {@link org.apache.amoro.spark.sql.parser.MixedFormatSqlExtendParser#locationSpec}.\n+\t * @param ctx the parse tree\n+\t */\n+\tvoid exitLocationSpec(MixedFormatSqlExtendParser.LocationSpecContext ctx);\n+\t/**\n+\t * Enter a parse tree produced by {@link org.apache.amoro.spark.sql.parser.MixedFormatSqlExtendParser#commentSpec}.\n+\t * @param ctx the parse tree\n+\t */\n+\tvoid enterCommentSpec(MixedFormatSqlExtendParser.CommentSpecContext ctx);\n+\t/**\n+\t * Exit a parse tree produced by {@link org.apache.amoro.spark.sql.parser.MixedFormatSqlExtendParser#commentSpec}.\n+\t * @param ctx the parse tree\n+\t */\n+\tvoid exitCommentSpec(MixedFormatSqlExtendParser.CommentSpecContext ctx);\n+\t/**\n+\t * Enter a parse tree produced by {@link org.apache.amoro.spark.sql.parser.MixedFormatSqlExtendParser#query}.\n+\t * @param ctx the parse tree\n+\t */\n+\tvoid enterQuery(MixedFormatSqlExtendParser.QueryContext ctx);\n+\t/**\n+\t * Exit a parse tree produced by {@link org.apache.amoro.spark.sql.parser.MixedFormatSqlExtendParser#query}.\n+\t * @param ctx the parse tree\n+\t */\n+\tvoid exitQuery(MixedFormatSqlExtendParser.QueryContext ctx);\n+\t/**\n+\t * Enter a parse tree produced by {@link org.apache.amoro.spark.sql.parser.MixedFormatSqlExtendParser#ctes}.\n+\t * @param ctx the parse tree\n+\t */\n+\tvoid enterCtes(MixedFormatSqlExtendParser.CtesContext ctx);\n+\t/**\n+\t * Exit a parse tree produced by {@link org.apache.amoro.spark.sql.parser.MixedFormatSqlExtendParser#ctes}.\n+\t * @param ctx the parse tree\n+\t */\n+\tvoid exitCtes(MixedFormatSqlExtendParser.CtesContext ctx);\n+\t/**\n+\t * Enter a parse tree produced by {@link org.apache.amoro.spark.sql.parser.MixedFormatSqlExtendParser#namedQuery}.\n+\t * @param ctx the parse tree\n+\t */\n+\tvoid enterNamedQuery(MixedFormatSqlExtendParser.NamedQueryContext ctx);\n+\t/**\n+\t * Exit a parse tree produced by {@link org.apache.amoro.spark.sql.parser.MixedFormatSqlExtendParser#namedQuery}.\n+\t * @param ctx the parse tree\n+\t */\n+\tvoid exitNamedQuery(MixedFormatSqlExtendParser.NamedQueryContext ctx);\n+\t/**\n+\t * Enter a parse tree produced by {@link org.apache.amoro.spark.sql.parser.MixedFormatSqlExtendParser#tableProvider}.\n+\t * @param ctx the parse tree\n+\t */\n+\tvoid enterTableProvider(MixedFormatSqlExtendParser.TableProviderContext ctx);\n+\t/**\n+\t * Exit a parse tree produced by {@link org.apache.amoro.spark.sql.parser.MixedFormatSqlExtendParser#tableProvider}.\n+\t * @param ctx the parse tree\n+\t */\n+\tvoid exitTableProvider(MixedFormatSqlExtendParser.TableProviderContext ctx);\n+\t/**\n+\t * Enter a parse tree produced by {@link org.apache.amoro.spark.sql.parser.MixedFormatSqlExtendParser#createTableClauses}.\n+\t * @param ctx the parse tree\n+\t */\n+\tvoid enterCreateTableClauses(MixedFormatSqlExtendParser.CreateTableClausesContext ctx);\n+\t/**\n+\t * Exit a parse tree produced by {@link org.apache.amoro.spark.sql.parser.MixedFormatSqlExtendParser#createTableClauses}.\n+\t * @param ctx the parse tree\n+\t */\n+\tvoid exitCreateTableClauses(MixedFormatSqlExtendParser.CreateTableClausesContext ctx);\n+\t/**\n+\t * Enter a parse tree produced by {@link org.apache.amoro.spark.sql.parser.MixedFormatSqlExtendParser#propertyList}.\n+\t * @param ctx the parse tree\n+\t */\n+\tvoid enterPropertyList(MixedFormatSqlExtendParser.PropertyListContext ctx);\n+\t/**\n+\t * Exit a parse tree produced by {@link org.apache.amoro.spark.sql.parser.MixedFormatSqlExtendParser#propertyList}.\n+\t * @param ctx the parse tree\n+\t */\n+\tvoid exitPropertyList(MixedFormatSqlExtendParser.PropertyListContext ctx);\n+\t/**\n+\t * Enter a parse tree produced by {@link org.apache.amoro.spark.sql.parser.MixedFormatSqlExtendParser#property}.\n+\t * @param ctx the parse tree\n+\t */\n+\tvoid enterProperty(MixedFormatSqlExtendParser.PropertyContext ctx);\n+\t/**\n+\t * Exit a parse tree produced by {@link org.apache.amoro.spark.sql.parser.MixedFormatSqlExtendParser#property}.\n+\t * @param ctx the parse tree\n+\t */\n+\tvoid exitProperty(MixedFormatSqlExtendParser.PropertyContext ctx);\n+\t/**\n+\t * Enter a parse tree produced by {@link org.apache.amoro.spark.sql.parser.MixedFormatSqlExtendParser#propertyKey}.\n+\t * @param ctx the parse tree\n+\t */\n+\tvoid enterPropertyKey(MixedFormatSqlExtendParser.PropertyKeyContext ctx);\n+\t/**\n+\t * Exit a parse tree produced by {@link org.apache.amoro.spark.sql.parser.MixedFormatSqlExtendParser#propertyKey}.\n+\t * @param ctx the parse tree\n+\t */\n+\tvoid exitPropertyKey(MixedFormatSqlExtendParser.PropertyKeyContext ctx);\n+\t/**\n+\t * Enter a parse tree produced by {@link org.apache.amoro.spark.sql.parser.MixedFormatSqlExtendParser#propertyValue}.\n+\t * @param ctx the parse tree\n+\t */\n+\tvoid enterPropertyValue(MixedFormatSqlExtendParser.PropertyValueContext ctx);\n+\t/**\n+\t * Exit a parse tree produced by {@link org.apache.amoro.spark.sql.parser.MixedFormatSqlExtendParser#propertyValue}.\n+\t * @param ctx the parse tree\n+\t */\n+\tvoid exitPropertyValue(MixedFormatSqlExtendParser.PropertyValueContext ctx);\n+\t/**\n+\t * Enter a parse tree produced by {@link org.apache.amoro.spark.sql.parser.MixedFormatSqlExtendParser#constantList}.\n+\t * @param ctx the parse tree\n+\t */\n+\tvoid enterConstantList(MixedFormatSqlExtendParser.ConstantListContext ctx);\n+\t/**\n+\t * Exit a parse tree produced by {@link org.apache.amoro.spark.sql.parser.MixedFormatSqlExtendParser#constantList}.\n+\t * @param ctx the parse tree\n+\t */\n+\tvoid exitConstantList(MixedFormatSqlExtendParser.ConstantListContext ctx);\n+\t/**\n+\t * Enter a parse tree produced by {@link org.apache.amoro.spark.sql.parser.MixedFormatSqlExtendParser#nestedConstantList}.\n+\t * @param ctx the parse tree\n+\t */\n+\tvoid enterNestedConstantList(MixedFormatSqlExtendParser.NestedConstantListContext ctx);\n+\t/**\n+\t * Exit a parse tree produced by {@link org.apache.amoro.spark.sql.parser.MixedFormatSqlExtendParser#nestedConstantList}.\n+\t * @param ctx the parse tree\n+\t */\n+\tvoid exitNestedConstantList(MixedFormatSqlExtendParser.NestedConstantListContext ctx);\n+\t/**\n+\t * Enter a parse tree produced by {@link org.apache.amoro.spark.sql.parser.MixedFormatSqlExtendParser#createFileFormat}.\n+\t * @param ctx the parse tree\n+\t */\n+\tvoid enterCreateFileFormat(MixedFormatSqlExtendParser.CreateFileFormatContext ctx);\n+\t/**\n+\t * Exit a parse tree produced by {@link org.apache.amoro.spark.sql.parser.MixedFormatSqlExtendParser#createFileFormat}.\n+\t * @param ctx the parse tree\n+\t */\n+\tvoid exitCreateFileFormat(MixedFormatSqlExtendParser.CreateFileFormatContext ctx);\n+\t/**\n+\t * Enter a parse tree produced by the {@code tableFileFormat}\n+\t * labeled alternative in {@link org.apache.amoro.spark.sql.parser.MixedFormatSqlExtendPars\n...<truncated>...",
    "section": "code",
    "embedding": "[-0.007658567,-0.025676988,-0.014320413,-0.011416421,0.0355708,0.0049326294,0.06527142,0.023305245,-0.017308667,-0.048330966,-0.033887412,0.0017507896,-0.054314777,0.017596979,-0.014153489,0.06631956,0.04344875,-0.014741969,0.039877206,-0.026470307,0.024314709,0.0098422,-0.01698761,0.043783274,0.022509426,0.039616328,-0.0064180857,0.010649981,-0.06487959,-0.025984583,0.04248746,8.4"
  },
  {
    "pr_id": "e2e361f7-5f72-4045-9534-c2cb38d536b9",
    "content": "@@ -0,0 +1,1164 @@\n+// Generated from org/apache/amoro/spark/sql/parser/MixedFormatSqlExtend.g4 by ANTLR 4.8\n+package org.apache.amoro.spark.sql.parser;\n+import org.antlr.v4.runtime.tree.ParseTreeVisitor;\n+\n+/**\n+ * This interface defines a complete generic visitor for a parse tree produced\n+ * by {@link org.apache.amoro.spark.sql.parser.MixedFormatSqlExtendParser}.\n+ *\n+ * @param <T> The return type of the visit operation. Use {@link Void} for\n+ * operations with no return type.\n+ */\n+public interface MixedFormatSqlExtendVisitor<T> extends ParseTreeVisitor<T> {\n+\t/**\n+\t * Visit a parse tree produced by {@link org.apache.amoro.spark.sql.parser.MixedFormatSqlExtendParser#extendStatement}.\n+\t * @param ctx the parse tree\n+\t * @return the visitor result\n+\t */\n+\tT visitExtendStatement(MixedFormatSqlExtendParser.ExtendStatementContext ctx);\n+\t/**\n+\t * Visit a parse tree produced by the {@code createTableWithPk}\n+\t * labeled alternative in {@link org.apache.amoro.spark.sql.parser.MixedFormatSqlExtendParser#statement}.\n+\t * @param ctx the parse tree\n+\t * @return the visitor result\n+\t */\n+\tT visitCreateTableWithPk(MixedFormatSqlExtendParser.CreateTableWithPkContext ctx);\n+\t/**\n+\t * Visit a parse tree produced by the {@code explain}\n+\t * labeled alternative in {@link org.apache.amoro.spark.sql.parser.MixedFormatSqlExtendParser#statement}.\n+\t * @param ctx the parse tree\n+\t * @return the visitor result\n+\t */\n+\tT visitExplain(MixedFormatSqlExtendParser.ExplainContext ctx);\n+\t/**\n+\t * Visit a parse tree produced by {@link org.apache.amoro.spark.sql.parser.MixedFormatSqlExtendParser#createTableHeader}.\n+\t * @param ctx the parse tree\n+\t * @return the visitor result\n+\t */\n+\tT visitCreateTableHeader(MixedFormatSqlExtendParser.CreateTableHeaderContext ctx);\n+\t/**\n+\t * Visit a parse tree produced by the {@code colListWithPk}\n+\t * labeled alternative in {@link org.apache.amoro.spark.sql.parser.MixedFormatSqlExtendParser#colListAndPk}.\n+\t * @param ctx the parse tree\n+\t * @return the visitor result\n+\t */\n+\tT visitColListWithPk(MixedFormatSqlExtendParser.ColListWithPkContext ctx);\n+\t/**\n+\t * Visit a parse tree produced by the {@code colListOnlyPk}\n+\t * labeled alternative in {@link org.apache.amoro.spark.sql.parser.MixedFormatSqlExtendParser#colListAndPk}.\n+\t * @param ctx the parse tree\n+\t * @return the visitor result\n+\t */\n+\tT visitColListOnlyPk(MixedFormatSqlExtendParser.ColListOnlyPkContext ctx);\n+\t/**\n+\t * Visit a parse tree produced by {@link org.apache.amoro.spark.sql.parser.MixedFormatSqlExtendParser#primarySpec}.\n+\t * @param ctx the parse tree\n+\t * @return the visitor result\n+\t */\n+\tT visitPrimarySpec(MixedFormatSqlExtendParser.PrimarySpecContext ctx);\n+\t/**\n+\t * Visit a parse tree produced by {@link org.apache.amoro.spark.sql.parser.MixedFormatSqlExtendParser#bucketSpec}.\n+\t * @param ctx the parse tree\n+\t * @return the visitor result\n+\t */\n+\tT visitBucketSpec(MixedFormatSqlExtendParser.BucketSpecContext ctx);\n+\t/**\n+\t * Visit a parse tree produced by {@link org.apache.amoro.spark.sql.parser.MixedFormatSqlExtendParser#skewSpec}.\n+\t * @param ctx the parse tree\n+\t * @return the visitor result\n+\t */\n+\tT visitSkewSpec(MixedFormatSqlExtendParser.SkewSpecContext ctx);\n+\t/**\n+\t * Visit a parse tree produced by {@link org.apache.amoro.spark.sql.parser.MixedFormatSqlExtendParser#locationSpec}.\n+\t * @param ctx the parse tree\n+\t * @return the visitor result\n+\t */\n+\tT visitLocationSpec(MixedFormatSqlExtendParser.LocationSpecContext ctx);\n+\t/**\n+\t * Visit a parse tree produced by {@link org.apache.amoro.spark.sql.parser.MixedFormatSqlExtendParser#commentSpec}.\n+\t * @param ctx the parse tree\n+\t * @return the visitor result\n+\t */\n+\tT visitCommentSpec(MixedFormatSqlExtendParser.CommentSpecContext ctx);\n+\t/**\n+\t * Visit a parse tree produced by {@link org.apache.amoro.spark.sql.parser.MixedFormatSqlExtendParser#query}.\n+\t * @param ctx the parse tree\n+\t * @return the visitor result\n+\t */\n+\tT visitQuery(MixedFormatSqlExtendParser.QueryContext ctx);\n+\t/**\n+\t * Visit a parse tree produced by {@link org.apache.amoro.spark.sql.parser.MixedFormatSqlExtendParser#ctes}.\n+\t * @param ctx the parse tree\n+\t * @return the visitor result\n+\t */\n+\tT visitCtes(MixedFormatSqlExtendParser.CtesContext ctx);\n+\t/**\n+\t * Visit a parse tree produced by {@link org.apache.amoro.spark.sql.parser.MixedFormatSqlExtendParser#namedQuery}.\n+\t * @param ctx the parse tree\n+\t * @return the visitor result\n+\t */\n+\tT visitNamedQuery(MixedFormatSqlExtendParser.NamedQueryContext ctx);\n+\t/**\n+\t * Visit a parse tree produced by {@link org.apache.amoro.spark.sql.parser.MixedFormatSqlExtendParser#tableProvider}.\n+\t * @param ctx the parse tree\n+\t * @return the visitor result\n+\t */\n+\tT visitTableProvider(MixedFormatSqlExtendParser.TableProviderContext ctx);\n+\t/**\n+\t * Visit a parse tree produced by {@link org.apache.amoro.spark.sql.parser.MixedFormatSqlExtendParser#createTableClauses}.\n+\t * @param ctx the parse tree\n+\t * @return the visitor result\n+\t */\n+\tT visitCreateTableClauses(MixedFormatSqlExtendParser.CreateTableClausesContext ctx);\n+\t/**\n+\t * Visit a parse tree produced by {@link org.apache.amoro.spark.sql.parser.MixedFormatSqlExtendParser#propertyList}.\n+\t * @param ctx the parse tree\n+\t * @return the visitor result\n+\t */\n+\tT visitPropertyList(MixedFormatSqlExtendParser.PropertyListContext ctx);\n+\t/**\n+\t * Visit a parse tree produced by {@link org.apache.amoro.spark.sql.parser.MixedFormatSqlExtendParser#property}.\n+\t * @param ctx the parse tree\n+\t * @return the visitor result\n+\t */\n+\tT visitProperty(MixedFormatSqlExtendParser.PropertyContext ctx);\n+\t/**\n+\t * Visit a parse tree produced by {@link org.apache.amoro.spark.sql.parser.MixedFormatSqlExtendParser#propertyKey}.\n+\t * @param ctx the parse tree\n+\t * @return the visitor result\n+\t */\n+\tT visitPropertyKey(MixedFormatSqlExtendParser.PropertyKeyContext ctx);\n+\t/**\n+\t * Visit a parse tree produced by {@link org.apache.amoro.spark.sql.parser.MixedFormatSqlExtendParser#propertyValue}.\n+\t * @param ctx the parse tree\n+\t * @return the visitor result\n+\t */\n+\tT visitPropertyValue(MixedFormatSqlExtendParser.PropertyValueContext ctx);\n+\t/**\n+\t * Visit a parse tree produced by {@link org.apache.amoro.spark.sql.parser.MixedFormatSqlExtendParser#constantList}.\n+\t * @param ctx the parse tree\n+\t * @return the visitor result\n+\t */\n+\tT visitConstantList(MixedFormatSqlExtendParser.ConstantListContext ctx);\n+\t/**\n+\t * Visit a parse tree produced by {@link org.apache.amoro.spark.sql.parser.MixedFormatSqlExtendParser#nestedConstantList}.\n+\t * @param ctx the parse tree\n+\t * @return the visitor result\n+\t */\n+\tT visitNestedConstantList(MixedFormatSqlExtendParser.NestedConstantListContext ctx);\n+\t/**\n+\t * Visit a parse tree produced by {@link org.apache.amoro.spark.sql.parser.MixedFormatSqlExtendParser#createFileFormat}.\n+\t * @param ctx the parse tree\n+\t * @return the visitor result\n+\t */\n+\tT visitCreateFileFormat(MixedFormatSqlExtendParser.CreateFileFormatContext ctx);\n+\t/**\n+\t * Visit a parse tree produced by the {@code tableFileFormat}\n+\t * labeled alternative in {@link org.apache.amoro.spark.sql.parser.MixedFormatSqlExtendParser#fileFormat}.\n+\t * @param ctx the parse tree\n+\t * @return the visitor result\n+\t */\n+\tT visitTableFileFormat(MixedFormatSqlExtendParser.TableFileFormatContext ctx);\n+\t/**\n+\t * Visit a parse tree produced by the {@code genericFileFormat}\n+\t * labeled alternative in {@link org.apache.amoro.spark.sql.parser.MixedFormatSqlExtendParser#fileFormat}.\n+\t * @param ctx the parse tree\n+\t * @return the visitor result\n+\t */\n+\tT visitGenericFileFormat(MixedFormatSqlExtendParser.GenericFileFormatContext ctx);\n+\t/**\n+\t * Visit a parse tree produced by {@link org.apache.amoro.spark.sql.parser.MixedFormatSqlExtendParser#storageHandler}.\n+\t * @param ctx the parse tree\n+\t * @return the visitor result\n+\t */\n+\tT visitStorageHandler(MixedFormatSqlExtendParser.StorageHandlerContext ctx);\n+\t/**\n+\t * Visit a parse tree produced by {@link org.apache.amoro.spark.sql.parser.MixedFormatSqlExtendParser#queryOrganization}.\n+\t * @param ctx the parse tree\n+\t * @return the visitor result\n+\t */\n+\tT visitQueryOrganization(MixedFormatSqlExtendParser.QueryOrganizationContext ctx);\n+\t/**\n+\t * Visit a parse tree produced by the {@code queryTermDefault}\n+\t * labeled alternative in {@link org.apache.amoro.spark.sql.parser.MixedFormatSqlExtendParser#queryTerm}.\n+\t * @param ctx the parse tree\n+\t * @return the visitor result\n+\t */\n+\tT visitQueryTermDefault(MixedFormatSqlExtendParser.QueryTermDefaultContext ctx);\n+\t/**\n+\t * Visit a parse tree produced by the {@code setOperation}\n+\t * labeled alternative in {@link org.apache.amoro.spark.sql.parser.MixedFormatSqlExtendParser#queryTerm}.\n+\t * @param ctx the parse tree\n+\t * @return the visitor result\n+\t */\n+\tT visitSetOperation(MixedFormatSqlExtendParser.SetOperationContext ctx);\n+\t/**\n+\t * Visit a parse tree produced by the {@code queryPrimaryDefault}\n+\t * labeled alternative in {@link org.apache.amoro.spark.sql.parser.MixedFormatSqlExtendParser#queryPrimary}.\n+\t * @param ctx the parse tree\n+\t * @return the visitor result\n+\t */\n+\tT visitQueryPrimaryDefault(MixedFormatSqlExtendParser.QueryPrimaryDefaultContext ctx);\n+\t/**\n+\t * Visit a parse tree produced by the {@code fromStmt}\n+\t * labeled alternative in {@link org.apache.amoro.spark.sql.parser.MixedFormatSqlExtendParser#queryPrimary}.\n+\t * @param ctx the parse tree\n+\t * @return the visitor result\n+\t */\n+\tT visitFromStmt(MixedFormatSqlExtendParser.FromStmtContext ctx);\n+\t/**\n+\t * Visit a parse tree produced by the {@code table}\n+\t * labeled alternative in {@link org.apache.amoro.spark.sql.parser.MixedFormatSqlExtendParser#queryPrimary}.\n+\t * @param ctx the parse tree\n+\t * @return the visitor result\n+\t */\n+\tT visitTable(MixedFormatSqlExtendParser.TableContext ctx);\n+\t/**\n+\t * Visit a parse tree produced by the {@code inlineTableDefault1}\n+\t * labeled alternative in {@link org.apache.amoro.spark.sql.parser.MixedFormatSqlExtendParser#queryPrimary}.\n+\t * @param ctx the parse tree\n+\t * @return the visitor result\n+\t */\n+\tT visitInlineTableDefault1(MixedFormatSqlExtendParser.InlineTableDefault1Context ctx);\n+\t/**\n+\t * Visit a parse tree produced by the {@code subquery}\n+\t * labeled alternative in {@link org.apache.amoro.spark.sql.parser.MixedFormatSqlExtendParser#queryPrimary}.\n+\t * @param ctx the parse tree\n+\t * @return the visitor result\n+\t */\n+\tT visitSubquery(MixedFormatSqlExtendParser.SubqueryContext ctx);\n+\t/**\n+\t * Visit a parse tree produced by {@link org.apache.amoro.spark.sql.parser.MixedFormatSqlExtendParser#sortItem}.\n+\t * @param ctx the parse tree\n+\t * @return the visitor result\n+\t */\n+\tT visitSortItem(MixedFormatSqlExtendParser.SortItemContext ctx);\n+\t/**\n+\t * Visit a parse tree produced by {@link org.apache.amoro.spark.sql.parser.MixedFormatSqlExtendParser#fromStatement}.\n+\t * @param ctx the parse tree\n+\t * @return the visitor result\n+\t */\n+\tT visitFromStatement(MixedFormatSqlExtendParser.FromStatementContext ctx);\n+\t/**\n+\t * Visit a parse tree produced by {@link org.apache.amoro.spark.sql.parser.MixedFormatSqlExtendParser#fromStatementBody}.\n+\t * @param ctx the parse tree\n+\t * @return the visitor result\n+\t */\n+\tT visitFromStatementBody(MixedFormatSqlExtendParser.FromStatementBodyContext ctx);\n+\t/**\n+\t * Visit a parse tree produced by the {@code transformQuerySpecification}\n+\t * labeled alternative in {@link org.apache.amoro.spark.sql.parser.MixedFormatSqlExtendParser#querySpecification}.\n+\t * @param ctx the parse tree\n+\t * @return the visitor result\n+\t */\n+\tT visitTransformQuerySpecification(MixedFormatSqlExtendParser.TransformQuerySpecificationContext ctx);\n+\t/**\n+\t * Visit a parse tree produced by the {@code regularQuerySpecification}\n+\t * labeled alternative in {@link org.apache.amoro.spark.sql.parser.MixedFormatSqlExtendParser#querySpecification}.\n+\t * @param ctx the parse tree\n+\t * @return the visitor result\n+\t */\n+\tT visitRegularQuerySpecification(MixedFormatSqlExtendParser.RegularQuerySpecificationContext ctx);\n+\t/**\n+\t * \n...<truncated>...",
    "section": "code",
    "embedding": "[-0.007798593,-0.024123257,0.0035614837,-0.023977622,0.026159666,0.0013082314,0.049507562,0.028306676,-0.021965856,-0.03316546,-0.051118836,-0.019827582,-0.046131596,0.017082095,0.0060835173,0.05486317,0.03229663,-0.025132872,0.037139583,-0.044769842,0.025001913,0.012594934,-0.01830174,0.026170516,0.009420426,0.017981064,0.019904813,-0.0019246619,-0.053368296,-0.04282634,0.07036567"
  },
  {
    "pr_id": "e2e361f7-5f72-4045-9534-c2cb38d536b9",
    "content": "@@ -0,0 +1,282 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.amoro.spark;\n+\n+import org.apache.amoro.shade.guava32.com.google.common.base.Preconditions;\n+import org.apache.amoro.shade.guava32.com.google.common.collect.Maps;\n+import org.apache.amoro.spark.mixed.MixedSparkCatalogBase;\n+import org.apache.amoro.spark.mixed.MixedTableStoreType;\n+import org.apache.amoro.spark.table.MixedSparkTable;\n+import org.apache.amoro.spark.table.SparkChangeTable;\n+import org.apache.amoro.table.BasicUnkeyedTable;\n+import org.apache.amoro.table.KeyedTable;\n+import org.apache.amoro.table.MixedTable;\n+import org.apache.amoro.table.PrimaryKeySpec;\n+import org.apache.amoro.table.TableBuilder;\n+import org.apache.amoro.table.TableIdentifier;\n+import org.apache.amoro.table.UnkeyedTable;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.Transaction;\n+import org.apache.iceberg.exceptions.AlreadyExistsException;\n+import org.apache.iceberg.spark.Spark3Util;\n+import org.apache.iceberg.spark.SparkSchemaUtil;\n+import org.apache.iceberg.types.Types;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchTableException;\n+import org.apache.spark.sql.catalyst.analysis.TableAlreadyExistsException;\n+import org.apache.spark.sql.connector.catalog.Identifier;\n+import org.apache.spark.sql.connector.catalog.Table;\n+import org.apache.spark.sql.connector.catalog.TableCatalog;\n+import org.apache.spark.sql.connector.catalog.TableChange;\n+import org.apache.spark.sql.connector.catalog.TableChange.ColumnChange;\n+import org.apache.spark.sql.connector.catalog.TableChange.RemoveProperty;\n+import org.apache.spark.sql.connector.catalog.TableChange.SetProperty;\n+import org.apache.spark.sql.connector.expressions.Transform;\n+import org.apache.spark.sql.types.StructType;\n+import scala.Option;\n+\n+import java.util.ArrayList;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+\n+public class MixedFormatSparkCatalog extends MixedSparkCatalogBase implements SupportsFunctions {\n+\n+  @Override\n+  public Table loadTable(Identifier ident) throws NoSuchTableException {\n+    checkAndRefreshCatalogMeta();\n+    TableIdentifier identifier;\n+    MixedTable table;\n+    try {\n+      if (isInnerTableIdentifier(ident)) {\n+        MixedTableStoreType type = MixedTableStoreType.from(ident.name());\n+        identifier = buildInnerTableIdentifier(ident);\n+        table = catalog.loadTable(identifier);\n+        return loadInnerTable(table, type);\n+      } else {\n+        identifier = buildIdentifier(ident);\n+        table = catalog.loadTable(identifier);\n+      }\n+    } catch (org.apache.iceberg.exceptions.NoSuchTableException e) {\n+      throw new NoSuchTableException(ident);\n+    }\n+    return MixedSparkTable.ofMixedTable(table, catalog, name());\n+  }\n+\n+  private Table loadInnerTable(MixedTable table, MixedTableStoreType type) {\n+    if (type != null) {\n+      switch (type) {\n+        case CHANGE:\n+          return new SparkChangeTable(\n+              (BasicUnkeyedTable) table.asKeyedTable().changeTable(), false);\n+        default:\n+          throw new IllegalArgumentException(\"Unknown inner table type: \" + type);\n+      }\n+    } else {\n+      throw new IllegalArgumentException(\"Table does not exist: \" + table);\n+    }\n+  }\n+\n+  @Override\n+  public Table createTable(\n+      Identifier ident, StructType schema, Transform[] transforms, Map<String, String> properties)\n+      throws TableAlreadyExistsException {\n+    checkAndRefreshCatalogMeta();\n+    properties = Maps.newHashMap(properties);\n+    Schema finalSchema = checkAndConvertSchema(schema, properties);\n+    TableIdentifier identifier = buildIdentifier(ident);\n+    TableBuilder builder = catalog.newTableBuilder(identifier, finalSchema);\n+    PartitionSpec spec = Spark3Util.toPartitionSpec(finalSchema, transforms);\n+    if (properties.containsKey(TableCatalog.PROP_LOCATION)\n+        && isIdentifierLocation(properties.get(TableCatalog.PROP_LOCATION), ident)) {\n+      properties.remove(TableCatalog.PROP_LOCATION);\n+    }\n+    try {\n+      if (properties.containsKey(\"primary.keys\")) {\n+        PrimaryKeySpec primaryKeySpec =\n+            PrimaryKeySpec.fromDescription(finalSchema, properties.get(\"primary.keys\"));\n+        properties.remove(\"primary.keys\");\n+        builder\n+            .withPartitionSpec(spec)\n+            .withProperties(properties)\n+            .withPrimaryKeySpec(primaryKeySpec);\n+      } else {\n+        builder.withPartitionSpec(spec).withProperties(properties);\n+      }\n+      MixedTable table = builder.create();\n+      return MixedSparkTable.ofMixedTable(table, catalog, name());\n+    } catch (AlreadyExistsException e) {\n+      throw new TableAlreadyExistsException(\"Table \" + ident + \" already exists\", Option.apply(e));\n+    }\n+  }\n+\n+  private Schema checkAndConvertSchema(StructType schema, Map<String, String> properties) {\n+    Schema convertSchema;\n+    convertSchema = SparkSchemaUtil.convert(schema);\n+\n+    // schema add primary keys\n+    if (properties.containsKey(\"primary.keys\")) {\n+      PrimaryKeySpec primaryKeySpec =\n+          PrimaryKeySpec.fromDescription(convertSchema, properties.get(\"primary.keys\"));\n+      List<String> primaryKeys = primaryKeySpec.fieldNames();\n+      Set<String> pkSet = new HashSet<>(primaryKeys);\n+      Set<Integer> identifierFieldIds = new HashSet<>();\n+      List<Types.NestedField> columnsWithPk = new ArrayList<>();\n+      convertSchema\n+          .columns()\n+          .forEach(\n+              nestedField -> {\n+                if (pkSet.contains(nestedField.name())) {\n+                  columnsWithPk.add(nestedField.asRequired());\n+                  identifierFieldIds.add(nestedField.fieldId());\n+                } else {\n+                  columnsWithPk.add(nestedField);\n+                }\n+              });\n+      return new Schema(columnsWithPk, identifierFieldIds);\n+    }\n+    return convertSchema;\n+  }\n+\n+  @Override\n+  public Table alterTable(Identifier ident, TableChange... changes) throws NoSuchTableException {\n+    TableIdentifier identifier = buildIdentifier(ident);\n+    MixedTable table;\n+    try {\n+      table = catalog.loadTable(identifier);\n+    } catch (org.apache.iceberg.exceptions.NoSuchTableException e) {\n+      throw new NoSuchTableException(ident);\n+    }\n+    if (table.isUnkeyedTable()) {\n+      alterUnKeyedTable(table.asUnkeyedTable(), changes);\n+      return MixedSparkTable.ofMixedTable(table, catalog, name());\n+    } else if (table.isKeyedTable()) {\n+      alterKeyedTable(table.asKeyedTable(), changes);\n+      return MixedSparkTable.ofMixedTable(table, catalog, name());\n+    }\n+    throw new UnsupportedOperationException(\"Unsupported alter table\");\n+  }\n+\n+  private void alterKeyedTable(KeyedTable table, TableChange... changes) {\n+    List<TableChange> schemaChanges = new ArrayList<>();\n+    List<TableChange> propertyChanges = new ArrayList<>();\n+    for (TableChange change : changes) {\n+      if (change instanceof ColumnChange) {\n+        schemaChanges.add(change);\n+      } else if (change instanceof SetProperty) {\n+        propertyChanges.add(change);\n+      } else if (change instanceof RemoveProperty) {\n+        propertyChanges.add(change);\n+      } else {\n+        throw new UnsupportedOperationException(\"Cannot apply unknown table change: \" + change);\n+      }\n+    }\n+    commitKeyedChanges(table, schemaChanges, propertyChanges);\n+  }\n+\n+  private void commitKeyedChanges(\n+      KeyedTable table, List<TableChange> schemaChanges, List<TableChange> propertyChanges) {\n+    if (!schemaChanges.isEmpty()) {\n+      Spark3Util.applySchemaChanges(table.updateSchema(), schemaChanges).commit();\n+    }\n+\n+    if (!propertyChanges.isEmpty()) {\n+      Spark3Util.applyPropertyChanges(table.updateProperties(), propertyChanges).commit();\n+    }\n+  }\n+\n+  private void alterUnKeyedTable(UnkeyedTable table, TableChange... changes) {\n+    SetProperty setLocation = null;\n+    SetProperty setSnapshotId = null;\n+    SetProperty pickSnapshotId = null;\n+    List<TableChange> propertyChanges = new ArrayList<>();\n+    List<TableChange> schemaChanges = new ArrayList<>();\n+\n+    for (TableChange change : changes) {\n+      if (change instanceof SetProperty) {\n+        SetProperty set = (SetProperty) change;\n+        if (TableCatalog.PROP_LOCATION.equalsIgnoreCase(set.property())) {\n+          setLocation = set;\n+        } else if (\"current-snapshot-id\".equalsIgnoreCase(set.property())) {\n+          setSnapshotId = set;\n+        } else if (\"cherry-pick-snapshot-id\".equalsIgnoreCase(set.property())) {\n+          pickSnapshotId = set;\n+        } else if (\"sort-order\".equalsIgnoreCase(set.property())) {\n+          throw new UnsupportedOperationException(\n+              \"Cannot specify the 'sort-order' because it's a reserved table \"\n+                  + \"property. Please use the command 'ALTER TABLE ... WRITE ORDERED BY' to specify write sort-orders.\");\n+        } else {\n+          propertyChanges.add(set);\n+        }\n+      } else if (change instanceof RemoveProperty) {\n+        propertyChanges.add(change);\n+      } else if (change instanceof ColumnChange) {\n+        schemaChanges.add(change);\n+      } else {\n+        throw new UnsupportedOperationException(\"Cannot apply unknown table change: \" + change);\n+      }\n+    }\n+\n+    commitUnKeyedChanges(\n+        table, setLocation, setSnapshotId, pickSnapshotId, propertyChanges, schemaChanges);\n+  }\n+\n+  protected void commitUnKeyedChanges(\n+      UnkeyedTable table,\n+      SetProperty setLocation,\n+      SetProperty setSnapshotId,\n+      SetProperty pickSnapshotId,\n+      List<TableChange> propertyChanges,\n+      List<TableChange> schemaChanges) {\n+    // don't allow setting the snapshot and picking a commit at the same time because order is\n+    // ambiguous and choosing\n+    // one order leads to different results\n+    Preconditions.checkArgument(\n+        setSnapshotId == null || pickSnapshotId == null,\n+        \"Cannot set the current snapshot ID and cherry-pick snapshot changes at the same time\");\n+\n+    if (setSnapshotId != null) {\n+      long newSnapshotId = Long.parseLong(setSnapshotId.value());\n+      table.manageSnapshots().setCurrentSnapshot(newSnapshotId).commit();\n+    }\n+\n+    // if updating the table snapshot, perform that update first in case it fails\n+    if (pickSnapshotId != null) {\n+      long newSnapshotId = Long.parseLong(pickSnapshotId.value());\n+      table.manageSnapshots().cherrypick(newSnapshotId).commit();\n+    }\n+\n+    Transaction transaction = table.newTransaction();\n+\n+    if (setLocation != null) {\n+      transaction.updateLocation().setLocation(setLocation.value()).commit();\n+    }\n+\n+    if (!propertyChanges.isEmpty()) {\n+      Spark3Util.applyPropertyChanges(transaction.updateProperties(), propertyChanges).commit();\n+    }\n+\n+    if (!schemaChanges.isEmpty()) {\n+      Spark3Util.applySchemaChanges(transaction.updateSchema(), schemaChanges).commit();\n+    }\n+\n+    transaction.commitTransaction();\n+  }\n+}",
    "section": "code",
    "embedding": "[-0.0124380225,-0.0007106332,-0.020061303,0.048038602,0.047524184,-0.015064393,0.046783112,0.035310857,0.0017104544,-0.023604667,-0.025058277,-0.040074844,-0.09311379,0.06939502,-0.0123375915,0.05892622,0.030320013,0.010338519,0.025352178,-0.024297338,0.004547851,-0.0026883513,-0.002626939,0.021732297,-0.06093602,0.015080465,0.04182975,0.033363827,-0.063949734,-0.002179759,0.027918"
  },
  {
    "pr_id": "e2e361f7-5f72-4045-9534-c2cb38d536b9",
    "content": "@@ -0,0 +1,58 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.amoro.spark;\n+\n+import org.apache.amoro.spark.mixed.MixedSessionCatalogBase;\n+import org.apache.amoro.spark.mixed.MixedSparkCatalogBase;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchFunctionException;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchNamespaceException;\n+import org.apache.spark.sql.connector.catalog.FunctionCatalog;\n+import org.apache.spark.sql.connector.catalog.Identifier;\n+import org.apache.spark.sql.connector.catalog.SupportsNamespaces;\n+import org.apache.spark.sql.connector.catalog.TableCatalog;\n+import org.apache.spark.sql.connector.catalog.functions.UnboundFunction;\n+import org.apache.spark.sql.util.CaseInsensitiveStringMap;\n+\n+/**\n+ * A Spark catalog that can also load non-Iceberg tables.\n+ *\n+ * @param <T> CatalogPlugin class to avoid casting to TableCatalog and SupportsNamespaces.\n+ */\n+public class MixedFormatSparkSessionCatalog<\n+        T extends TableCatalog & SupportsNamespaces & FunctionCatalog>\n+    extends MixedSessionCatalogBase<T> {\n+\n+  @Override\n+  protected MixedSparkCatalogBase buildTargetCatalog(\n+      String name, CaseInsensitiveStringMap options) {\n+    MixedFormatSparkCatalog newCatalog = new MixedFormatSparkCatalog();\n+    newCatalog.initialize(name, options);\n+    return newCatalog;\n+  }\n+\n+  @Override\n+  public Identifier[] listFunctions(String[] namespace) throws NoSuchNamespaceException {\n+    return getSessionCatalog().listFunctions(namespace);\n+  }\n+\n+  @Override\n+  public UnboundFunction loadFunction(Identifier ident) throws NoSuchFunctionException {\n+    return getSessionCatalog().loadFunction(ident);\n+  }\n+}",
    "section": "code",
    "embedding": "[-0.0067490293,-0.00081485684,-0.039559644,0.018598,0.06468464,-0.035744406,0.040032934,0.0050554764,0.0085558165,-0.035301644,-0.017737472,-0.02016548,-0.0712929,0.040186077,-0.0027622958,0.065996125,0.027109241,0.016178649,0.049906265,-0.01915229,-0.019866662,-0.010978633,-0.00520078,0.028880285,-0.04771478,0.036594935,0.012209868,0.0144481715,-0.081056476,0.008563982,0.03863998,"
  },
  {
    "pr_id": "e2e361f7-5f72-4045-9534-c2cb38d536b9",
    "content": "@@ -0,0 +1,30 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.amoro.spark;\n+\n+import org.apache.amoro.spark.util.ExpressionHelper;\n+\n+public class Spark34Adapter implements SparkAdapter {\n+  ExpressionHelper expressionHelper = new ExpressionHelper();\n+\n+  @Override\n+  public ExpressionHelper expressions() {\n+    return expressionHelper;\n+  }\n+}",
    "section": "code",
    "embedding": "[0.0147580225,-0.010035204,-0.0020049757,0.021012234,0.02515156,-0.039786495,0.056182243,0.008225156,0.022576384,-0.01216823,-0.02281748,-0.014673111,-0.06939492,0.062734984,-0.030723607,0.053875696,0.015885675,0.016668618,0.02685138,-0.030108135,-0.011041948,0.0012933874,0.0028966188,0.024264734,-0.04076244,0.017036967,0.026501002,0.03219918,-0.05667964,-0.0005389157,0.024011,0.01"
  },
  {
    "pr_id": "b2e982f0-8b05-43bf-a510-04e8a37c4771",
    "content": "@@ -9780,8 +9777,6 @@ const docTemplate = `{\n         \"schema.QuestionAdd\": {\n             \"type\": \"object\",\n             \"required\": [\n-                \"content\",\n-                \"tags\",\n                 \"title\"\n             ],\n             \"properties\": {",
    "section": "code",
    "embedding": "[0.010147658,0.016157063,0.0047737677,0.044330742,0.044626962,-0.034586478,0.039422013,0.012021869,-0.024925953,-0.045020487,-0.048052773,0.012190997,-0.05681716,-0.009489938,-0.0020332986,0.068172745,0.057725802,0.032698333,-0.0063386424,-0.055296578,-0.008748551,-0.018577116,0.006224175,-0.0022557813,0.031296533,-0.0036120305,0.012824075,0.017836018,-0.08264432,0.026854474,-0.039"
  },
  {
    "pr_id": "e2e361f7-5f72-4045-9534-c2cb38d536b9",
    "content": "@@ -0,0 +1,38 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.amoro.spark;\n+\n+import org.apache.amoro.spark.util.ExpressionHelper;\n+\n+/**\n+ * This interface will provider some util or helper object to shield api differences in different\n+ * spark versions.\n+ */\n+public interface SparkAdapter {\n+\n+  /**\n+   * A helper object to help build spark expressions {@link\n+   * org.apache.spark.sql.connector.expressions.Expression}, and provider a covert method to help\n+   * covert {@link org.apache.spark.sql.connector.expressions.Expression} to {@link\n+   * org.apache.spark.sql.catalyst.expressions.Expression}\n+   *\n+   * @return expression helper object\n+   */\n+  ExpressionHelper expressions();\n+}",
    "section": "code",
    "embedding": "[0.008375534,-0.0037060734,-0.021934718,0.005643728,0.035477694,-0.046840597,0.06048564,0.0024820403,0.034267504,-0.022287311,-0.018861717,-0.011248313,-0.06884335,0.04769462,-0.015085871,0.05195271,0.020495163,0.027109534,0.031497512,-0.02080469,-0.022571076,0.0003326494,-0.007325747,0.020311281,-0.021012787,0.032395836,0.033012748,0.013775121,-0.06927047,0.0058981534,0.04420926,0"
  },
  {
    "pr_id": "e2e361f7-5f72-4045-9534-c2cb38d536b9",
    "content": "@@ -0,0 +1,36 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.amoro.spark;\n+\n+/**\n+ * A util class to load spark adapter via spark version. This util class will move to spark3-common\n+ * module if multi spark modules added.\n+ */\n+public class SparkAdapterLoader {\n+  private static final SparkAdapter adapter = new Spark34Adapter();\n+\n+  /**\n+   * This method will implement as SPI if multi spark modules added.\n+   *\n+   * @return A SparkAdapter objects\n+   */\n+  public static SparkAdapter getOrLoad() {\n+    return adapter;\n+  }\n+}",
    "section": "code",
    "embedding": "[-0.008611787,-0.029631631,-0.003057841,0.030002775,0.0079025775,-0.043466933,0.053040054,0.003193048,0.005881491,-0.020317307,-0.0037557334,-0.013979284,-0.067556374,0.06106121,-0.03398744,0.0410588,0.012762718,0.014623181,0.015773257,-0.016549772,-0.007149045,0.010815089,0.0152539695,0.032763883,-0.056562558,0.012377657,0.029850468,0.036246516,-0.062848695,-0.007794236,0.03501028"
  },
  {
    "pr_id": "e2e361f7-5f72-4045-9534-c2cb38d536b9",
    "content": "@@ -0,0 +1,91 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.amoro.spark;\n+\n+import org.apache.iceberg.StructLike;\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.apache.spark.sql.types.BinaryType;\n+import org.apache.spark.sql.types.DataType;\n+import org.apache.spark.sql.types.DecimalType;\n+import org.apache.spark.sql.types.StringType;\n+import org.apache.spark.sql.types.StructField;\n+import org.apache.spark.sql.types.StructType;\n+\n+import java.nio.ByteBuffer;\n+import java.util.function.BiFunction;\n+import java.util.stream.Stream;\n+\n+/**\n+ * this class copied from iceberg org.apache.iceberg.spark.source.InternalRowWrapper for\n+ * InternalRowWrapper is not public class\n+ */\n+public class SparkInternalRowWrapper implements StructLike {\n+  private final DataType[] types;\n+  private final BiFunction<InternalRow, Integer, ?>[] getters;\n+  private InternalRow row = null;\n+\n+  public SparkInternalRowWrapper(StructType rowType) {\n+    this.types = Stream.of(rowType.fields()).map(StructField::dataType).toArray(DataType[]::new);\n+    this.getters = Stream.of(types).map(SparkInternalRowWrapper::getter).toArray(BiFunction[]::new);\n+  }\n+\n+  @Override\n+  public int size() {\n+    return types.length;\n+  }\n+\n+  @Override\n+  public <T> T get(int pos, Class<T> javaClass) {\n+    if (row.isNullAt(pos)) {\n+      return null;\n+    } else if (getters[pos] != null) {\n+      return javaClass.cast(getters[pos].apply(row, pos));\n+    }\n+\n+    return javaClass.cast(row.get(pos, types[pos]));\n+  }\n+\n+  @Override\n+  public <T> void set(int pos, T value) {\n+    row.update(pos, value);\n+  }\n+\n+  public SparkInternalRowWrapper wrap(InternalRow internalRow) {\n+    this.row = internalRow;\n+    return this;\n+  }\n+\n+  private static BiFunction<InternalRow, Integer, ?> getter(DataType type) {\n+    if (type instanceof StringType) {\n+      return (row, pos) -> row.getUTF8String(pos).toString();\n+    } else if (type instanceof DecimalType) {\n+      DecimalType decimal = (DecimalType) type;\n+      return (row, pos) ->\n+          row.getDecimal(pos, decimal.precision(), decimal.scale()).toJavaBigDecimal();\n+    } else if (type instanceof BinaryType) {\n+      return (row, pos) -> ByteBuffer.wrap(row.getBinary(pos));\n+    } else if (type instanceof StructType) {\n+      StructType structType = (StructType) type;\n+      SparkInternalRowWrapper nestedWrapper = new SparkInternalRowWrapper(structType);\n+      return (row, pos) -> nestedWrapper.wrap(row.getStruct(pos, structType.size()));\n+    }\n+\n+    return null;\n+  }\n+}",
    "section": "code",
    "embedding": "[-0.01256368,-0.015670879,-0.027726188,0.031254638,0.046492886,-0.02476929,0.06901635,0.001701574,0.018778091,-0.03857193,-0.005376785,-0.051523436,-0.07554704,0.055041973,-0.014480133,0.061038915,0.024970466,-0.0006360412,0.03635979,-0.0113964435,-0.0244111,0.0014161951,-0.0024418181,0.01871666,-0.0371842,0.028373579,0.01070311,0.02598381,-0.0713348,0.017629923,0.046835735,0.01696"
  },
  {
    "pr_id": "e2e361f7-5f72-4045-9534-c2cb38d536b9",
    "content": "@@ -0,0 +1,117 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.amoro.spark;\n+\n+import org.apache.amoro.TableFormat;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchFunctionException;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchNamespaceException;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchTableException;\n+import org.apache.spark.sql.connector.catalog.FunctionCatalog;\n+import org.apache.spark.sql.connector.catalog.Identifier;\n+import org.apache.spark.sql.connector.catalog.SupportsNamespaces;\n+import org.apache.spark.sql.connector.catalog.Table;\n+import org.apache.spark.sql.connector.catalog.TableCatalog;\n+import org.apache.spark.sql.connector.catalog.functions.UnboundFunction;\n+import org.apache.spark.sql.connector.iceberg.catalog.ProcedureCatalog;\n+\n+public class SparkUnifiedCatalog extends SparkUnifiedCatalogBase\n+    implements TableCatalog, SupportsNamespaces, ProcedureCatalog, FunctionCatalog {\n+\n+  /**\n+   * List the functions in a namespace from the catalog.\n+   *\n+   * <p>If there are no functions in the namespace, implementations should return an empty array.\n+   *\n+   * @param namespace a multi-part namespace\n+   * @return an array of Identifiers for functions\n+   * @throws org.apache.spark.sql.catalyst.analysis.NoSuchNamespaceException If the namespace does\n+   *     not exist (optional).\n+   */\n+  @Override\n+  public Identifier[] listFunctions(String[] namespace) throws NoSuchNamespaceException {\n+    TableCatalog tableCatalog = tableCatalog(TableFormat.ICEBERG);\n+    if (tableCatalog instanceof FunctionCatalog) {\n+      return ((FunctionCatalog) tableCatalog).listFunctions(namespace);\n+    }\n+    throw new NoSuchNamespaceException(namespace);\n+  }\n+\n+  /**\n+   * Load a function by {@link org.apache.spark.sql.connector.catalog.Identifier identifier} from\n+   * the catalog.\n+   *\n+   * @param ident a function identifier\n+   * @return an unbound function instance\n+   * @throws org.apache.spark.sql.catalyst.analysis.NoSuchFunctionException If the function doesn't\n+   *     exist\n+   */\n+  @Override\n+  public UnboundFunction loadFunction(Identifier ident) throws NoSuchFunctionException {\n+\n+    TableCatalog tableCatalog = tableCatalog(TableFormat.ICEBERG);\n+    if (tableCatalog instanceof FunctionCatalog) {\n+      return ((FunctionCatalog) tableCatalog).loadFunction(ident);\n+    }\n+    throw new NoSuchFunctionException(ident);\n+  }\n+\n+  /**\n+   * Load table metadata of a specific version by {@link\n+   * org.apache.spark.sql.connector.catalog.Identifier identifier} from the catalog.\n+   *\n+   * <p>If the catalog supports views and contains a view for the identifier and not a table, this\n+   * must throw {@link org.apache.spark.sql.catalyst.analysis.NoSuchTableException}.\n+   *\n+   * @param ident a table identifier\n+   * @param version version of the table\n+   * @return the table's metadata\n+   * @throws org.apache.spark.sql.catalyst.analysis.NoSuchTableException If the table doesn't exist\n+   *     or is a view\n+   */\n+  @Override\n+  public Table loadTable(Identifier ident, String version) throws NoSuchTableException {\n+    TableCatalog tableCatalog = tableCatalog(TableFormat.ICEBERG);\n+    if (tableCatalog == null) {\n+      throw new UnsupportedOperationException(\"Doesn't support iceberg table catalog\");\n+    }\n+    return tableCatalog.loadTable(ident, version);\n+  }\n+\n+  /**\n+   * Load table metadata at a specific time by {@link\n+   * org.apache.spark.sql.connector.catalog.Identifier identifier} from the catalog.\n+   *\n+   * <p>If the catalog supports views and contains a view for the identifier and not a table, this\n+   * must throw {@link org.apache.spark.sql.catalyst.analysis.NoSuchTableException}.\n+   *\n+   * @param ident a table identifier\n+   * @param timestamp timestamp of the table, which is microseconds since 1970-01-01 00:00:00 UTC\n+   * @return the table's metadata\n+   * @throws org.apache.spark.sql.catalyst.analysis.NoSuchTableException If the table doesn't exist\n+   *     or is a view\n+   */\n+  @Override\n+  public Table loadTable(Identifier ident, long timestamp) throws NoSuchTableException {\n+    TableCatalog tableCatalog = tableCatalog(TableFormat.ICEBERG);\n+    if (tableCatalog == null) {\n+      throw new UnsupportedOperationException(\"Only support iceberg format now!\");\n+    }\n+    return tableCatalog.loadTable(ident, timestamp);\n+  }\n+}",
    "section": "code",
    "embedding": "[0.00010909828,-0.0038113133,-0.031834543,0.019755904,0.06625103,-0.029023254,0.052164633,-0.0026137359,0.020640684,-0.030147478,-0.012138305,-0.0072242566,-0.07324683,0.0435386,-0.011916909,0.056032024,0.0274083,0.028622763,0.044948634,-0.012367391,-0.023124425,-0.013235583,0.0048766662,0.019972952,-0.039411627,0.0476044,0.012337373,0.019561676,-0.08196629,0.009752147,0.040797126,"
  },
  {
    "pr_id": "e2e361f7-5f72-4045-9534-c2cb38d536b9",
    "content": "@@ -0,0 +1,132 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.amoro.spark;\n+\n+import org.apache.amoro.TableFormat;\n+import org.apache.iceberg.spark.functions.SparkFunctions;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchFunctionException;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchNamespaceException;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchTableException;\n+import org.apache.spark.sql.catalyst.analysis.NonEmptyNamespaceException;\n+import org.apache.spark.sql.connector.catalog.FunctionCatalog;\n+import org.apache.spark.sql.connector.catalog.Identifier;\n+import org.apache.spark.sql.connector.catalog.SupportsNamespaces;\n+import org.apache.spark.sql.connector.catalog.Table;\n+import org.apache.spark.sql.connector.catalog.TableCatalog;\n+import org.apache.spark.sql.connector.catalog.functions.UnboundFunction;\n+import org.apache.spark.sql.util.CaseInsensitiveStringMap;\n+\n+/**\n+ * For TableCatalog in spark 3.3 is different with spark 3.2。 so we define it separately 1、 we\n+ * support the grammar feature of time travel. 2、 support FunctionCatalog\n+ */\n+public class SparkUnifiedSessionCatalog<\n+        T extends TableCatalog & SupportsNamespaces & FunctionCatalog>\n+    extends SparkUnifiedSessionCatalogBase<T> {\n+\n+  @Override\n+  protected SparkUnifiedCatalogBase createUnifiedCatalog(\n+      String name, CaseInsensitiveStringMap options) {\n+    return new SparkUnifiedCatalog();\n+  }\n+\n+  @Override\n+  public Table loadTable(Identifier ident, String version) throws NoSuchTableException {\n+    try {\n+      TableCatalog catalog = getTargetCatalog();\n+      SparkUnifiedCatalogBase unifiedCatalog = (SparkUnifiedCatalogBase) catalog;\n+      return unifiedCatalog.tableCatalog(TableFormat.ICEBERG).loadTable(ident, version);\n+    } catch (org.apache.iceberg.exceptions.NoSuchTableException e) {\n+      return getSessionCatalog().loadTable(ident, version);\n+    }\n+  }\n+\n+  @Override\n+  public Table loadTable(Identifier ident, long timestamp) throws NoSuchTableException {\n+    try {\n+      TableCatalog catalog = getTargetCatalog();\n+      SparkUnifiedCatalogBase unifiedCatalog = (SparkUnifiedCatalogBase) catalog;\n+      return unifiedCatalog.tableCatalog(TableFormat.ICEBERG).loadTable(ident, timestamp);\n+    } catch (org.apache.iceberg.exceptions.NoSuchTableException e) {\n+      return getSessionCatalog().loadTable(ident, timestamp);\n+    }\n+  }\n+\n+  /**\n+   * List the functions in a namespace from the catalog.\n+   *\n+   * <p>If there are no functions in the namespace, implementations should return an empty array.\n+   *\n+   * @param namespace a multi-part namespace\n+   * @return an array of Identifiers for functions\n+   * @throws org.apache.spark.sql.catalyst.analysis.NoSuchNamespaceException If the namespace does\n+   *     not exist (optional).\n+   */\n+  @Override\n+  public Identifier[] listFunctions(String[] namespace) throws NoSuchNamespaceException {\n+    SparkUnifiedCatalog catalog = (SparkUnifiedCatalog) getTargetCatalog();\n+    return catalog.listFunctions(namespace);\n+  }\n+\n+  @Override\n+  public UnboundFunction loadFunction(Identifier ident) throws NoSuchFunctionException {\n+    String[] namespace = ident.namespace();\n+    String name = ident.name();\n+\n+    // Allow for empty namespace, as Spark's storage partitioned joins look up\n+    // the corresponding functions to generate transforms for partitioning\n+    // with an empty namespace, such as `bucket`.\n+    // Otherwise, use `system` namespace.\n+    if (namespace.length == 0 || isSystemNamespace(namespace)) {\n+      UnboundFunction func = SparkFunctions.load(name);\n+      if (func != null) {\n+        return func;\n+      }\n+    }\n+\n+    throw new NoSuchFunctionException(ident);\n+  }\n+\n+  private static boolean isSystemNamespace(String[] namespace) {\n+    return namespace.length == 1 && namespace[0].equalsIgnoreCase(\"system\");\n+  }\n+\n+  /**\n+   * Drop a namespace from the catalog with cascade mode, recursively dropping all objects within\n+   * the namespace if cascade is true.\n+   *\n+   * <p>If the catalog implementation does not support this operation, it may throw {@link\n+   * UnsupportedOperationException}.\n+   *\n+   * @param namespace a multi-part namespace\n+   * @param cascade When true, deletes all objects under the namespace\n+   * @return true if the namespace was dropped\n+   * @throws org.apache.spark.sql.catalyst.analysis.NoSuchNamespaceException If the namespace does\n+   *     not exist (optional)\n+   * @throws org.apache.spark.sql.catalyst.analysis.NonEmptyNamespaceException If the namespace is\n+   *     non-empty and cascade is false\n+   * @throws UnsupportedOperationException If drop is not a supported operation\n+   */\n+  @Override\n+  public boolean dropNamespace(String[] namespace, boolean cascade)\n+      throws NoSuchNamespaceException, NonEmptyNamespaceException {\n+    SparkUnifiedCatalog catalog = (SparkUnifiedCatalog) getTargetCatalog();\n+    return catalog.dropNamespace(namespace, cascade);\n+  }\n+}",
    "section": "code",
    "embedding": "[-0.0026899609,0.0056720944,-0.028914051,0.014567094,0.067991756,-0.035244346,0.03677739,0.012034427,0.026548067,-0.027949462,-0.009965802,-0.025900591,-0.058530193,0.053699844,-0.014296047,0.0567453,0.044472836,-0.0014585454,0.048564978,-0.021917699,-0.016789494,-0.00576134,0.009414701,0.023230378,-0.03538386,0.03226181,-6.121361e-05,0.022606961,-0.07874235,-0.011792494,0.044063,0"
  },
  {
    "pr_id": "b2e982f0-8b05-43bf-a510-04e8a37c4771",
    "content": "@@ -43,7 +43,7 @@ You can also check out the [plugins here](https://answer.apache.org/plugins).\n - Golang >= 1.23\n - Node.js >= 20\n - pnpm >= 9\n-- [mockgen](https://github.com/uber-go/mock?tab=readme-ov-file#installation) >= 1.6.0\n+- [mockgen](https://github.com/uber-go/mock?tab=readme-ov-file#installation) >= 0.6.0\n - [wire](https://github.com/google/wire/) >= 0.5.0\n \n ### Build",
    "section": "code",
    "embedding": "[-0.016644053,-0.012890814,-0.0025995807,0.023978978,0.046898678,0.00621111,0.07414453,0.04865786,0.029563142,-0.027566861,0.046482172,-0.003108681,-0.103003405,-0.0020203132,-0.0032370184,0.033243034,0.053490855,-0.014690715,0.021782732,-0.029820202,0.030239766,0.000612282,-0.011984082,0.021405311,-0.03650535,-0.026337165,0.053881854,-0.0036896532,-0.051233586,-0.04257661,-0.00067"
  },
  {
    "pr_id": "b2e982f0-8b05-43bf-a510-04e8a37c4771",
    "content": "@@ -1,5 +1,4 @@\n //go:build wireinject\n-// +build wireinject\n \n /*\n  * Licensed to the Apache Software Foundation (ASF) under one",
    "section": "code",
    "embedding": "[0.031064996,-0.040989045,0.023814099,-0.012495063,0.027633594,0.00822447,0.09859508,-0.026283888,0.041623723,-0.037129242,0.011906976,-0.01724646,-0.09842549,0.040272567,-0.034220845,0.056618083,0.055861033,-0.008244961,0.044006746,-0.01171539,0.008552164,0.039176337,0.01526468,0.020167165,0.004063974,0.037172224,0.03554347,-0.004786389,-0.03352993,-0.036645185,0.044237427,0.01428"
  },
  {
    "pr_id": "458c49ba-dd61-47d9-99ad-f67485e4e8c5",
    "content": "",
    "section": "prose",
    "embedding": "[0.012244709,0.039513454,-0.007785414,0.0021320505,0.046604965,0.012159259,0.03977907,0.04207942,-0.020763515,-0.039035235,0.024283497,-0.00805646,-0.04972243,0.017127944,-0.009930698,0.038848016,0.008790611,0.0133490125,-0.017834641,0.0071114884,0.030171363,-0.022801451,0.031216213,-0.00092527375,0.025564943,-0.024615467,0.027907416,0.0069565354,-0.02231851,0.03557899,0.019480716,"
  },
  {
    "pr_id": "e2e361f7-5f72-4045-9534-c2cb38d536b9",
    "content": "@@ -0,0 +1,63 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.amoro.spark;\n+\n+import org.apache.iceberg.spark.functions.SparkFunctions;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchFunctionException;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchNamespaceException;\n+import org.apache.spark.sql.connector.catalog.FunctionCatalog;\n+import org.apache.spark.sql.connector.catalog.Identifier;\n+import org.apache.spark.sql.connector.catalog.functions.UnboundFunction;\n+\n+interface SupportsFunctions extends FunctionCatalog {\n+\n+  default boolean isFunctionNamespace(String[] namespace) {\n+    return namespace.length == 0;\n+  }\n+\n+  default boolean isExistingNamespace(String[] namespace) {\n+    return namespace.length == 0;\n+  }\n+\n+  default Identifier[] listFunctions(String[] namespace) throws NoSuchNamespaceException {\n+    if (isFunctionNamespace(namespace)) {\n+      return SparkFunctions.list().stream()\n+          .map(name -> Identifier.of(namespace, name))\n+          .toArray(Identifier[]::new);\n+    } else if (isExistingNamespace(namespace)) {\n+      return new Identifier[0];\n+    }\n+\n+    throw new NoSuchNamespaceException(namespace);\n+  }\n+\n+  default UnboundFunction loadFunction(Identifier ident) throws NoSuchFunctionException {\n+    String[] namespace = ident.namespace();\n+    String name = ident.name();\n+\n+    if (isFunctionNamespace(namespace)) {\n+      UnboundFunction func = SparkFunctions.load(name);\n+      if (func != null) {\n+        return func;\n+      }\n+    }\n+\n+    throw new NoSuchFunctionException(ident);\n+  }\n+}",
    "section": "code",
    "embedding": "[-0.013660368,-0.013532998,-0.03679143,0.0034113936,0.05455865,-0.024120532,0.048629664,-0.00775166,0.009517539,-0.030750062,-0.014242502,-0.018602032,-0.057919037,0.048570037,-0.014589375,0.053527717,0.028125525,0.028134473,0.04258409,-0.013026245,-0.0204488,-0.0048711165,-0.0012214128,0.035189133,-0.033552315,0.04445603,0.008386184,0.019738972,-0.05862463,0.002737923,0.04504576,0"
  },
  {
    "pr_id": "e2e361f7-5f72-4045-9534-c2cb38d536b9",
    "content": "@@ -0,0 +1,349 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.amoro.spark.command;\n+\n+import org.apache.amoro.shade.guava32.com.google.common.base.Joiner;\n+import org.apache.amoro.shade.guava32.com.google.common.base.Preconditions;\n+import org.apache.amoro.shade.guava32.com.google.common.collect.ImmutableList;\n+import org.apache.amoro.shade.guava32.com.google.common.collect.Lists;\n+import org.apache.amoro.shade.guava32.com.google.common.collect.Maps;\n+import org.apache.amoro.spark.MixedFormatSparkCatalog;\n+import org.apache.amoro.spark.MixedFormatSparkSessionCatalog;\n+import org.apache.amoro.spark.table.MixedSparkTable;\n+import org.apache.amoro.spark.table.UnkeyedSparkTable;\n+import org.apache.amoro.spark.util.MixedFormatSparkUtils;\n+import org.apache.amoro.table.UnkeyedTable;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.iceberg.AppendFiles;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.MetricsConfig;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.data.TableMigrationUtil;\n+import org.apache.iceberg.hadoop.Util;\n+import org.apache.iceberg.spark.SparkSchemaUtil;\n+import org.apache.iceberg.spark.SparkTableUtil;\n+import org.apache.spark.sql.AnalysisException;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.RowFactory;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.catalyst.TableIdentifier;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchDatabaseException;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchNamespaceException;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchTableException;\n+import org.apache.spark.sql.catalyst.analysis.TableAlreadyExistsException;\n+import org.apache.spark.sql.catalyst.catalog.CatalogTable;\n+import org.apache.spark.sql.connector.catalog.CatalogManager;\n+import org.apache.spark.sql.connector.catalog.Identifier;\n+import org.apache.spark.sql.connector.catalog.SupportsNamespaces;\n+import org.apache.spark.sql.connector.catalog.Table;\n+import org.apache.spark.sql.connector.catalog.TableCatalog;\n+import org.apache.spark.sql.connector.catalog.V1Table;\n+import org.apache.spark.sql.connector.expressions.Transform;\n+import org.apache.spark.sql.types.DataTypes;\n+import org.apache.spark.sql.types.Metadata;\n+import org.apache.spark.sql.types.StructField;\n+import org.apache.spark.sql.types.StructType;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import scala.Option;\n+import scala.Some;\n+\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * migrate a v1 table to mixed-format table. will reuse file in v1 table , but delete metadata in\n+ * session catalog\n+ */\n+public class MigrateToMixedFormatCommand implements MixedFormatSparkCommand {\n+  private static final Logger LOG = LoggerFactory.getLogger(MigrateToMixedFormatCommand.class);\n+\n+  private static final String V1TABLE_BACKUP_SUFFIX = \"_BAK_MIXED_FORMAT_\";\n+  protected static final List<String> EXCLUDED_PROPERTIES =\n+      ImmutableList.of(\"path\", \"transient_lastDdlTime\", \"serialization.format\");\n+\n+  private static final StructType OUTPUT_TYPE =\n+      new StructType(\n+          new StructField[] {\n+            new StructField(\"partition\", DataTypes.StringType, false, Metadata.empty()),\n+            new StructField(\"file_counts\", DataTypes.IntegerType, false, Metadata.empty())\n+          });\n+\n+  private final SparkSession spark;\n+  private final TableCatalog sourceCatalog;\n+  private final Identifier sourceIdentifier;\n+  private final Identifier backupV1TableIdentifier;\n+  private final TableCatalog targetCatalog;\n+  private final Identifier targetIdentifier;\n+\n+  protected MigrateToMixedFormatCommand(\n+      TableCatalog sourceCatalog,\n+      Identifier sourceIdentifier,\n+      TableCatalog catalog,\n+      Identifier identifier,\n+      SparkSession spark) {\n+    this.spark = spark;\n+    this.sourceCatalog = sourceCatalog;\n+    this.targetCatalog = catalog;\n+    this.targetIdentifier = identifier;\n+    this.sourceIdentifier = sourceIdentifier;\n+    String backupName = sourceIdentifier.name();\n+    backupV1TableIdentifier = Identifier.of(sourceIdentifier.namespace(), backupName);\n+  }\n+\n+  @Override\n+  public String name() {\n+    return \"MigrateToMixedFormat\";\n+  }\n+\n+  @Override\n+  public StructType outputType() {\n+    return OUTPUT_TYPE;\n+  }\n+\n+  @Override\n+  public Row[] execute() throws AnalysisException {\n+    List<DataFile> dataFiles;\n+    TableIdentifier ident;\n+    PartitionSpec spec;\n+    Schema schema;\n+    LOG.info(\n+        \"start to migrate {} to {}, using temp backup table {}\",\n+        sourceIdentifier,\n+        targetIdentifier,\n+        backupV1TableIdentifier);\n+    V1Table sourceTable = loadV1Table(sourceCatalog, backupV1TableIdentifier);\n+    ident =\n+        new TableIdentifier(\n+            backupV1TableIdentifier.name(), Some.apply(backupV1TableIdentifier.namespace()[0]));\n+    dataFiles = loadDataFiles(ident);\n+    UnkeyedTable table = createUnkeyedTable(sourceTable);\n+\n+    spec = table.spec();\n+\n+    AppendFiles appendFiles = table.newAppend();\n+    dataFiles.forEach(appendFiles::appendFile);\n+    appendFiles.commit();\n+\n+    LOG.info(\n+        \"migrate table {} finished, remove metadata of backup {} table\",\n+        targetIdentifier,\n+        backupV1TableIdentifier);\n+\n+    if (PartitionSpec.unpartitioned().equals(spec)) {\n+      return new Row[] {RowFactory.create(\"ALL\", dataFiles.size())};\n+    }\n+\n+    Map<String, List<DataFile>> partitions = Maps.newHashMap();\n+    dataFiles.forEach(\n+        d -> {\n+          String partition = spec.partitionToPath(d.partition());\n+          List<DataFile> df = partitions.computeIfAbsent(partition, p -> Lists.newArrayList());\n+          df.add(d);\n+        });\n+    return partitions.keySet().stream()\n+        .sorted()\n+        .map(p -> RowFactory.create(p, partitions.get(p).size()))\n+        .toArray(Row[]::new);\n+  }\n+\n+  private List<DataFile> loadDataFiles(TableIdentifier ident) throws AnalysisException {\n+    PartitionSpec spec =\n+        SparkSchemaUtil.specForTable(spark, ident.database().get() + \".\" + ident.table());\n+\n+    if (spec.equals(PartitionSpec.unpartitioned())) {\n+      return listUnPartitionedSparkTable(spark, ident);\n+    } else {\n+      List<SparkTableUtil.SparkPartition> sparkPartitions =\n+          SparkTableUtil.getPartitions(spark, ident, Maps.newHashMap());\n+      Preconditions.checkArgument(\n+          !sparkPartitions.isEmpty(), \"Cannot find any partitions in table %s\", ident);\n+      return listPartitionDataFiles(spark, sparkPartitions, spec);\n+    }\n+  }\n+\n+  private UnkeyedTable createUnkeyedTable(V1Table sourceTable)\n+      throws TableAlreadyExistsException, NoSuchNamespaceException {\n+    Map<String, String> properties = Maps.newHashMap();\n+    properties.putAll(sourceTable.properties());\n+    EXCLUDED_PROPERTIES.forEach(properties::remove);\n+    properties.put(TableCatalog.PROP_PROVIDER, \"arctic\");\n+    properties.put(\"migrated\", \"true\");\n+\n+    StructType schema = sourceTable.schema();\n+    Transform[] partitions = sourceTable.partitioning();\n+    boolean threw = true;\n+    Table table = null;\n+    try {\n+      table = targetCatalog.createTable(targetIdentifier, schema, partitions, properties);\n+      if (table instanceof UnkeyedSparkTable) {\n+        threw = false;\n+        return ((UnkeyedSparkTable) table).table();\n+      } else if (table instanceof MixedSparkTable) {\n+        threw = false;\n+        return ((MixedSparkTable) table).table().asUnkeyedTable();\n+      }\n+      throw new IllegalStateException(\"target table must be un-keyed table\");\n+    } finally {\n+      if (threw && table != null) {\n+        try {\n+          targetCatalog.dropTable(targetIdentifier);\n+        } catch (Exception e) {\n+          LOG.warn(\"error when rollback table\", e);\n+        }\n+      }\n+    }\n+  }\n+\n+  private static V1Table loadV1Table(TableCatalog catalog, Identifier identifier)\n+      throws NoSuchTableException {\n+    Table table = catalog.loadTable(identifier);\n+    Preconditions.checkArgument(table instanceof V1Table, \"source table must be V1Table\");\n+    return (V1Table) table;\n+  }\n+\n+  private static List<DataFile> listUnPartitionedSparkTable(\n+      SparkSession spark, TableIdentifier sourceTableIdent)\n+      throws NoSuchDatabaseException, NoSuchTableException {\n+    CatalogTable sourceTable = spark.sessionState().catalog().getTableMetadata(sourceTableIdent);\n+    Option<String> format =\n+        sourceTable.storage().serde().nonEmpty()\n+            ? sourceTable.storage().serde()\n+            : sourceTable.provider();\n+    Preconditions.checkArgument(format.nonEmpty(), \"Could not determine table format\");\n+\n+    Map<String, String> partition = Collections.emptyMap();\n+    PartitionSpec spec = PartitionSpec.unpartitioned();\n+    Configuration conf = spark.sessionState().newHadoopConf();\n+    MetricsConfig metricsConfig = MetricsConfig.getDefault();\n+    return TableMigrationUtil.listPartition(\n+        partition,\n+        Util.uriToString(sourceTable.location()),\n+        format.get(),\n+        spec,\n+        conf,\n+        metricsConfig,\n+        null);\n+  }\n+\n+  private static List<DataFile> listPartitionDataFiles(\n+      SparkSession spark, List<SparkTableUtil.SparkPartition> partitions, PartitionSpec spec) {\n+\n+    Configuration conf = spark.sessionState().newHadoopConf();\n+    MetricsConfig metricsConfig = MetricsConfig.getDefault();\n+\n+    return partitions.stream()\n+        .map(\n+            p ->\n+                TableMigrationUtil.listPartition(\n+                    p.getValues(), p.getUri(), p.getFormat(), spec, conf, metricsConfig, null))\n+        .flatMap(Collection::stream)\n+        .collect(Collectors.toList());\n+  }\n+\n+  public static Builder newBuilder(SparkSession spark) {\n+    return new Builder(spark);\n+  }\n+\n+  public static class Builder {\n+\n+    List<String> source;\n+    List<String> target;\n+\n+    SparkSession spark;\n+\n+    private Builder(SparkSession spark) {\n+      this.spark = spark;\n+    }\n+\n+    public Builder withSource(List<String> source) {\n+      this.source = source;\n+      return this;\n+    }\n+\n+    public Builder withTarget(List<String> target) {\n+      this.target = target;\n+      return this;\n+    }\n+\n+    public MigrateToMixedFormatCommand build() throws NoSuchTableException {\n+      MixedFormatSparkUtils.TableCatalogAndIdentifier tableCatalogAndIdentifier =\n+          MixedFormatSparkUtils.tableCatalogAndIdentifier(spark, source);\n+      TableCatalog sourceCatalog = tableCatalogAndIdentifier.catalog();\n+      Identifier sourceTableIdentifier = tableCatalogAndIdentifier.identifier();\n+\n+      checkSourceCatalogAndTable(sourceCatalog, sourceTableIdentifier);\n+\n+      tableCatalogAndIdentifier = MixedFormatSparkUtils.tableCatalogAndIdentifier(spark, target);\n+      TableCatalog targetCatalog = tableCatalogAndIdentifier.catalog();\n+      Identifier targetTableIdentifier = tableCatalogAndIdentifier.identifier();\n+\n+      checkTargetCatalog(targetCatalog);\n+      checkTargetTable(targetCatalog, targetTableIdentifier);\n+\n+      \n...<truncated>...",
    "section": "code",
    "embedding": "[0.0024004104,-0.026974974,-0.012619794,0.051242683,0.042281676,-0.009781282,0.045455016,0.037549466,0.010161707,-0.028280927,-0.025896212,-0.031924393,-0.0977997,0.06369448,-0.022567134,0.06158112,0.028187148,0.015320892,0.024759538,-0.014396875,0.007492248,-0.004734488,-0.008663449,0.024743285,-0.061640542,0.03352443,0.03091757,0.034146193,-0.060414102,0.0048005735,0.031003647,0."
  },
  {
    "pr_id": "e2e361f7-5f72-4045-9534-c2cb38d536b9",
    "content": "## [Codecov](https://app.codecov.io/gh/apache/amoro/pull/4007?dropdown=coverage&src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=apache) Report\n:x: Patch coverage is `78.20513%` with `17 lines` in your changes missing coverage. Please review.\n:white_check_mark: Project coverage is 7.93%. Comparing base ([`cbdc517`](https://app.codecov.io/gh/apache/amoro/commit/cbdc5176b760b2ab46daa5051f29b72f1fee7175?dropdown=coverage&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=apache)) to head ([`a14d3d8`](https://app.codecov.io/gh/apache/amoro/commit/a14d3d8661e825a11ab01c0e9eb8a54fc0c9b615?dropdown=coverage&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=apache)).\n:warning: Report is 12 commits behind head on master.\n\n| [Files with missing lines](https://app.codecov.io/gh/apache/amoro/pull/4007?dropdown=coverage&src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=apache) | Patch % | Lines |\n|---|---|---|\n| [.../sql/amoro/catalyst/MixedFormatSpark34Helper.scala](https://app.codecov.io/gh/apache/amoro/pull/4007?src=pr&el=tree&filepath=amoro-format-mixed%2Famoro-mixed-spark%2Fv3.4%2Famoro-mixed-spark-3.4%2Fsrc%2Fmain%2Fscala%2Forg%2Fapache%2Fspark%2Fsql%2Famoro%2Fcatalyst%2FMixedFormatSpark34Helper.scala&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=apache#diff-YW1vcm8tZm9ybWF0LW1peGVkL2Ftb3JvLW1peGVkLXNwYXJrL3YzLjQvYW1vcm8tbWl4ZWQtc3BhcmstMy40L3NyYy9tYWluL3NjYWxhL29yZy9hcGFjaGUvc3Bhcmsvc3FsL2Ftb3JvL2NhdGFseXN0L01peGVkRm9ybWF0U3BhcmszNEhlbHBlci5zY2FsYQ==) | 77.33% | [9 Missing and 8 partials :warning: ](https://app.codecov.io/gh/apache/amoro/pull/4007?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=apache) |\n> :exclamation:  There is a different number of reports uploaded between BASE (cbdc517) and HEAD (a14d3d8). Click for more details.\n> \n> <details><summary>HEAD has 1 upload less than BASE</summary>\n>\n>| Flag | BASE (cbdc517) | HEAD (a14d3d8) |\n>|------|------|------|\n>|trino|1|0|\n></details>\n\n<details><summary>Additional details and impacted files</summary>\n\n\n\n```diff\n@@             Coverage Diff              @@\n##             master   #4007       +/-   ##\n============================================\n- Coverage     22.12%   7.93%   -14.20%     \n+ Complexity     2461     899     -1562     \n============================================\n  Files           445     641      +196     \n  Lines         40897   51507    +10610     \n  Branches       5767    6552      +785     \n============================================\n- Hits           9050    4085     -4965     \n- Misses        31089   47160    +16071     \n+ Partials        758     262      -496     \n```\n\n| [Flag](https://app.codecov.io/gh/apache/amoro/pull/4007/flags?src=pr&el=flags&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=apache) | Coverage Δ | |\n|---|---|---|\n| [core](https://app.codecov.io/gh/apache/amoro/pull/4007/flags?src=pr&el=flag&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=apache) | `7.93% <78.20%> (?)` | |\n| [trino](https://app.codecov.io/gh/apache/amoro/pull/4007/flags?src=pr&el=flag&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=apache) | `?` | |\n\nFlags with carried forward coverage won't be shown. [Click here](https://docs.codecov.io/docs/carryforward-flags?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=apache#carryforward-flags-in-the-pull-request-comment) to find out more.\n</details>\n\n[:umbrella: View full report in Codecov by Sentry](https://app.codecov.io/gh/apache/amoro/pull/4007?dropdown=coverage&src=pr&el=continue&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=apache).   \n:loudspeaker: Have feedback on the report? [Share it here](https://about.codecov.io/codecov-pr-comment-feedback/?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=apache).\n<details><summary> :rocket: New features to boost your workflow: </summary>\n\n- :snowflake: [Test Analytics](https://docs.codecov.com/docs/test-analytics): Detect flaky tests, report on failures, and find test suite problems.\n- :package: [JS Bundle Analysis](https://docs.codecov.com/docs/javascript-bundle-analysis): Save yourself from yourself by tracking and limiting bundle sizes in JS merges.\n</details>",
    "section": "comment",
    "embedding": "[0.038051873,-0.011407859,0.015364271,5.7377006e-06,0.06865615,0.02415269,0.04314447,0.03578874,0.0041027884,-0.030510502,-0.016048335,0.016185852,-0.09371455,0.0064022006,-0.020096026,0.08589159,0.06202266,0.034185376,0.0336152,0.0041280994,0.029584683,0.025065089,-0.041735206,0.019065795,-0.017291646,-0.035954412,0.03910534,-0.0063616526,-0.057610378,-0.02190731,0.04033938,0.0126"
  },
  {
    "pr_id": "e2e361f7-5f72-4045-9534-c2cb38d536b9",
    "content": "@turboFei I would be very grateful if you could help review this PR.",
    "section": "comment",
    "embedding": "[0.045857556,-0.033902526,0.032797124,0.024711756,0.020553993,0.03757724,0.003947975,0.005112598,0.013848848,-0.016019428,0.03162534,0.017330073,-0.047591403,0.010387493,0.032105908,0.075217046,0.029290978,0.057248585,0.01737141,-0.022600846,0.0010444018,0.028712433,0.029288778,0.0029226146,0.01628785,0.0021791046,0.04144772,0.039855488,-0.0760593,-0.03604068,0.034920834,0.01021727"
  },
  {
    "pr_id": "0d87991c-85ba-4516-8473-d9424d326e2b",
    "content": "TITLE: [Feature]: Support LDAP Authentication for Dashboard Login",
    "section": "title",
    "embedding": "[-0.0062007033,-0.0047613564,-0.009901463,0.059428226,0.057369802,0.008525974,0.054172445,-0.023689235,0.006502955,-0.05285065,-0.038798727,-0.03007719,-0.05774262,-0.018245945,0.02255836,0.01810661,0.009851047,-0.0074480237,0.026146011,0.00797581,-0.036490694,0.018183794,-0.030029733,0.004771544,0.02083438,0.027302558,-0.011677611,-0.00766616,-0.07068333,-0.011501519,0.025087507,0"
  },
  {
    "pr_id": "0d87991c-85ba-4516-8473-d9424d326e2b",
    "content": "TITLE: [Feature]: Support LDAP Authentication for Dashboard Login\n\nBODY:\n## Why are the changes needed?\r\nThe current Dashboard login uses an admin-user with a plaintext password, which is highly insecure. To enhance security, centralize user management, and align with industry standard practices, we need to switch to LDAP authentication.\r\n\r\nClose #4008.\r\n\r\n## Brief change log\r\n- Introduced three new login-related configurations in AmoroManagementConf.\r\n- Added LdapPasswdAuthenticationProvider to support LDAP integration (implements PasswdAuthenticationProvider).\r\n- Updated LoginController to use PasswdAuthenticationProvider for user login validation.\r\n\r\n\r\n## How was this patch tested?\r\n\r\n- [ ] Add some test cases that check the changes thoroughly including negative and positive cases if possible\r\n\r\n- [ ] Run test locally before making a pull request\r\n\r\n## Documentation\r\n\r\n- Does this pull request introduce a new feature? (yes / no) yes\r\n- If yes, how is the feature documented? (not applicable / docs / JavaDocs / not documented) not documented\r\n",
    "section": "full",
    "embedding": "[-0.02047082,-0.031241924,0.0016844736,0.0476647,0.081154265,-0.016109178,0.061581187,0.00048039845,0.0034292373,-0.06565143,-0.018831208,-0.032711085,-0.06256487,-0.0037553562,-0.022302205,0.05858739,0.034020994,-0.006545085,0.026389018,0.0014626666,-0.01954711,0.017810076,-0.00804846,0.018459354,0.031001782,0.043097377,-0.025890933,-0.0056044613,-0.081314884,-0.04066757,0.0302724"
  },
  {
    "pr_id": "0d87991c-85ba-4516-8473-d9424d326e2b",
    "content": "## Why are the changes needed?\r\nThe current Dashboard login uses an admin-user with a plaintext password, which is highly insecure. To enhance security, centralize user management, and align with industry standard practices, we need to switch to LDAP authentication.\r\n\r\nClose #4008.\r\n\r\n## Brief change log\r\n- Introduced three new login-related configurations in AmoroManagementConf.\r\n- Added LdapPasswdAuthenticationProvider to support LDAP integration (implements PasswdAuthenticationProvider).\r\n- Updated LoginController to use PasswdAuthenticationProvider for user login validation.\r\n\r\n\r\n## How was this patch tested?\r\n\r\n- [ ] Add some test cases that check the changes thoroughly including negative and positive cases if possible\r\n\r\n- [ ] Run test locally before making a pull request\r\n\r\n## Documentation\r\n\r\n- Does this pull request introduce a new feature? (yes / no) yes\r\n- If yes, how is the feature documented? (not applicable / docs / JavaDocs / not documented) not documented",
    "section": "prose",
    "embedding": "[-0.032920573,-0.028743537,0.0028760873,0.05230826,0.07014532,-0.028978573,0.07567962,-0.0059981435,-0.00062368123,-0.06563462,-0.012233816,-0.014088422,-0.058221802,-0.012527963,-0.020006647,0.053668354,0.026327014,-0.0021931166,0.04313628,0.0140741095,-0.027742257,0.0071406257,-0.00996314,0.00072062964,0.011812301,0.023609402,-0.02890229,-0.01508698,-0.07390455,-0.03962509,0.0150"
  },
  {
    "pr_id": "0d87991c-85ba-4516-8473-d9424d326e2b",
    "content": "@@ -500,6 +500,13 @@\n             <version>${guava.version}</version>\n             <scope>test</scope>\n         </dependency>\n+\n+        <dependency>\n+            <groupId>org.apache.directory.server</groupId>\n+            <artifactId>apacheds-test-framework</artifactId>\n+            <version>${apache-directory-server.version}</version>\n+            <scope>test</scope>\n+        </dependency>\n     </dependencies>\n \n     <build>",
    "section": "code",
    "embedding": "[-0.024062604,-0.0038887837,0.0039636944,0.008752301,0.04776634,-0.01120035,0.057640284,0.00053252437,-0.028969813,-0.034224607,0.013765942,0.0070903893,-0.059401166,0.06794922,-0.03810013,0.06860894,0.05036527,-0.0031352479,0.017406072,-0.017198596,0.022571234,0.006250573,-0.008232737,0.035874225,0.020443397,0.029431736,0.09466263,0.017632809,-0.043334514,-0.05854691,0.025397249,0"
  },
  {
    "pr_id": "0d87991c-85ba-4516-8473-d9424d326e2b",
    "content": "@@ -310,6 +310,29 @@ public class AmoroManagementConf {\n               \"User-defined password authentication implementation of\"\n                   + \" org.apache.amoro.authentication.PasswdAuthenticationProvider\");\n \n+  public static final ConfigOption<String> HTTP_SERVER_LOGIN_AUTH_PROVIDER =\n+      ConfigOptions.key(\"http-server.login-auth-provider\")\n+          .stringType()\n+          .defaultValue(DefaultPasswdAuthenticationProvider.class.getName())\n+          .withDescription(\n+              \"User-defined login authentication implementation of\"\n+                  + \" org.apache.amoro.authentication.PasswdAuthenticationProvider\");\n+\n+  public static final ConfigOption<String> HTTP_SERVER_LOGIN_AUTH_LDAP_USERPARTTERN =\n+      ConfigOptions.key(\"http-server.login-auth-ldap-user-pattern\")\n+          .stringType()\n+          .defaultValue(\"cn={0},ou=people,dc=example,dc=com\")\n+          .withDescription(\n+              \"LDAP user pattern for authentication. The pattern defines how to construct the user's distinguished name (DN) in the LDAP directory. \"\n+                  + \"Use {0} as a placeholder for the username. For example, 'cn={0},ou=people,dc=example,dc=com' will search for users in the specified organizational unit.\");\n+\n+  public static final ConfigOption<String> HTTP_SERVER_LOGIN_AUTH_LDAP_URL =\n+      ConfigOptions.key(\"http-server.login-auth-ldap-url\")\n+          .stringType()\n+          .defaultValue(\"objectGUID\")\n+          .withDescription(\n+              \"LDAP connection URL(s), value could be a SPACE separated list of URLs to multiple LDAP servers for resiliency. URLs are tried in the order specified until the connection is successful\");\n+\n   public static final ConfigOption<String> HTTP_SERVER_AUTH_JWT_PROVIDER =\n       ConfigOptions.key(\"http-server.auth-jwt-provider\")\n           .stringType()",
    "section": "code",
    "embedding": "[-0.016535806,-0.0024260052,0.01843113,0.03479685,0.06813255,-0.0053239446,0.06939037,0.0030527269,-0.017408986,-0.03411595,-0.03935752,-0.032565784,-0.06897045,0.025250504,-0.016309258,0.002489789,0.025218623,-0.011465053,0.035816032,-0.028129904,-0.025422921,-0.0036871608,-0.017823089,0.0023562834,0.011540391,0.041060142,-0.0043240893,0.0151191475,-0.009636103,-0.03620187,0.01662"
  },
  {
    "pr_id": "b2e982f0-8b05-43bf-a510-04e8a37c4771",
    "content": "@@ -32,7 +31,7 @@ import (\n \t\"github.com/apache/answer/internal/base/server\"\n \t\"github.com/apache/answer/internal/base/translator\"\n \t\"github.com/apache/answer/internal/controller\"\n-\t\"github.com/apache/answer/internal/controller/template_render\"\n+\ttemplaterender \"github.com/apache/answer/internal/controller/template_render\"\n \t\"github.com/apache/answer/internal/controller_admin\"\n \t\"github.com/apache/answer/internal/repo\"\n \t\"github.com/apache/answer/internal/router\"",
    "section": "code",
    "embedding": "[-0.002095968,0.018011296,-0.0065241787,0.012021288,0.01655239,-0.013858087,0.05224838,0.012102123,-0.010073719,-0.04656319,0.011309107,-0.039623316,-0.0404262,0.04100931,-0.026122766,0.053817082,0.045517482,-0.016107805,0.0054263766,-0.011729569,0.004612588,0.008470958,0.0031991128,0.0042877765,-0.012940953,-0.023208918,0.049054883,0.015628535,-0.036458988,-0.011835434,0.02570515,"
  },
  {
    "pr_id": "aa46a847-37b9-44a4-b77a-3c1eb8e19161",
    "content": "TITLE: GH-48417: [Packaging][CI] Skip downgrade testing for Debian testing",
    "section": "title",
    "embedding": "[-0.011846241,0.028132726,-0.018100426,0.049197447,0.077607304,0.017687416,0.032424215,-0.010971659,0.030084595,-0.022553042,0.0069527826,0.04756715,-0.06682398,0.030339653,-0.0019439798,0.015376405,0.041309875,-0.0177182,0.0005334352,0.036866788,0.024040734,0.023551134,-0.06546194,0.010366416,-0.013291919,0.0013940219,0.03165133,0.020038305,-0.10935185,-0.009433827,0.05753531,-0.0"
  },
  {
    "pr_id": "0d87991c-85ba-4516-8473-d9424d326e2b",
    "content": "@@ -0,0 +1,75 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.amoro.server.authentication;\n+\n+import org.apache.amoro.authentication.BasicPrincipal;\n+import org.apache.amoro.authentication.PasswdAuthenticationProvider;\n+import org.apache.amoro.authentication.PasswordCredential;\n+import org.apache.amoro.config.Configurations;\n+import org.apache.amoro.exception.SignatureCheckException;\n+import org.apache.amoro.server.AmoroManagementConf;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import javax.naming.Context;\n+import javax.naming.NamingException;\n+import javax.naming.directory.InitialDirContext;\n+\n+import java.text.MessageFormat;\n+import java.util.Hashtable;\n+\n+public class LdapPasswdAuthenticationProvider implements PasswdAuthenticationProvider {\n+  public static final Logger LOG = LoggerFactory.getLogger(LdapPasswdAuthenticationProvider.class);\n+\n+  private String ldapUrl;\n+  private String ldapUserParttern;\n+  private MessageFormat formatter;\n+\n+  public LdapPasswdAuthenticationProvider(Configurations conf) {\n+    this.ldapUrl = conf.get(AmoroManagementConf.HTTP_SERVER_LOGIN_AUTH_LDAP_URL);\n+    this.ldapUserParttern = conf.get(AmoroManagementConf.HTTP_SERVER_LOGIN_AUTH_LDAP_USERPARTTERN);\n+    this.formatter = new MessageFormat(ldapUserParttern);\n+  }\n+\n+  @Override\n+  public BasicPrincipal authenticate(PasswordCredential credential) throws SignatureCheckException {\n+    Hashtable<String, Object> env = new Hashtable<String, Object>();\n+    env.put(Context.INITIAL_CONTEXT_FACTORY, \"com.sun.jndi.ldap.LdapCtxFactory\");\n+    env.put(Context.PROVIDER_URL, ldapUrl);\n+    env.put(Context.SECURITY_AUTHENTICATION, \"simple\");\n+    env.put(Context.SECURITY_CREDENTIALS, credential.password());\n+    env.put(Context.SECURITY_PRINCIPAL, formatter.format(new String[] {credential.username()}));\n+\n+    InitialDirContext initialLdapContext = null;\n+    try {\n+      initialLdapContext = new InitialDirContext(env);\n+    } catch (NamingException e) {\n+      throw new SignatureCheckException(\"Failed to authenticate via ldap authentication\", e);\n+    } finally {\n+      if (initialLdapContext != null) {\n+        try {\n+          initialLdapContext.close();\n+        } catch (NamingException e) {\n+          LOG.error(\"Exception in closing {}\", initialLdapContext, e);\n+        }\n+      }\n+    }\n+    return new BasicPrincipal(credential.username());\n+  }\n+}",
    "section": "code",
    "embedding": "[0.011675046,0.022199983,-0.017377574,0.014939197,0.044994894,-0.02509807,0.07934968,-0.0068182526,0.011054591,-0.021624317,-0.02610763,-0.02611189,-0.08933172,0.059523463,-0.0013386365,0.04389534,0.043111473,0.0062523186,0.03967664,-0.016534647,-0.012259632,-0.00750575,0.008586024,0.006741924,-0.024913732,0.040350053,0.009650213,0.038506214,-0.06017425,-0.008504212,0.027220305,-0."
  },
  {
    "pr_id": "0d87991c-85ba-4516-8473-d9424d326e2b",
    "content": "@@ -92,6 +92,8 @@ public class DashboardServer {\n \n   private final PasswdAuthenticationProvider basicAuthProvider;\n   private final TokenAuthenticationProvider jwtAuthProvider;\n+\n+  private final PasswdAuthenticationProvider loginAuthProvider;\n   private final String proxyClientIpHeader;\n \n   public DashboardServer(",
    "section": "code",
    "embedding": "[-0.03173921,-0.03583586,0.04938261,0.019472817,0.025094533,-0.025740309,0.09678469,0.010528641,-0.013930043,-0.055290826,0.039534084,-0.053820297,-0.023730192,0.032002248,0.012050416,0.006769603,0.04702453,-0.0062804027,0.012898832,0.011567701,-0.020033982,0.00870061,-0.025366893,-0.025128651,0.017909467,0.014230076,-0.0068615098,0.053972784,-0.058429334,-0.021485876,0.050279785,0"
  },
  {
    "pr_id": "0d87991c-85ba-4516-8473-d9424d326e2b",
    "content": "@@ -103,7 +105,7 @@ public DashboardServer(\n     PlatformFileManager platformFileManager = new PlatformFileManager();\n     this.catalogController = new CatalogController(catalogManager, platformFileManager);\n     this.healthCheckController = new HealthCheckController();\n-    this.loginController = new LoginController(serviceConfig);\n+    this.loginController = new LoginController(this);\n     this.optimizerGroupController = new OptimizerGroupController(tableManager, optimizerManager);\n     this.optimizerController = new OptimizerController(optimizerManager);\n     this.platformFileInfoController = new PlatformFileInfoController(platformFileManager);",
    "section": "code",
    "embedding": "[-0.041849073,-0.052674934,0.029064765,0.018722082,0.05797652,-0.023600137,0.019543894,0.02779722,-0.08292554,-0.0636654,0.012319959,-0.04685685,-0.016634353,0.027654586,0.023993233,-0.012635805,0.045312084,0.015856208,0.058212668,0.01808014,-0.034665853,0.0041173752,-0.024920952,-0.018532144,0.0048006694,0.016444873,0.003916299,0.0678874,-0.056946993,-0.0048394264,0.04194677,-0.02"
  },
  {
    "pr_id": "0d87991c-85ba-4516-8473-d9424d326e2b",
    "content": "@@ -131,6 +133,9 @@ public DashboardServer(\n             ? HttpAuthenticationFactory.getBearerAuthenticationProvider(\n                 serviceConfig.get(AmoroManagementConf.HTTP_SERVER_AUTH_JWT_PROVIDER), serviceConfig)\n             : null;\n+    this.loginAuthProvider =\n+        HttpAuthenticationFactory.getPasswordAuthenticationProvider(\n+            serviceConfig.get(AmoroManagementConf.HTTP_SERVER_LOGIN_AUTH_PROVIDER), serviceConfig);\n     this.proxyClientIpHeader =\n         serviceConfig.get(AmoroManagementConf.HTTP_SERVER_PROXY_CLIENT_IP_HEADER);\n   }",
    "section": "code",
    "embedding": "[-0.019095803,-0.051829018,0.041214824,-0.000120947785,0.03788104,-0.018629685,0.087558866,-0.0016332923,-0.044918746,-0.046596184,0.0075541837,-0.057508036,-0.02388038,0.012861283,-0.007940627,-0.001587166,0.067738965,0.00037495655,0.025622496,-0.0050196233,-0.011177632,0.009265784,-0.017219033,-0.009052137,0.035200816,0.010063668,0.004914505,0.045614813,-0.04396924,-0.01679955,0."
  },
  {
    "pr_id": "0d87991c-85ba-4516-8473-d9424d326e2b",
    "content": "@@ -484,4 +489,8 @@ private static boolean inWhiteList(String uri) {\n     }\n     return false;\n   }\n+\n+  public PasswdAuthenticationProvider getLoginAuthProvider() {\n+    return loginAuthProvider;\n+  }\n }",
    "section": "code",
    "embedding": "[0.004450373,-0.011368694,0.0054881833,0.022032727,0.042929597,-0.0014120743,0.0871284,-0.019352859,-0.017628007,-0.018073596,-0.019390779,-0.019086873,-0.033037037,0.018124865,-0.0003333697,0.06370409,0.051254075,-0.00081087317,0.032455873,-0.059414495,-0.024753392,0.030354446,-0.011127297,-0.012452806,0.021338716,0.02293992,0.0046125464,0.0067653246,-0.035212543,-0.017165665,0.07"
  },
  {
    "pr_id": "0d87991c-85ba-4516-8473-d9424d326e2b",
    "content": "@@ -19,22 +19,23 @@\n package org.apache.amoro.server.dashboard.controller;\n \n import io.javalin.http.Context;\n-import org.apache.amoro.config.Configurations;\n-import org.apache.amoro.server.AmoroManagementConf;\n+import org.apache.amoro.server.authentication.DefaultPasswordCredential;\n+import org.apache.amoro.server.dashboard.DashboardServer;\n import org.apache.amoro.server.dashboard.response.OkResponse;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n \n import java.io.Serializable;\n import java.util.Map;\n \n /** The controller that handles login requests. */\n public class LoginController {\n+  public static final Logger LOG = LoggerFactory.getLogger(LoginController.class);\n \n-  private final String adminUser;\n-  private final String adminPassword;\n+  private final DashboardServer dashboardServer;\n \n-  public LoginController(Configurations serviceConfig) {\n-    adminUser = serviceConfig.get(AmoroManagementConf.ADMIN_USERNAME);\n-    adminPassword = serviceConfig.get(AmoroManagementConf.ADMIN_PASSWORD);\n+  public LoginController(DashboardServer dashboardServer) {\n+    this.dashboardServer = dashboardServer;\n   }\n \n   /** Get current user. */",
    "section": "code",
    "embedding": "[-0.038489953,0.00107363,0.009686892,0.024731256,0.050411586,-0.030727817,0.04341069,0.027674861,-0.012513629,-0.06293109,-0.022553496,-0.051641796,-0.075910434,-0.003793922,-0.0061294995,0.019877518,0.03373621,0.0002723453,0.04430966,-0.015839543,0.00053056155,0.011051083,-0.012362619,0.0058109295,0.010629414,-0.0047472017,0.0016777812,0.036161605,-0.049462836,-0.02489038,0.013872"
  },
  {
    "pr_id": "0d87991c-85ba-4516-8473-d9424d326e2b",
    "content": "@@ -49,10 +50,11 @@ public void login(Context ctx) {\n     Map<String, String> bodyParams = ctx.bodyAsClass(Map.class);\n     String user = bodyParams.get(\"user\");\n     String pwd = bodyParams.get(\"password\");\n-    if (adminUser.equals(user) && (adminPassword.equals(pwd))) {\n-      ctx.sessionAttribute(\"user\", new SessionInfo(adminUser, System.currentTimeMillis() + \"\"));\n-      ctx.json(OkResponse.of(\"success\"));\n-    } else {\n+    DefaultPasswordCredential credential = new DefaultPasswordCredential(user, pwd);\n+    try {\n+      dashboardServer.getLoginAuthProvider().authenticate(credential);\n+    } catch (Exception e) {\n+      LOG.error(\"authenticate user {} failed\", user, e);\n       throw new RuntimeException(\"invalid user \" + user + \" or password!\");\n     }\n   }",
    "section": "code",
    "embedding": "[-0.028998738,-0.012671075,-0.013886888,0.025833508,0.056067843,-0.017747689,0.049040098,0.043037623,-0.020833433,-0.065110415,-0.01192491,-0.037220675,-0.04037007,-0.027651723,-0.021800399,0.060904823,0.0540303,-0.021960545,0.03319573,-0.054016758,0.0042634835,-0.028871976,-0.016307363,0.02990144,0.0016183481,0.0038709752,-0.022200894,-0.034767635,-0.06185063,-0.02347841,0.0120624"
  },
  {
    "pr_id": "0d87991c-85ba-4516-8473-d9424d326e2b",
    "content": "@@ -0,0 +1,94 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.amoro.server.authentication;\n+\n+import static org.junit.Assert.assertTrue;\n+import static org.junit.jupiter.api.Assertions.assertThrows;\n+\n+import org.apache.amoro.authentication.PasswdAuthenticationProvider;\n+import org.apache.amoro.authentication.PasswordCredential;\n+import org.apache.amoro.config.Configurations;\n+import org.apache.amoro.exception.SignatureCheckException;\n+import org.apache.amoro.server.AmoroManagementConf;\n+import org.apache.directory.server.annotations.CreateLdapServer;\n+import org.apache.directory.server.annotations.CreateTransport;\n+import org.apache.directory.server.core.annotations.ApplyLdifFiles;\n+import org.apache.directory.server.core.annotations.CreateDS;\n+import org.apache.directory.server.core.annotations.CreatePartition;\n+import org.apache.directory.server.core.integ.AbstractLdapTestUnit;\n+import org.apache.directory.server.core.integ.FrameworkRunner;\n+import org.junit.Test;\n+import org.junit.runner.RunWith;\n+\n+import java.security.Principal;\n+\n+/**\n+ * TestSuite to test amoro's LDAP Authentication provider with an in-process LDAP Server (Apache\n+ * Directory Server instance).\n+ *\n+ * <p>refer to https://directory.apache.org/apacheds/advanced-ug/7-embedding-apacheds.html\n+ */\n+@RunWith(FrameworkRunner.class)\n+@CreateLdapServer(\n+    transports = {@CreateTransport(protocol = \"LDAP\"), @CreateTransport(protocol = \"LDAPS\")})\n+@CreateDS(partitions = {@CreatePartition(name = \"test\", suffix = \"dc=test,dc=com\")})\n+@ApplyLdifFiles({\n+  \"ldap/users.ldif\",\n+})\n+public class LdapPasswdAuthenticationProviderTest extends AbstractLdapTestUnit {\n+\n+  @Test\n+  public void testAuthenticate() throws Exception {\n+    assertTrue(ldapServer.isStarted());\n+    assertTrue(ldapServer.getPort() > 0);\n+\n+    Configurations conf = new Configurations();\n+    String ldapUrl = \"ldap://localhost:\" + ldapServer.getPort();\n+    conf.set(\n+        AmoroManagementConf.HTTP_SERVER_LOGIN_AUTH_PROVIDER,\n+        LdapPasswdAuthenticationProvider.class.getName());\n+    conf.set(AmoroManagementConf.HTTP_SERVER_LOGIN_AUTH_LDAP_URL, ldapUrl + \" \" + ldapUrl);\n+    conf.set(\n+        AmoroManagementConf.HTTP_SERVER_LOGIN_AUTH_LDAP_USERPARTTERN,\n+        \"cn={0},ou=Users,dc=test,dc=com\");\n+\n+    PasswdAuthenticationProvider provider =\n+        HttpAuthenticationFactory.getPasswordAuthenticationProvider(\n+            conf.get(AmoroManagementConf.HTTP_SERVER_LOGIN_AUTH_PROVIDER), conf);\n+\n+    // Test successful authentication with correct password\n+    PasswordCredential correctCredential = new DefaultPasswordCredential(\"user1\", \"12345\");\n+    Principal principal = provider.authenticate(correctCredential);\n+    assertTrue(principal.getName().equals(\"user1\"));\n+\n+    // Test successful authentication with incorrect password\n+    assertThrows(\n+        SignatureCheckException.class,\n+        () -> {\n+          provider.authenticate(new DefaultPasswordCredential(\"user1\", \"123456\"));\n+        });\n+\n+    // Test successful authentication with incorrect user\n+    assertThrows(\n+        SignatureCheckException.class,\n+        () -> {\n+          provider.authenticate(new DefaultPasswordCredential(\"user2\", \"12345\"));\n+        });\n+  }\n+}",
    "section": "code",
    "embedding": "[-0.004400264,-0.009752613,-0.022111978,0.04119505,0.042260688,-0.01683579,0.07152729,0.00721813,-0.0064566038,-0.01756996,-0.013503506,-0.01313437,-0.08622787,0.04264795,-0.0056137787,0.045073643,0.041035257,0.009589425,0.034593713,-0.021500118,-0.010471189,-0.013167727,0.0052089007,0.0069802715,-0.059244007,0.036968473,0.031124022,0.041931152,-0.06708892,-0.014310294,0.024851521,"
  },
  {
    "pr_id": "b2e982f0-8b05-43bf-a510-04e8a37c4771",
    "content": "@@ -192,22 +192,22 @@ func initApplication(debug bool, serverConf *conf.Server, dbConf *data.Database,\n \tobjService := object_info.NewObjService(answerRepo, questionRepo, commentCommonRepo, tagCommonRepo, tagCommonService)\n \tnotificationQueueService := notice_queue.NewNotificationQueueService()\n \texternalNotificationQueueService := notice_queue.NewNewQuestionNotificationQueueService()\n+\treviewRepo := review.NewReviewRepo(dataData)\n+\treviewService := review2.NewReviewService(reviewRepo, objService, userCommon, userRepo, questionRepo, answerRepo, userRoleRelService, externalNotificationQueueService, tagCommonService, questionCommon, notificationQueueService, siteInfoCommonService, commentCommonRepo)\n+\tcommentService := comment2.NewCommentService(commentRepo, commentCommonRepo, userCommon, objService, voteRepo, emailService, userRepo, notificationQueueService, externalNotificationQueueService, activityQueueService, eventQueueService, reviewService)\n \trolePowerRelRepo := role.NewRolePowerRelRepo(dataData)\n \trolePowerRelService := role2.NewRolePowerRelService(rolePowerRelRepo, userRoleRelService)\n \trankService := rank2.NewRankService(userCommon, userRankRepo, objService, userRoleRelService, rolePowerRelService, configService)\n \tlimitRepo := limit.NewRateLimitRepo(dataData)\n \trateLimitMiddleware := middleware.NewRateLimitMiddleware(limitRepo)\n+\tcommentController := controller.NewCommentController(commentService, rankService, captchaService, rateLimitMiddleware)\n \treportRepo := report.NewReportRepo(dataData, uniqueIDRepo)\n \ttagService := tag2.NewTagService(tagRepo, tagCommonService, revisionService, followRepo, siteInfoCommonService, activityQueueService)\n \tanswerActivityRepo := activity.NewAnswerActivityRepo(dataData, activityRepo, userRankRepo, notificationQueueService)\n \tanswerActivityService := activity2.NewAnswerActivityService(answerActivityRepo, configService)\n \texternalNotificationService := notification.NewExternalNotificationService(dataData, userNotificationConfigRepo, followRepo, emailService, userRepo, externalNotificationQueueService, userExternalLoginRepo, siteInfoCommonService)\n-\treviewRepo := review.NewReviewRepo(dataData)\n-\treviewService := review2.NewReviewService(reviewRepo, objService, userCommon, userRepo, questionRepo, answerRepo, userRoleRelService, externalNotificationQueueService, tagCommonService, questionCommon, notificationQueueService, siteInfoCommonService, commentCommonRepo)\n \tquestionService := content.NewQuestionService(activityRepo, questionRepo, answerRepo, tagCommonService, tagService, questionCommon, userCommon, userRepo, userRoleRelService, revisionService, metaCommonService, collectionCommon, answerActivityService, emailService, notificationQueueService, externalNotificationQueueService, activityQueueService, siteInfoCommonService, externalNotificationService, reviewService, configService, eventQueueService, reviewRepo)\n \tanswerService := content.NewAnswerService(answerRepo, questionRepo, questionCommon, userCommon, collectionCommon, userRepo, revisionService, answerActivityService, answerCommon, voteRepo, emailService, userRoleRelService, notificationQueueService, externalNotificationQueueService, activityQueueService, reviewService, eventQueueService)\n-\tcommentService := comment2.NewCommentService(commentRepo, commentCommonRepo, userCommon, objService, voteRepo, emailService, userRepo, notificationQueueService, externalNotificationQueueService, activityQueueService, eventQueueService, reviewService)\n-\tcommentController := controller.NewCommentController(commentService, rankService, captchaService, rateLimitMiddleware)\n \treportHandle := report_handle.NewReportHandle(questionService, answerService, commentService)\n \treportService := report2.NewReportService(reportRepo, objService, userCommon, answerRepo, questionRepo, commentCommonRepo, reportHandle, configService, eventQueueService)\n \treportController := controller.NewReportController(reportService, rankService, captchaService)",
    "section": "code",
    "embedding": "[0.019940175,-0.029317541,-0.00029214864,0.010151117,0.03377257,0.04146928,0.060446873,-0.053841718,-0.0039340043,-0.035862964,0.023734774,-0.04446164,-0.054694917,0.0177606,0.019536354,0.06309972,0.06096685,0.03677796,-0.0029174853,0.016828796,-0.011130047,0.070125334,0.009837864,-0.011963499,0.04032177,0.046098333,0.0014571962,-0.017311607,-0.03443308,0.0018518679,-0.023487177,0."
  },
  {
    "pr_id": "21414c95-7182-4f43-9953-170191c7b346",
    "content": "TITLE: ARTEMIS-5805 Bump mockito.version from 5.20.0 to 5.21.0\n\nBODY:\n",
    "section": "full",
    "embedding": "[-0.039423004,-0.062806964,-0.012790979,0.029568888,0.07296723,0.037894692,0.04611299,0.0085380105,-0.046278667,-0.06054329,0.00069558574,0.017302314,-0.04157863,0.0030450004,-0.041623436,0.03616555,0.027290942,-0.013812143,0.011834581,0.028646287,0.036458537,0.009737666,-0.026725048,0.023513421,-0.07014944,0.018831871,0.03827464,0.015795154,-0.077343404,-0.038252894,0.03408729,-0."
  },
  {
    "pr_id": "0d87991c-85ba-4516-8473-d9424d326e2b",
    "content": "@@ -0,0 +1,37 @@\n+################################################################################\n+#  Licensed to the Apache Software Foundation (ASF) under one\n+#  or more contributor license agreements.  See the NOTICE file\n+#  distributed with this work for additional information\n+#  regarding copyright ownership.  The ASF licenses this file\n+#  to you under the Apache License, Version 2.0 (the\n+#  \"License\"); you may not use this file except in compliance\n+#  with the License.  You may obtain a copy of the License at\n+#\n+#      http://www.apache.org/licenses/LICENSE-2.0\n+#\n+#  Unless required by applicable law or agreed to in writing, software\n+#  distributed under the License is distributed on an \"AS IS\" BASIS,\n+#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+#  See the License for the specific language governing permissions and\n+#  limitations under the License.\n+################################################################################\n+version: 1\n+dn: dc=test,dc=com\n+objectClass: domain\n+objectClass: top\n+dc: test\n+\n+dn: ou=Users,dc=test,dc=com\n+objectClass: organizationalUnit\n+objectClass: top\n+ou: Users\n+\n+dn: cn=user1,ou=Users,dc=test,dc=com\n+objectClass: inetOrgPerson\n+objectClass: organizationalPerson\n+objectClass: person\n+objectClass: top\n+cn: user1\n+sn: Ldap\n+uid: ldaptest1\n+userPassword: 12345\n\\ No newline at end of file",
    "section": "code",
    "embedding": "[0.022825912,0.026708305,-0.020137211,0.028521845,0.056619298,-0.042860974,0.08195257,0.0048771836,0.02777316,-0.0050228885,-0.026556045,-0.0043749986,-0.08191648,0.07303044,0.0015108747,0.045170072,0.040783156,0.0044964757,0.028027238,0.005112772,-0.0060797217,-0.004526907,-0.026385782,-0.005585277,-0.024719188,0.046336338,0.023336826,0.044415955,-0.061132018,-0.03818219,0.0274514"
  },
  {
    "pr_id": "0d87991c-85ba-4516-8473-d9424d326e2b",
    "content": "@@ -163,7 +163,7 @@\n         <pagehelper.version>6.1.0</pagehelper.version>\n         <jsqlparser.version>4.7</jsqlparser.version>\n         <fasterxml.jackson.version>2.14.2</fasterxml.jackson.version>\n-\n+        <apache-directory-server.version>2.0.0.AM25</apache-directory-server.version>\n         <rocksdb-dependency-scope>compile</rocksdb-dependency-scope>\n         <lucene-dependency-scope>compile</lucene-dependency-scope>\n         <aliyun-sdk-dependency-scope>provided</aliyun-sdk-dependency-scope>",
    "section": "code",
    "embedding": "[-0.042273965,0.00027550626,0.029032126,-0.030353846,0.049213357,-0.041354436,0.07540725,-0.019413313,0.018709475,-0.012605941,-0.060911108,-0.01552984,-0.06694333,0.014472592,0.004755917,0.080730304,0.054630656,3.6828955e-05,-0.04152104,-0.0033541364,0.005201321,0.027592307,-0.00012673844,0.017235976,0.037623554,0.031215286,0.034388624,-1.7508053e-05,-0.038643893,-0.033107016,0.05"
  },
  {
    "pr_id": "5d2c55be-8622-413c-bd3f-4d71ccb63151",
    "content": "TITLE: [AMORO-3628] Add user logo wall on the home page.",
    "section": "title",
    "embedding": "[0.017771402,0.034624323,-0.011603753,0.011629451,0.048462264,-0.014178245,0.022853253,0.0018777424,-0.03444446,-0.042713065,-0.066558026,-0.029611086,-0.07034803,-0.00963055,0.0039171097,0.0025724499,0.024832185,0.08267517,0.031899862,-0.026094152,-0.028763685,0.028711975,-0.019392487,-0.027324568,-0.013750583,-0.004696523,0.042918276,-0.0021159821,-0.06374545,0.016336499,0.059173"
  },
  {
    "pr_id": "5d2c55be-8622-413c-bd3f-4d71ccb63151",
    "content": "TITLE: [AMORO-3628] Add user logo wall on the home page.\n\nBODY:\n<!--\r\nThanks for sending a pull request!\r\n\r\nHere are some tips for you:\r\n  1. If this is your first time, please read our contributor guidelines: https://amoro.apache.org/how-to-contribute/\r\n  2. If the PR is related to an issue in https://github.com/apache/amoro/issues, add '[AMORO-XXXX]' in your PR title, e.g., '[AMORO-XXXX] Your PR title ...'.\r\n  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][AMORO-XXXX] Your PR title ...'.\r\n-->\r\n\r\n## Why are the changes needed?\r\n<!--\r\nPlease clarify why the changes are needed. For instance,\r\n  1. If you add a feature, you can talk about its use case.\r\n  2. If you fix a bug, you can clarify why it is a bug.\r\n  3. Use Fix/Resolve/Close #{ISSUE_NUMBER} to link this PR to its related issue\r\n-->\r\n\r\nClose #3628 \r\n\r\n## Brief change log\r\n<!--\r\nClearly describe the changes made in modules, classes, methods, etc.\r\n-->\r\n\r\n<img width=\"1399\" height=\"988\" alt=\"image\" src=\"https://github.com/user-attachments/assets/3022b2bd-c67e-4c39-a5f6-f298133144a4\" />\r\n\r\n\r\n## How was this patch tested?\r\n\r\n- [ ] Add some test cases that check the changes thoroughly including negative and positive cases if possible\r\n\r\n- [ ] Add screenshots for manual tests if appropriate\r\n\r\n- [x] Run test locally before making a pull request\r\n\r\n## Documentation\r\n\r\n- Does this pull request introduce a new feature? (no)\r\n- If yes, how is the feature documented? (not applicable)\r\n",
    "section": "full",
    "embedding": "[0.023291323,0.009861473,0.0015425254,0.023500469,0.05684632,-0.011572291,0.021878935,0.010560102,-0.025321674,-0.039479885,-0.049343877,-0.0045306557,-0.08204297,0.013307608,-0.018512212,0.04145882,0.05218986,0.05284561,0.024534734,-0.0017721313,-0.0024496117,0.010369994,-0.024820315,-0.012465033,-0.030442316,-0.013199711,0.021839183,0.003394419,-0.06856245,-0.039612677,0.05248518"
  },
  {
    "pr_id": "5d2c55be-8622-413c-bd3f-4d71ccb63151",
    "content": "",
    "section": "prose",
    "embedding": "[0.012244709,0.039513454,-0.007785414,0.0021320505,0.046604965,0.012159259,0.03977907,0.04207942,-0.020763515,-0.039035235,0.024283497,-0.00805646,-0.04972243,0.017127944,-0.009930698,0.038848016,0.008790611,0.0133490125,-0.017834641,0.0071114884,0.030171363,-0.022801451,0.031216213,-0.00092527375,0.025564943,-0.024615467,0.027907416,0.0069565354,-0.02231851,0.03557899,0.019480716,"
  },
  {
    "pr_id": "5d2c55be-8622-413c-bd3f-4d71ccb63151",
    "content": "<!--\r\nThanks for sending a pull request!\r\n\r\nHere are some tips for you:\r\n  1. If this is your first time, please read our contributor guidelines: https://amoro.apache.org/how-to-contribute/\r\n  2. If the PR is related to an issue in https://github.com/apache/amoro/issues, add '[AMORO-XXXX]' in your PR title, e.g., '[AMORO-XXXX] Your PR title ...'.\r\n  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][AMORO-XXXX] Your PR title ...'.\r\n-->\r\n\r\n## Why are the changes needed?\r\n<!--\r\nPlease clarify why the changes are needed. For instance,\r\n  1. If you add a feature, you can talk about its use case.\r\n  2. If you fix a bug, you can clarify why it is a bug.\r\n  3. Use Fix/Resolve/Close #{ISSUE_NUMBER} to link this PR to its related issue\r\n-->\r\n\r\nClose #3628 \r\n\r\n## Brief change log\r\n<!--\r\nClearly describe the changes made in modules, classes, methods, etc.\r\n-->\r\n\r\n<img width=\"1399\" height=\"988\" alt=\"image\" src=\"https://github.com/user-attachments/assets/3022b2bd-c67e-4c39-a5f6-f298133144a4\" />\r\n\r\n\r\n## How was this patch tested?\r\n\r\n- [ ] Add some test cases that check the changes thoroughly including negative and positive cases if possible\r\n\r\n- [ ] Add screenshots for manual tests if appropriate\r\n\r\n- [x] Run test locally before making a pull request\r\n\r\n## Documentation\r\n\r\n- Does this pull request introduce a new feature? (no)\r\n- If yes, how is the feature documented? (not applicable)",
    "section": "prose",
    "embedding": "[0.021666192,-0.01901542,0.020372435,0.015464438,0.038307805,0.004293466,0.026555132,0.014021688,-0.02923175,-0.046563264,-0.045763843,0.011917378,-0.08439875,0.0015824094,-0.029935772,0.05503617,0.05111989,0.027792787,0.03081624,0.0049132784,0.014097476,0.0011339409,-0.017213434,0.0006062615,-0.03555419,-0.021139761,-0.00040395025,0.0020643824,-0.05235862,-0.056295916,0.050405864,"
  },
  {
    "pr_id": "5d2c55be-8622-413c-bd3f-4d71ccb63151",
    "content": "@@ -50,6 +50,68 @@ sectionPagesMenu = \"main\"\n   desc = \"Can be easily deployed and used in private environments, cloud environments, hybrid cloud environments, and multi-cloud environments.\"\n   icon = \"Infrastructure-Idependent\"\n \n+# Users who are using Amoro\n+[[params.users]]\n+  name = \"ByteDance\"\n+  logo = \"bytedance.png\"\n+[[params.users]]\n+  name = \"Tencent Cloud\"\n+  logo = \"tencent-cloud.svg\"\n+[[params.users]]\n+  name = \"Cisco\"\n+  logo = \"cisco.png\"\n+[[params.users]]\n+  name = \"Youdao\"\n+  logo = \"youdao.png\"\n+[[params.users]]\n+  name = \"Huya\"\n+  logo = \"huya.png\"\n+[[params.users]]\n+  name = \"NetEase Media\"\n+  logo = \"netease-media.png\"\n+[[params.users]]\n+  name = \"GREE\"\n+  logo = \"gree.png\"\n+[[params.users]]\n+  name = \"Lalamove\"\n+  logo = \"lalamove.png\"\n+[[params.users]]\n+  name = \"Lilith Games\"\n+  logo = \"lilith.png\"\n+[[params.users]]\n+  name = \"Huatai Securities\"\n+  logo = \"huatai.png\"\n+[[params.users]]\n+  name = \"Gaoji Medical\"\n+  logo = \"gaoji.png\"\n+[[params.users]]\n+  name = \"TRS\"\n+  logo = \"trs.png\"\n+[[params.users]]\n+  name = \"Shunwang\"\n+  logo = \"shunwang.png\"\n+[[params.users]]\n+  name = \"China Telecom Zhejiang\"\n+  logo = \"china-telecom.png\"\n+[[params.users]]\n+  name = \"SEMI\"\n+  logo = \"semi.png\"\n+[[params.users]]\n+  name = \"China Mobile Suzhou\"\n+  logo = \"china-mobile.png\"\n+[[params.users]]\n+  name = \"Onething Tech\"\n+  logo = \"onething.svg\"\n+[[params.users]]\n+  name = \"DMALL\"\n+  logo = \"dmall.png\"\n+[[params.users]]\n+  name = \"Ctrip\"\n+  logo = \"ctrip.png\"\n+[[params.users]]\n+  name = \"Huifu\"\n+  logo = \"huifu.svg\"\n+\n [outputFormats.SearchIndex]\n baseName = \"landingpagesearch\"\n mediaType = \"application/json\"",
    "section": "code",
    "embedding": "[0.03336584,0.042623457,-0.0066180183,0.032723505,0.03406303,0.024757659,0.012348906,0.07047525,-0.040018428,-0.006041359,-0.026199918,-0.023365965,-0.08337203,0.008972435,-0.018582111,0.038631234,0.055443775,0.017938746,0.037017975,-0.040832732,-0.01948766,0.0039502485,-0.011474513,0.0019218684,-0.01771355,0.0027981836,0.013015817,0.025193844,-0.03402648,-0.03790619,0.04505395,0.0"
  },
  {
    "pr_id": "b2e982f0-8b05-43bf-a510-04e8a37c4771",
    "content": "@@ -10899,7 +10890,7 @@\n                 },\n                 \"theme_config\": {\n                     \"type\": \"object\",\n-                    \"additionalProperties\": true\n+                    \"additionalProperties\": {}\n                 }\n             }\n         },",
    "section": "code",
    "embedding": "[0.002236093,0.0031506186,0.06975845,-0.009273964,0.050609663,0.02334474,0.07004288,0.0023892187,-0.06973956,-0.04225818,-0.06630718,-0.007079389,-0.07275225,0.0101475,0.0039878925,0.05108503,0.055691767,-0.0041771,-0.008943435,-0.014908873,0.01857168,-0.004981817,-0.029779,0.007812692,-0.023869587,-0.0029325369,0.002494816,0.037821483,-0.07654131,0.009080889,-0.0076563056,0.037406"
  },
  {
    "pr_id": "5d2c55be-8622-413c-bd3f-4d71ccb63151",
    "content": "@@ -0,0 +1,41 @@\n+<!-- - Licensed to the Apache Software Foundation (ASF) under one or more-->\n+<!-- - contributor license agreements.  See the NOTICE file distributed with-->\n+<!-- - this work for additional information regarding copyright ownership.-->\n+<!-- - The ASF licenses this file to You under the Apache License, Version 2.0-->\n+<!-- - (the \"License\"); you may not use this file except in compliance with-->\n+<!-- - the License.  You may obtain a copy of the License at-->\n+<!-- - -->\n+<!-- -   http://www.apache.org/licenses/LICENSE-2.0-->\n+<!-- - -->\n+<!-- - Unless required by applicable law or agreed to in writing, software-->\n+<!-- - distributed under the License is distributed on an \"AS IS\" BASIS,-->\n+<!-- - WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.-->\n+<!-- - See the License for the specific language governing permissions and-->\n+<!-- - limitations under the License.-->\n+<div id=\"home-users\">\n+  <div class=\"content\">\n+    <div class=\"title\">Who is Using Amoro</div>\n+    <div class=\"subtitle\">Trusted by leading companies worldwide</div>\n+    <div class=\"users-wall\">\n+      <div class=\"users-track\">\n+        {{range .Site.Params.users}}\n+        <div class=\"user-item\" title=\"{{ .name }}\">\n+          <img src=\"{{ .Site.BaseURL }}/img/users/{{ .logo }}\" alt=\"{{ .name }}\" />\n+        </div>\n+        {{end}}\n+        <!-- Duplicate for infinite scroll effect -->\n+        {{range .Site.Params.users}}\n+        <div class=\"user-item\" title=\"{{ .name }}\">\n+          <img src=\"{{ .Site.BaseURL }}/img/users/{{ .logo }}\" alt=\"{{ .name }}\" />\n+        </div>\n+        {{end}}\n+      </div>\n+    </div>\n+    <div class=\"add-company\">\n+      <a href=\"https://github.com/apache/amoro/issues/1853\" target=\"_blank\" class=\"btn btn-add\">\n+        Add Your Company\n+      </a>\n+    </div>\n+  </div>\n+</div>\n+",
    "section": "code",
    "embedding": "[0.0031710442,-0.0025347113,0.018678328,0.008937746,0.035623286,-0.026122242,0.07062417,0.017205395,0.0032806646,-0.005644216,-0.03840514,-0.0015434546,-0.07958559,0.0626099,0.0033938743,0.055124935,0.041014034,0.020228352,0.036982175,-0.029082576,-0.00944772,-0.0137651125,-0.0011102109,-0.012305013,-0.05717253,0.027997432,0.00379662,0.067172505,-0.052570015,0.0077304533,0.03817002"
  },
  {
    "pr_id": "5d2c55be-8622-413c-bd3f-4d71ccb63151",
    "content": "@@ -206,4 +206,150 @@\n \n #home-footer .apache .logo{\n   padding-top: 20px;\n-}\n\\ No newline at end of file\n+}\n+\n+/* User Wall Styles */\n+#home-users {\n+  background: #fff;\n+  margin: 0 auto;\n+  display: flex;\n+  justify-content: center;\n+  overflow: hidden;\n+}\n+\n+#home-users .content {\n+  width: 100%;\n+  max-width: 1182px;\n+  padding: 64px 41px;\n+}\n+\n+#home-users .content .title {\n+  font-size: 32px;\n+  font-weight: bold;\n+  line-height: 24px;\n+  text-align: center;\n+  margin-bottom: 16px;\n+}\n+\n+#home-users .content .subtitle {\n+  font-size: 16px;\n+  color: #53576a;\n+  text-align: center;\n+  margin-bottom: 40px;\n+}\n+\n+#home-users .users-wall {\n+  width: 100%;\n+  overflow: hidden;\n+  position: relative;\n+}\n+\n+#home-users .users-wall::before,\n+#home-users .users-wall::after {\n+  content: '';\n+  position: absolute;\n+  top: 0;\n+  bottom: 0;\n+  width: 100px;\n+  z-index: 2;\n+  pointer-events: none;\n+}\n+\n+#home-users .users-wall::before {\n+  left: 0;\n+  background: linear-gradient(to right, #fff 0%, transparent 100%);\n+}\n+\n+#home-users .users-wall::after {\n+  right: 0;\n+  background: linear-gradient(to left, #fff 0%, transparent 100%);\n+}\n+\n+#home-users .users-track {\n+  display: flex;\n+  animation: scroll-left 40s linear infinite;\n+  width: fit-content;\n+}\n+\n+#home-users .users-track:hover {\n+  animation-play-state: paused;\n+}\n+\n+@keyframes scroll-left {\n+  0% {\n+    transform: translateX(0);\n+  }\n+  100% {\n+    transform: translateX(-50%);\n+  }\n+}\n+\n+#home-users .user-item {\n+  flex-shrink: 0;\n+  width: 180px;\n+  height: 80px;\n+  margin: 0 20px;\n+  padding: 12px;\n+  display: flex;\n+  align-items: center;\n+  justify-content: center;\n+  background: #f5f6fa;\n+  border-radius: 8px;\n+  transition: transform 0.3s ease, box-shadow 0.3s ease;\n+}\n+\n+#home-users .user-item:hover {\n+  transform: scale(1.05);\n+  box-shadow: 0 4px 12px rgba(0, 0, 0, 0.1);\n+}\n+\n+\n+#home-users .user-item img {\n+  max-width: 100%;\n+  max-height: 100%;\n+  object-fit: contain;\n+  filter: grayscale(100%);\n+  opacity: 0.7;\n+  transition: filter 0.3s ease, opacity 0.3s ease;\n+}\n+\n+#home-users .user-item:hover img {\n+  filter: grayscale(0%);\n+  opacity: 1;\n+}\n+\n+#home-users .add-company {\n+  text-align: center;\n+  margin-top: 40px;\n+}\n+\n+#home-users .btn-add {\n+  display: inline-block;\n+  padding: 12px 32px;\n+  background: #0036A1;\n+  color: #fff;\n+  font-size: 16px;\n+  font-weight: 500;\n+  border-radius: 8px;\n+  text-decoration: none;\n+  transition: background 0.3s ease;\n+}\n+\n+#home-users .btn-add:hover {\n+  background: #002880;\n+  color: #fff;\n+  text-decoration: none;\n+}\n+\n+@media screen and (max-width: 800px) {\n+  #home-users .user-item {\n+    width: 140px;\n+    height: 60px;\n+    margin: 0 12px;\n+  }\n+\n+  #home-users .users-wall::before,\n+  #home-users .users-wall::after {\n+    width: 50px;\n+  }\n+}",
    "section": "code",
    "embedding": "[-0.007162697,0.039757814,0.0018033737,-0.019884598,0.06959296,-0.052909806,0.04772895,0.03336007,-0.011619498,-0.07749642,-0.052043546,-0.054712843,-0.03281438,0.013744323,0.021262677,0.045287654,0.032378998,0.05420864,0.009515578,-0.0054519027,-0.009560812,-0.004832367,-0.053879857,-0.02655515,-0.020636488,-0.036468394,0.027418463,0.053180743,-0.06775203,0.032913826,0.040030602,-"
  }
]